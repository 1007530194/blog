<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!-->
<html class="no-js" lang="en"><!--<![endif]-->
    <head>
<meta charset="utf-8">
<title>零基础入门深度学习(5) - 循环神经网络 &mdash; 魑魅魍魉</title>

<meta name="author" content="niult">






<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">



<link href="../../../theme/css/main.css" media="screen, projection"
      rel="stylesheet" type="text/css">

<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
      rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
      rel="stylesheet" type="text/css">
</head>

<body>

<script src="../../../theme/js/modernizr-2.0.js"></script>
<script src="../../../theme/js/ender.js"></script>
<script src="../../../theme/js/octopress.js" type="text/javascript"></script>
<script src="../../../theme/js/echarts.min.js" type="text/javascript"></script>
<script src="../../../theme/js/require.min.js" type="text/javascript"></script>

<header role="banner"><hgroup>
  <h1><a href="../../../">魑魅魍魉</a></h1>
</hgroup></header>
<nav role="navigation">    <ul class="subscription" data-subscription="rss">
    </ul>


<ul class="main-navigation">
            <li >
                <a href="../../../category/01chang yong gong ju.html">01常用工具</a>
            </li>
            <li >
                <a href="../../../category/02.wo ai du shu.html">02.我爱读书</a>
            </li>
            <li >
                <a href="../../../category/algorithms.html">Algorithms</a>
            </li>
            <li >
                <a href="../../../category/book.html">Book</a>
            </li>
            <li >
                <a href="../../../category/book-pydata.html">Book-pydata</a>
            </li>
            <li >
                <a href="../../../category/deep-learning-with-python.html">Deep-learning-with-python</a>
            </li>
            <li >
                <a href="../../../category/ji qi xue xi shi zhan.html">机器学习实战</a>
            </li>
            <li >
                <a href="../../../category/ke wai du wu.html">课外读物</a>
            </li>
            <li class="active">
                <a href="../../../category/ling ji chu ru men shen du xue xi.html">零基础入门深度学习</a>
            </li>
            <li >
                <a href="../../../category/shen du xue xi.html">深度学习</a>
            </li>
            <li >
                <a href="../../../category/shen jing wang luo.html">神经网络</a>
            </li>
            <li >
                <a href="../../../category/shu ju wa jue.html">数据挖掘</a>
            </li>
            <li >
                <a href="../../../category/shu xue ji chu.html">数学基础</a>
            </li>
            <li >
                <a href="../../../category/tf-example.html">Tf-example</a>
            </li>
            <li >
                <a href="../../../category/tool1.html">Tool1</a>
            </li>
            <li >
                <a href="../../../category/tool2.html">Tool2</a>
            </li>
            <li >
                <a href="../../../category/tools.html">Tools</a>
            </li>
            <li >
                <a href="../../../category/tui jian xi tong.html">推荐系统</a>
            </li>
            <li >
                <a href="../../../category/wen ben wa jue.html">文本挖掘</a>
            </li>
</ul>
</nav>
<div id="main">
    <div id="content">
    <div>

        <h4>Contents</h4>
        

        <article class="hentry" role="article">
<header>
        <h1 class="entry-title">零基础入门深度学习(5) - 循环神经网络</h1>
    <p class="meta">
<time datetime="2018-01-01T00:00:00+08:00" pubdate>2018-01-01 00:00</time>    </p>
</header>

    <div class="entry-content"><style type="text/css">/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI colors. */
.ansibold {
  font-weight: bold;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  border-left-width: 1px;
  padding-left: 5px;
  background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%);
}
div.cell.jupyter-soft-selected {
  border-left-color: #90CAF9;
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected {
  border-color: #ababab;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%);
}
@media print {
  div.cell.selected {
    border-color: transparent;
  }
}
div.cell.selected.jupyter-soft-selected {
  border-left-width: 0;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%);
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%);
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  padding: 0.4em;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */
  /* .CodeMirror-lines */
  padding: 0;
  border: 0;
  border-radius: 0;
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area 
div.output_area 
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}



.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}






.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}








.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}


.rendered_html pre,



.rendered_html tr,
.rendered_html th,

.rendered_html td,


.rendered_html * + table {
  margin-top: 1em;
}

.rendered_html * + p {
  margin-top: 1em;
}

.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,

.rendered_html img.unconfined,

div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered 
.text_cell.unrendered .text_cell_render {
  display: none;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
</style>
<style type="text/css">.highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */</style>
<style type="text/css">
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }
</style><body>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote><p>无论即将到来的是大数据时代还是人工智能时代，亦或是传统行业使用人工智能在云上处理大数据的时代，作为一个有理想有追求的程序员，不懂深度学习（Deep Learning）这个超热的技术，会不会感觉马上就out了？现在救命稻草来了，《零基础入门深度学习》系列文章旨在讲帮助爱编程的你从零基础达到入门级水平。零基础意味着你不需要太多的数学知识，只要会写程序就行了，没错，这是专门为程序员写的文章。虽然文中会有很多公式你也许看不懂，但同时也会有更多的代码，程序员的你一定能看懂的（我周围是一群狂热的Clean Code程序员，所以我写的代码也不会很差）。</p>
</blockquote>
<p><a href="https://www.zybuluo.com/hanbingtao/note/433855">零基础入门深度学习(1) - 感知器</a><br/>
<a href="https://www.zybuluo.com/hanbingtao/note/448086">零基础入门深度学习(2) - 线性单元和梯度下降 </a><br/>
<a href="https://www.zybuluo.com/hanbingtao/note/476663">零基础入门深度学习(3) - 神经网络和反向传播算法 </a><br/>
<a href="https://www.zybuluo.com/hanbingtao/note/485480">零基础入门深度学习(4) - 卷积神经网络 </a><br/>
<a href="https://zybuluo.com/hanbingtao/note/541458">零基础入门深度学习(5) - 循环神经网络 </a><br/>
<a href="https://zybuluo.com/hanbingtao/note/581764">零基础入门深度学习(6) - 长短时记忆网络(LSTM) </a><br/>
<a href="https://zybuluo.com/hanbingtao/note/626300">零基础入门深度学习(7) - 递归神经网络</a></p>
<h1 id="往期回顾">往期回顾<a class="anchor-link" href="#往期回顾">¶</a></h1><p>在前面的文章系列文章中，我们介绍了全连接神经网络和卷积神经网络，以及它们的训练和使用。他们都只能单独的取处理一个个的输入，前一个输入和后一个输入是完全没有关系的。但是，某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的。比如，当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个<strong>序列</strong>；当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要分析这些帧连接起来的整个<strong>序列</strong>。这时，就需要用到深度学习领域中另一类非常重要神经网络：<strong>循环神经网络</strong>(Recurrent Neural Network)。RNN种类很多，也比较绕脑子。不过读者不用担心，本文将一如既往的对复杂的东西剥茧抽丝，帮助您理解RNNs以及它的训练算法，并动手实现一个<strong>循环神经网络</strong>。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="语言模型">语言模型<a class="anchor-link" href="#语言模型">¶</a></h1><p>RNN是在<strong>自然语言处理</strong>领域中最先被用起来的，比如，RNN可以为语言模型来建模。那么，什么是<strong>语言模型</strong>呢？</p>
<p>我们可以和电脑玩一个游戏，我们写出一个句子前面的一些词，然后，让电脑帮我们写下接下来的一个词。比如下面这句：</p>
<blockquote><p>我昨天上学迟到了，老师批评了<em>__</em>。</p>
</blockquote>
<p>我们给电脑展示了这句话前面这些词，然后，让电脑写下接下来的一个词。在这个例子中，接下来的这个词最有可能是『我』，而不太可能是『小明』，甚至是『吃饭』。</p>
<p><strong>语言模型</strong>就是这样的东西：给定一个一句话前面的部分，预测接下来最有可能的一个词是什么。</p>
<p><strong>语言模型</strong>是对一种语言的特征进行建模，它有很多很多用处。比如在语音转文本(STT)的应用中，声学模型输出的结果，往往是若干个可能的候选词，这时候就需要语言模型来从这些候选词中选择一个最可能的。当然，它同样也可以用在图像到文本的识别中(OCR)。</p>
<p>使用RNN之前，语言模型主要是采用N-Gram。N可以是一个自然数，比如2或者3。它的含义是，假设一个词出现的概率只与前面N个词相关。我们以2-Gram为例。首先，对前面的一句话进行切词：</p>
<blockquote><p>我 昨天 上学 迟到 了 ，老师 批评 了 <em>__</em>。</p>
</blockquote>
<p>如果用2-Gram进行建模，那么电脑在预测的时候，只会看到前面的『了』，然后，电脑会在语料库中，搜索『了』后面最可能的一个词。不管最后电脑选的是不是『我』，我们都知道这个模型是不靠谱的，因为『了』前面说了那么一大堆实际上是没有用到的。如果是3-Gram模型呢，会搜索『批评了』后面最可能的词，感觉上比2-Gram靠谱了不少，但还是远远不够的。因为这句话最关键的信息『我』，远在9个词之前！</p>
<p>现在读者可能会想，可以提升继续提升N的值呀，比如4-Gram、5-Gram.......。实际上，这个想法是没有实用性的。因为我们想处理任意长度的句子，N设为多少都不合适；另外，模型的大小和N的关系是指数级的，4-Gram模型就会占用海量的存储空间。</p>
<p>所以，该轮到RNN出场了，RNN理论上可以往前看(往后看)任意多个词。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="循环神经网络是啥">循环神经网络是啥<a class="anchor-link" href="#循环神经网络是啥">¶</a></h1><p>循环神经网络种类繁多，我们先从最简单的基本循环神经网络开始吧。</p>
<h2 id="基本循环神经网络">基本循环神经网络<a class="anchor-link" href="#基本循环神经网络">¶</a></h2><p>下图是一个简单的循环神经网络如，它由输入层、一个隐藏层和一个输出层组成：
<img alt="image.png" src="http://upload-images.jianshu.io/upload_images/2256672-479f2a7488b91671.jpg"/></p>
<p>纳尼？！相信第一次看到这个玩意的读者内心和我一样是崩溃的。因为<strong>循环神经网络</strong>实在是太难画出来了，网上所有大神们都不得不用了这种抽象艺术手法。不过，静下心来仔细看看的话，其实也是很好理解的。如果把上面有W的那个带箭头的圈去掉，它就变成了最普通的<strong>全连接神经网络</strong>。x是一个向量，它表示<strong>输入层</strong>的值（这里面没有画出来表示神经元节点的圆圈）；s是一个向量，它表示<strong>隐藏层</strong>的值（这里隐藏层面画了一个节点，你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；U是输入层到隐藏层的<strong>权重矩阵</strong>（读者可以回到第三篇文章零基础入门深度学习(3) - 神经网络和反向传播算法，看看我们是怎样用矩阵来表示全连接神经网络的计算的）；o也是一个向量，它表示输出层的值；V是隐藏层到输出层的权重矩阵。那么，现在我们来看看W是什么。<strong>循环神经网络</strong>的<strong>隐藏层</strong>的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层的值s。<strong>权重矩阵</strong> W就是隐藏层上一次的值作为这一次的输入的权重。</p>
<p>如果我们把上面的图展开，<strong>循环神经网络</strong>也可以画成下面这个样子：</p>
<p><img alt="循环神经网络" src="http://upload-images.jianshu.io/upload_images/2256672-cf18bb1f06e750a4.jpg"/></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>现在看上去就比较清楚了，这个网络在t时刻接收到输入$x_t$之后，隐藏层的值是$s_t$，输出值是$o_t$。关键一点是，$s_t$的值不仅仅取决于$x_t$，还取决于$s_{t-1}$。我们可以用下面的公式来表示循环神经网络的计算方法：</p>
<p>\begin{align}
\mathrm{o}_t&amp;=g(V\mathrm{s}_t) \label{eq:eq001}\\
\mathrm{s}_t&amp;=f(U\mathrm{x}_t+W\mathrm{s}_{t-1})\label{eq:eq002}
\end{align}</p>
<p>式\ref{eq:eq001}是<strong>输出层</strong>的计算公式，输出层是一个<strong>全连接层</strong>，也就是它的每个节点都和隐藏层的每个节点相连。V是输出层的<strong>权重矩阵</strong>，g是<strong>激活函数</strong>。式\ref{eq:eq002}是隐藏层的计算公式，它是<strong>循环层</strong>。U是输入x的权重矩阵，W是上一次的值作为这一次的输入的<strong>权重矩阵</strong>，f是<strong>激活函数</strong>。</p>
<p>从上面的公式我们可以看出，<strong>循环层</strong>和<strong>全连接层</strong>的区别就是循环层多了一个<strong>权重矩阵</strong> W。</p>
<p>如果反复把式\ref{eq:eq002}带入到式\ref{eq:eq001}，我们将得到</p>
<p>\begin{align}
\mathrm{o}_t&amp;=g(V\mathrm{s}_t)\\
&amp;=Vf(U\mathrm{x}_t+W\mathrm{s}_{t-1})\\
&amp;=Vf(U\mathrm{x}_t+Wf(U\mathrm{x}_{t-1}+W\mathrm{s}_{t-2}))\\
&amp;=Vf(U\mathrm{x}_t+Wf(U\mathrm{x}_{t-1}+Wf(U\mathrm{x}_{t-2}+W\mathrm{s}_{t-3})))\\
&amp;=Vf(U\mathrm{x}_t+Wf(U\mathrm{x}_{t-1}+Wf(U\mathrm{x}_{t-2}+Wf(U\mathrm{x}_{t-3}+...))))
\end{align}</p>
<p>从上面可以看出，<strong>循环神经网络</strong>的输出值$o_t$，是受前面历次输入值$x_t$、$x_{t-1}$、$x_{t-2}$、$x_{t-3}$、...影响的，这就是为什么<strong>循环神经网络</strong>可以往前看<strong>任意多个输入值</strong>的原因。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="双向循环神经网络">双向循环神经网络<a class="anchor-link" href="#双向循环神经网络">¶</a></h2><p>对于<strong>语言模型</strong>来说，很多时候光看前面的词是不够的，比如下面这句话：</p>
<blockquote><p>我的手机坏了，我打算____一部新手机。</p>
</blockquote>
<p>可以想象，如果我们只看横线前面的词，手机坏了，那么我是打算修一修？换一部新的？还是大哭一场？这些都是无法确定的。但如果我们也看到了横线后面的词是『一部新手机』，那么，横线上的词填『买』的概率就大得多了。</p>
<p>在上一小节中的<strong>基本循环神经网络</strong>是无法对此进行建模的，因此，我们需要<strong>双向循环神经网络</strong>，如下图所示：</p>
<p><img alt="双向神经网络" src="http://upload-images.jianshu.io/upload_images/2256672-039a45251aa5d220.png"/></p>
<p>当遇到这种从未来穿越回来的场景时，难免处于懵逼的状态。不过我们还是可以用屡试不爽的老办法：先分析一个特殊场景，然后再总结一般规律。我们先考虑上图中，$y_2$的计算。</p>
<p>从上图可以看出，双向卷积神经网络的隐藏层要保存两个值，一个A参与正向计算，另一个值A'参与反向计算。最终的输出值$y_2$取决于$A_2$和$A_2'$。其计算方法为：</p>
<p>$$ \mathrm{y}_2=g(VA_2+V'A_2')$$</p>
<p>$A_2$和$A_2'$则分别计算：</p>
<p>\begin{align}
A_2&amp;=f(WA_1+U\mathrm{x}_2)\\
A_2'&amp;=f(W'A_3'+U'\mathrm{x}_2)\\
\end{align}</p>
<p>现在，我们已经可以看出一般的规律：正向计算时，隐藏层的值$s_t$与$s_{t-1}$有关；反向计算时，隐藏层的值$s_t'$与$s_{t+1}'$有关；最终的输出取决于正向和反向计算的加和。现在，我们仿照式\ref{eq:eq001}和式\ref{eq:eq002}，写出双向循环神经网络的计算方法</p>
<p>\begin{align}
\mathrm{o}_t&amp;=g(V\mathrm{s}_t+V'\mathrm{s}_t')\\
\mathrm{s}_t&amp;=f(U\mathrm{x}_t+W\mathrm{s}_{t-1})\\
\mathrm{s}_t'&amp;=f(U'\mathrm{x}_t+W'\mathrm{s}_{t+1}')\\
\end{align}</p>
<p>从上面三个公式我们可以看到，正向计算和反向计算不共享权重，也就是说U和U'、W和W'、V和V'都是不同的<strong>权重矩阵</strong>。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="深度循环神经网络">深度循环神经网络<a class="anchor-link" href="#深度循环神经网络">¶</a></h2><p>前面我们介绍的循环神经网络只有一个隐藏层，我们当然也可以堆叠两个以上的隐藏层，这样就得到了<strong>深度循环神经网络</strong>。如下图所示：</p>
<p><img alt="深度循环神经网络" src="http://upload-images.jianshu.io/upload_images/2256672-df137de8007c3d26.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/480"/></p>
<p>我们把第i个隐藏层的值表示为$s_t^{(i)}$、$s_t^{'(i)}$，则深度循环神经网络的计算方式可以表示为：</p>
<p>\begin{align}
\mathrm{o}_t&amp;=g(V^{(i)}\mathrm{s}_t^{(i)}+V'^{(i)}\mathrm{s}_t'^{(i)})\\
\mathrm{s}_t^{(i)}&amp;=f(U^{(i)}\mathrm{s}_t^{(i-1)}+W^{(i)}\mathrm{s}_{t-1})\\
\mathrm{s}_t'^{(i)}&amp;=f(U'^{(i)}\mathrm{s}_t'^{(i-1)}+W'^{(i)}\mathrm{s}_{t+1}')\\
...\\
\mathrm{s}_t^{(1)}&amp;=f(U^{(1)}\mathrm{x}_t+W^{(1)}\mathrm{s}_{t-1})\\
\mathrm{s}_t'^{(1)}&amp;=f(U'^{(1)}\mathrm{x}_t+W'^{(1)}\mathrm{s}_{t+1}')\\
\end{align}</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="循环神经网络的训练">循环神经网络的训练<a class="anchor-link" href="#循环神经网络的训练">¶</a></h1><h2 id="循环神经网络的训练算法：BPTT">循环神经网络的训练算法：BPTT<a class="anchor-link" href="#循环神经网络的训练算法：BPTT">¶</a></h2><p>BPTT算法是针对<strong>循环层</strong>的训练算法，它的基本原理和BP算法是一样的，也包含同样的三个步骤：</p>
<ul>
<li>前向计算每个神经元的输出值；</li>
<li>反向计算每个神经元的<strong>误差项</strong>$\delta_j$值，它是误差函数E对神经元j的<strong>加权输入</strong>$net_j$的偏导数；</li>
<li>计算每个权重的梯度。</li>
</ul>
<p>最后再用<strong>随机梯度下降</strong>算法更新权重。</p>
<p>循环层如下图所示：</p>
<p><img alt="" src="http://upload-images.jianshu.io/upload_images/2256672-3b20294694c3904b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/480"/></p>
<h3 id="前向计算">前向计算<a class="anchor-link" href="#前向计算">¶</a></h3><p>使用前面的式\ref{eq:eq002}对循环层进行前向计算：</p>
<p>$$\mathrm{s}_t=f(U\mathrm{x}_t+W\mathrm{s}_{t-1})$$</p>
<p>注意，上面的$s_t$、$x_t$、$s_{t-1}$都是向量，用<strong>黑体字母</strong>表示；而U、V是<strong>矩阵</strong>，用<strong>大写字母</strong>表示。向量的下标表示时刻，例如，表示在t时刻向量s的值。</p>
<p>我们假设输入向量x的维度是m，输出向量s的维度是n，则矩阵U的维度是$n\times m$，矩阵W的维度是$n\times n$。下面是上式展开成矩阵的样子，看起来更直观一些：</p>
<p>\begin{align}
\begin{bmatrix}
s_1^t\\
s_2^t\\
.\\.\\
s_n^t\\
\end{bmatrix}=f\Big(
\begin{bmatrix}
u_{11} u_{12} ... u_{1m}\\
u_{21} u_{22} ... u_{2m}\\
.\\.\\
u_{n1} u_{n2} ... u_{nm}\\
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
.\\.\\
x_m\\
\end{bmatrix}+
\begin{bmatrix}
w_{11} w_{12} ... w_{1n}\\
w_{21} w_{22} ... w_{2n}\\
.\\.\\
w_{n1} w_{n2} ... w_{nn}\\
\end{bmatrix}
\begin{bmatrix}
s_1^{t-1}\\
s_2^{t-1}\\
.\\.\\
s_n^{t-1}\\
\end{bmatrix}\Big)
\end{align}</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>在这里我们用手写体字母表示向量的一个元素，它的下标表示它是这个向量的第几个元素，它的上标表示第几个时刻。例如，$s_j^t$表示向量s的第j个元素在t时刻的值。$u_{ji}$表示输入层第i个神经元到循环层第j个神经元的权重。$u_{ji}$表示循环层第t-1时刻的第i个神经元到循环层第t个时刻的第j个神经元的权重。</p>
<p>误差项的计算
BTPP算法将第l层t时刻的<strong>误差项</strong>$\delta_t^l$值沿两个方向传播，一个方向是其传递到上一层网络，得到$\delta_t^{l-1}$，这部分只和权重矩阵U有关；另一个是方向是将其沿时间线传递到初始$t_1$时刻，得到$\delta_t^l$，这部分只和权重矩阵W有关。</p>
<p>我们用向量$net_t$表示神经元在t时刻的加权输入，因为：
\begin{align}
\mathrm{net}_t&amp;=U\mathrm{x}_t+W\mathrm{s}_{t-1}\\
\mathrm{s}_{t-1}&amp;=f(\mathrm{net}_{t-1})\\
\end{align}</p>
<p>因此</p>
<p>\begin{align}
\frac{\partial{\mathrm{net}_t}}{\partial{\mathrm{net}_{t-1}}}&amp;=\frac{\partial{\mathrm{net}_t}}{\partial{\mathrm{s}_{t-1}}}\frac{\partial{\mathrm{s}_{t-1}}}{\partial{\mathrm{net}_{t-1}}}\\
\end{align}</p>
<p>我们用a表示列向量，用表示行向量。上式的第一项是向量函数对向量求导，其结果为Jacobian矩阵：</p>
<p>\begin{align}
\frac{\partial{\mathrm{net}_t}}{\partial{\mathrm{s}_{t-1}}}&amp;=
\begin{bmatrix}
\frac{\partial{net_1^t}}{\partial{s_1^{t-1}}}&amp; \frac{\partial{net_1^t}}{\partial{s_2^{t-1}}}&amp; ...&amp;  \frac{\partial{net_1^t}}{\partial{s_n^{t-1}}}\\
\frac{\partial{net_2^t}}{\partial{s_1^{t-1}}}&amp; \frac{\partial{net_2^t}}{\partial{s_2^{t-1}}}&amp; ...&amp;  \frac{\partial{net_2^t}}{\partial{s_n^{t-1}}}\\
&amp;.\\&amp;.\\
\frac{\partial{net_n^t}}{\partial{s_1^{t-1}}}&amp; \frac{\partial{net_n^t}}{\partial{s_2^{t-1}}}&amp; ...&amp;  \frac{\partial{net_n^t}}{\partial{s_n^{t-1}}}\\
\end{bmatrix}\\
&amp;=\begin{bmatrix}
w_{11} &amp; w_{12} &amp; ... &amp; w_{1n}\\
w_{21} &amp; w_{22} &amp; ... &amp; w_{2n}\\
&amp;.\\&amp;.\\
w_{n1} &amp; w_{n2} &amp; ... &amp; w_{nn}\\
\end{bmatrix}\\
&amp;=W
\end{align}</p>
<p>同理，上式第二项也是一个Jacobian矩阵：</p>
<p>\begin{align}
\frac{\partial{\mathrm{s}_{t-1}}}{\partial{\mathrm{net}_{t-1}}}&amp;=
\begin{bmatrix}
\frac{\partial{s_1^{t-1}}}{\partial{net_1^{t-1}}}&amp; \frac{\partial{s_1^{t-1}}}{\partial{net_2^{t-1}}}&amp; ...&amp;  \frac{\partial{s_1^{t-1}}}{\partial{net_n^{t-1}}}\\
\frac{\partial{s_2^{t-1}}}{\partial{net_1^{t-1}}}&amp; \frac{\partial{s_2^{t-1}}}{\partial{net_2^{t-1}}}&amp; ...&amp;  \frac{\partial{s_2^{t-1}}}{\partial{net_n^{t-1}}}\\
&amp;.\\&amp;.\\
\frac{\partial{s_n^{t-1}}}{\partial{net_1^{t-1}}}&amp; \frac{\partial{s_n^{t-1}}}{\partial{net_2^{t-1}}}&amp; ...&amp;  \frac{\partial{s_n^{t-1}}}{\partial{net_n^{t-1}}}\\
\end{bmatrix}\\
&amp;=\begin{bmatrix}
f'(net_1^{t-1}) &amp; 0 &amp; ... &amp; 0\\
0 &amp; f'(net_2^{t-1}) &amp; ... &amp; 0\\
&amp;.\\&amp;.\\
0 &amp; 0 &amp; ... &amp; f'(net_n^{t-1})\\
\end{bmatrix}\\
&amp;=diag[f'(\mathrm{net}_{t-1})]
\end{align}</p>
<p>其中，diag[a]表示根据向量a创建一个对角矩阵，即</p>
<p>$$
diag(\mathrm{a})=\begin{bmatrix}
a_1 &amp; 0 &amp; ... &amp; 0\\
0 &amp; a_2 &amp; ... &amp; 0\\
&amp;.\\&amp;.\\
0 &amp; 0 &amp; ... &amp; a_n\\
\end{bmatrix}\\
$$</p>
<p>最后，将两项合在一起，可得：</p>
<p>\begin{align}
\frac{\partial{\mathrm{net}_t}}{\partial{\mathrm{net}_{t-1}}}&amp;=\frac{\partial{\mathrm{net}_t}}{\partial{\mathrm{s}_{t-1}}}\frac{\partial{\mathrm{s}_{t-1}}}{\partial{\mathrm{net}_{t-1}}}\\
&amp;=Wdiag[f'(\mathrm{net}_{t-1})]\\
&amp;=\begin{bmatrix}
w_{11}f'(net_1^{t-1}) &amp; w_{12}f'(net_2^{t-1}) &amp; ... &amp; w_{1n}f(net_n^{t-1})\\
w_{21}f'(net_1^{t-1}) &amp; w_{22} f'(net_2^{t-1}) &amp; ... &amp; w_{2n}f(net_n^{t-1})\\
&amp;.\\&amp;.\\
w_{n1}f'(net_1^{t-1}) &amp; w_{n2} f'(net_2^{t-1}) &amp; ... &amp; w_{nn} f'(net_n^{t-1})\\
\end{bmatrix}\\
\end{align}</p>
<p>上式描述了将沿时间往前传递一个时刻的规律，有了这个规律，我们就可以求得任意时刻k的误差项$\delta_k$：</p>
<p>\begin{align}
\delta_k^T=&amp;\frac{\partial{E}}{\partial{\mathrm{net}_k}}\\
=&amp;\frac{\partial{E}}{\partial{\mathrm{net}_t}}\frac{\partial{\mathrm{net}_t}}{\partial{\mathrm{net}_k}}\\
=&amp;\frac{\partial{E}}{\partial{\mathrm{net}_t}}\frac{\partial{\mathrm{net}_t}}{\partial{\mathrm{net}_{t-1}}}\frac{\partial{\mathrm{net}_{t-1}}}{\partial{\mathrm{net}_{t-2}}}...\frac{\partial{\mathrm{net}_{k+1}}}{\partial{\mathrm{net}_{k}}}\\
=&amp;Wdiag[f'(\mathrm{net}_{t-1})]
Wdiag[f'(\mathrm{net}_{t-2})]
...
Wdiag[f'(\mathrm{net}_{k})]
\delta_t^l\\
=&amp;\delta_t^T\prod_{i=k}^{t-1}Wdiag[f'(\mathrm{net}_{i})]\qquad\label{eq:eq003}
\end{align}</p>
<p>式\ref{eq:eq003}就是将误差项沿时间反向传播的算法。</p>
<p><strong>循环层</strong>将<strong>误差项</strong>反向传递到上一层网络，与普通的<strong>全连接层</strong>是完全一样的，这在前面的文章零基础入门深度学习(3) - 神经网络和反向传播算法中已经详细讲过了，在此仅简要描述一下。</p>
<p>循环层的加权输入$net^l$与上一层$net^{l-1}$的加权输入关系如下：</p>
<p>\begin{align}
\mathrm{net}_t^l=&amp;U\mathrm{a}_t^{l-1}+W\mathrm{s}_{t-1}\\
\mathrm{a}_t^{l-1}=&amp;f^{l-1}(\mathrm{net}_t^{l-1})
\end{align}</p>
<p>上式中$net_t^l$是第l层神经元的<strong>加权输入</strong>(假设第l层是<strong>循环层</strong>)；$net_t^{l-1}$是第l-1层神经元的加权输入；$a_t^{l-1}$是第l-1层神经元的输出；$f^{l-1}$是第l-1层的<strong>激活函数</strong>。</p>
<p>\begin{align}
\frac{\partial{\mathrm{net}_t^l}}{\partial{\mathrm{net}_t^{l-1}}}=&amp;\frac{\partial{\mathrm{net}^l}}{\partial{\mathrm{a}_t^{l-1}}}\frac{\partial{\mathrm{a}_t^{l-1}}}{\partial{\mathrm{net}_t^{l-1}}}\\
=&amp;Udiag[f'^{l-1}(\mathrm{net}_t^{l-1})]
\end{align}</p>
<p>所以,</p>
<p>\begin{align}
(\delta_t^{l-1})^T=&amp;\frac{\partial{E}}{\partial{\mathrm{net}_t^{l-1}}}\\
=&amp;\frac{\partial{E}}{\partial{\mathrm{net}_t^l}}\frac{\partial{\mathrm{net}_t^l}}{\partial{\mathrm{net}_t^{l-1}}}\\
=&amp;(\delta_t^l)^TUdiag[f'^{l-1}(\mathrm{net}_t^{l-1})]\qquad \label{eq:eq004}
\end{align}</p>
<p>式\ref{eq:eq004}就是将误差项传递到上一层算法。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>$$
\nabla_{W_t}E=\begin{bmatrix}
\delta_1^ts_1^{t-1} &amp; \delta_1^ts_2^{t-1} &amp; ... &amp;  \delta_1^ts_n^{t-1}\\
\delta_2^ts_1^{t-1} &amp; \delta_2^ts_2^{t-1} &amp; ... &amp;  \delta_2^ts_n^{t-1}\\
.\\.\\
\delta_n^ts_1^{t-1} &amp; \delta_n^ts_2^{t-1} &amp; ... &amp;  \delta_n^ts_n^{t-1}\\
\end{bmatrix}\qquad \label{eq:eq0055}
$$</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="权重梯度的计算">权重梯度的计算<a class="anchor-link" href="#权重梯度的计算">¶</a></h3><p>现在，我们终于来到了BPTT算法的最后一步：计算每个权重的梯度。</p>
<p>首先，我们计算误差函数E对权重矩阵W的梯度$\frac{\partial{E}}{\partial{W}}$。</p>
<p><img alt="梯度的计算" src="http://upload-images.jianshu.io/upload_images/2256672-f7d034c8f05812f7.png"/></p>
<p>上图展示了我们到目前为止，在前两步中已经计算得到的量，包括每个时刻t 循环层的输出值$s_t$，以及误差项$\delta_t$。</p>
<p>回忆一下我们在文章零基础入门深度学习(3) - 神经网络和反向传播算法介绍的全连接网络的权重梯度计算算法：只要知道了任意一个时刻的误差项$\delta_t$，以及上一个时刻循环层的输出值$s_{t-1}$，就可以按照下面的公式求出权重矩阵在t时刻的梯度$\nabla_{Wt}E$：</p>
<p>\begin{align}
\nabla_{W_t}E=\begin{bmatrix}
\delta_1^ts_1^{t-1} &amp; \delta_1^ts_2^{t-1} &amp; ... &amp;  \delta_1^ts_n^{t-1}\\
\delta_2^ts_1^{t-1} &amp; \delta_2^ts_2^{t-1} &amp; ... &amp;  \delta_2^ts_n^{t-1}\\
.\\.\\
\delta_n^ts_1^{t-1} &amp; \delta_n^ts_2^{t-1} &amp; ... &amp;  \delta_n^ts_n^{t-1}\\
\end{bmatrix} \label{eq:eq005}
\end{align}</p>
<p>在式\ref{eq:eq005}中，$\delta_i^t$表示t时刻误差项向量的第i个分量；$s_i^{t-1}$表示t-1时刻循环层第i个神经元的输出值。</p>
<p>我们下面可以简单推导一下式\ref{eq:eq005}。</p>
<p>我们知道：</p>
<p>\begin{align}
\mathrm{net}_t=&amp;U\mathrm{x}_t+W\mathrm{s}_{t-1}\\
\begin{bmatrix}
net_1^t\\
net_2^t\\
.\\.\\
net_n^t\\
\end{bmatrix}=&amp;U\mathrm{x}_t+
\begin{bmatrix}
w_{11} &amp; w_{12} &amp; ... &amp; w_{1n}\\
w_{21} &amp; w_{22} &amp; ... &amp; w_{2n}\\
.\\.\\
w_{n1} &amp; w_{n2} &amp; ... &amp; w_{nn}\\
\end{bmatrix}
\begin{bmatrix}
s_1^{t-1}\\
s_2^{t-1}\\
.\\.\\
s_n^{t-1}\\
\end{bmatrix}\\
=&amp;U\mathrm{x}_t+
\begin{bmatrix}
w_{11}s_1^{t-1}+w_{12}s_2^{t-1}...w_{1n}s_n^{t-1}\\
w_{21}s_1^{t-1}+w_{22}s_2^{t-1}...w_{2n}s_n^{t-1}\\
.\\.\\
w_{n1}s_1^{t-1}+w_{n2}s_2^{t-1}...w_{nn}s_n^{t-1}\\
\end{bmatrix}\\
\end{align}</p>
<p>因为对W求导与$U\mathrm{x}_t$无关，我们不再考虑。现在，我们考虑对权重项$w_{ji}$求导。通过观察上式我们可以看到$w_{ji}$只与$net_j^t$有关，所以：</p>
<p>\begin{align}
\frac{\partial{E}}{\partial{w_{ji}}}=&amp;\frac{\partial{E}}{\partial{net_j^t}}\frac{\partial{net_j^t}}{\partial{w_{ji}}}\\
=&amp;\delta_j^ts_i^{t-1}
\end{align}</p>
<p>按照上面的规律就可以生成式\ref{eq:eq005}里面的矩阵。</p>
<p>我们已经求得了权重矩阵W在t时刻的梯度$\nabla_{Wt}E$，最终的梯度$\nabla_{W}E$是各个时刻的梯度之和：</p>
<p>\begin{align}
\nabla_WE=&amp;\sum_{i=1}^t\nabla_{W_i}E\\
=&amp;\begin{bmatrix}
\delta_1^ts_1^{t-1} &amp; \delta_1^ts_2^{t-1} &amp; ... &amp;  \delta_1^ts_n^{t-1}\\
\delta_2^ts_1^{t-1} &amp; \delta_2^ts_2^{t-1} &amp; ... &amp;  \delta_2^ts_n^{t-1}\\
.\\.\\
\delta_n^ts_1^{t-1} &amp; \delta_n^ts_2^{t-1} &amp; ... &amp;  \delta_n^ts_n^{t-1}\\
\end{bmatrix}
+...+
\begin{bmatrix}
\delta_1^1s_1^0 &amp; \delta_1^1s_2^0 &amp; ... &amp;  \delta_1^1s_n^0\\
\delta_2^1s_1^0 &amp; \delta_2^1s_2^0 &amp; ... &amp;  \delta_2^1s_n^0\\
.\\.\\
\delta_n^1s_1^0 &amp; \delta_n^1s_2^0 &amp; ... &amp;  \delta_n^1s_n^0\\
\end{bmatrix}\qquad \label{eq:eq006}
\end{align}</p>
<p>式\ref{eq:eq006}就是计算循环层权重矩阵W的梯度的公式。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>----------数学公式超高能预警----------</p>
<p>前面已经介绍了$\nabla_WE$的计算方法，看上去还是比较直观的。然而，读者也许会困惑，为什么最终的梯度是各个时刻的梯度之和呢？我们前面只是直接用了这个结论，实际上这里面是有道理的，只是这个数学推导比较绕脑子。感兴趣的同学可以仔细阅读接下来这一段，它用到了矩阵对矩阵求导、张量与向量相乘运算的一些法则。</p>
<p>我们还是从这个式子开始：</p>
<p>$$\mathrm{net}_t=U\mathrm{x}_t+Wf(\mathrm{net}_{t-1})$$</p>
<p>因为$U\mathrm{x}_t$与W完全无关，我们把它看做常量。现在，考虑第一个式子加号右边的部分，因为W和$f(\mathrm{net}_{t-1})$都是W的函数，因此我们要用到大学里面都学过的导数乘法运算：</p>
<p>$$(uv)'=u'v+uv'$$</p>
<p>因此，上面第一个式子写成：</p>
<p>$$
\frac{\partial{\mathrm{net}_t}}{\partial{W}}=\frac{\partial{W}}{\partial{W}}f(\mathrm{net}_{t-1})+W\frac{\partial{f(\mathrm{net}_{t-1})}}{\partial{W}}
$$</p>
<p>我们最终需要计算的$\nabla_WE$是：</p>
<p>\begin{align}
\nabla_WE=&amp;\frac{\partial{E}}{\partial{W}}\\
=&amp;\frac{\partial{E}}{\partial{\mathrm{net}_t}}\frac{\partial{\mathrm{net}_t}}{\partial{W}}\\
=&amp;\delta_t^T\frac{\partial{W}}{\partial{W}}f(\mathrm{net}_{t-1})+ \delta_t^TW\frac{\partial{f(\mathrm{net}_{t-1})}}{\partial{W}}\qquad \label{eq:eq007}
\end{align}</p>
<p>我们先计算式\ref{eq:eq007}加号左边的部分。$\frac{\partial{W}}{\partial{W}}$是矩阵对矩阵求导，其结果是一个四维张量(tensor)，如下所示：</p>
<p>\begin{align}
\frac{\partial{W}}{\partial{W}}=&amp;
\begin{bmatrix}
\frac{\partial{w_{11}}}{\partial{W}} &amp; \frac{\partial{w_{12}}}{\partial{W}} &amp; ... &amp; \frac{\partial{w_{1n}}}{\partial{W}}\\
\frac{\partial{w_{21}}}{\partial{W}} &amp; \frac{\partial{w_{22}}}{\partial{W}} &amp; ... &amp; \frac{\partial{w_{2n}}}{\partial{W}}\\
.\\.\\
\frac{\partial{w_{n1}}}{\partial{W}} &amp; \frac{\partial{w_{n2}}}{\partial{W}} &amp; ... &amp; \frac{\partial{w_{nn}}}{\partial{W}}\\
\end{bmatrix}\\
=&amp;
\begin{bmatrix}
\begin{bmatrix}
\frac{\partial{w_{11}}}{\partial{w_{11}}} &amp; \frac{\partial{w_{11}}}{\partial{w_{12}}} &amp; ... &amp; \frac{\partial{w_{11}}}{\partial{_{1n}}}\\
\frac{\partial{w_{11}}}{\partial{w_{21}}} &amp; \frac{\partial{w_{11}}}{\partial{w_{22}}} &amp; ... &amp; \frac{\partial{w_{11}}}{\partial{_{2n}}}\\
.\\.\\
\frac{\partial{w_{11}}}{\partial{w_{n1}}} &amp; \frac{\partial{w_{11}}}{\partial{w_{n2}}} &amp; ... &amp; \frac{\partial{w_{11}}}{\partial{_{nn}}}\\
\end{bmatrix} &amp;
\begin{bmatrix}
\frac{\partial{w_{12}}}{\partial{w_{11}}} &amp; \frac{\partial{w_{12}}}{\partial{w_{12}}} &amp; ... &amp; \frac{\partial{w_{12}}}{\partial{_{1n}}}\\
\frac{\partial{w_{12}}}{\partial{w_{21}}} &amp; \frac{\partial{w_{12}}}{\partial{w_{22}}} &amp; ... &amp; \frac{\partial{w_{12}}}{\partial{_{2n}}}\\
.\\.\\
\frac{\partial{w_{12}}}{\partial{w_{n1}}} &amp; \frac{\partial{w_{12}}}{\partial{w_{n2}}} &amp; ... &amp; \frac{\partial{w_{12}}}{\partial{_{nn}}}\\
\end{bmatrix}&amp;...\\
.\\.\\
\end{bmatrix}\\
=&amp;
\begin{bmatrix}
\begin{bmatrix}
1 &amp; 0 &amp; ... &amp; 0\\
0 &amp; 0 &amp; ... &amp; 0\\
.\\.\\
0 &amp; 0 &amp; ... &amp; 0\\
\end{bmatrix} &amp;
\begin{bmatrix}
0 &amp; 1 &amp; ... &amp; 0\\
0 &amp; 0 &amp; ... &amp; 0\\
.\\.\\
0 &amp; 0 &amp; ... &amp; 0\\
\end{bmatrix}&amp;...\\
.\\.\\
\end{bmatrix}\\
\end{align}</p>
<p>接下来，我们知道$s_{t-1}=f({\mathrm{net}_{t-1}})$，它是一个列向量。我们让上面的四维张量与这个向量相乘，得到了一个三维张量，再左乘行向量$\delta_t^T$，最终得到一个矩阵：</p>
<p>\begin{align}
\delta_t^T\frac{\partial{W}}{\partial{W}}f({\mathrm{net}_{t-1}})=&amp;
\delta_t^T\frac{\partial{W}}{\partial{W}}{\mathrm{s}_{t-1}}\\
=&amp;\delta_t^T
\begin{bmatrix}
\begin{bmatrix}
1 &amp; 0 &amp; ... &amp; 0\\
0 &amp; 0 &amp; ... &amp; 0\\
.\\.\\
0 &amp; 0 &amp; ... &amp; 0\\
\end{bmatrix} &amp;
\begin{bmatrix}
0 &amp; 1 &amp; ... &amp; 0\\
0 &amp; 0 &amp; ... &amp; 0\\
.\\.\\
0 &amp; 0 &amp; ... &amp; 0\\
\end{bmatrix}&amp;...\\
.\\.\\
\end{bmatrix}
\begin{bmatrix}
s_1^{t-1}\\
s_2^{t-1}\\
.\\.\\
s_n^{t-1}\\
\end{bmatrix}\\
=&amp;\delta_t^T
\begin{bmatrix}
\begin{bmatrix}
s_1^{t-1}\\
0\\
.\\.\\
0\\
\end{bmatrix} &amp;
\begin{bmatrix}
s_2^{t-1}\\
0\\
.\\.\\
0\\
\end{bmatrix}&amp;...\\
.\\.\\
\end{bmatrix}\\
=&amp;
\begin{bmatrix}
\delta_1^t &amp; \delta_2^t &amp; ... &amp;\delta_n^t
\end{bmatrix}
\begin{bmatrix}
\begin{bmatrix}
s_1^{t-1}\\
0\\
.\\.\\
0\\
\end{bmatrix} &amp;
\begin{bmatrix}
s_2^{t-1}\\
0\\
.\\.\\
0\\
\end{bmatrix}&amp;...\\
.\\.\\
\end{bmatrix}\\
=&amp;
\begin{bmatrix}
\delta_1^ts_1^{t-1} &amp; \delta_1^ts_2^{t-1} &amp; ... &amp;  \delta_1^ts_n^{t-1}\\
\delta_2^ts_1^{t-1} &amp; \delta_2^ts_2^{t-1} &amp; ... &amp;  \delta_2^ts_n^{t-1}\\
.\\.\\
\delta_n^ts_1^{t-1} &amp; \delta_n^ts_2^{t-1} &amp; ... &amp;  \delta_n^ts_n^{t-1}\\
\end{bmatrix}\\
=&amp;\nabla_{Wt}E
\end{align}</p>
<p>接下来，我们计算式\ref{eq:eq007}加号右边的部分：</p>
<p>\begin{align}
\delta_t^TW\frac{\partial{f(\mathrm{net}_{t-1})}}{\partial{W}}=&amp;
\delta_t^TW\frac{\partial{f(\mathrm{net}_{t-1})}}{\partial{\mathrm{net}_{t-1}}}\frac{\partial{\mathrm{net}_{t-1}}}{\partial{W}}\\
=&amp;\delta_t^TWf'(\mathrm{net}_{t-1})\frac{\partial{\mathrm{net}_{t-1}}}{\partial{W}}\\
=&amp;\delta_t^T\frac{\partial{\mathrm{net}_t}}{\partial{\mathrm{net}_{t-1}}}\frac{\partial{\mathrm{net}_{t-1}}}{\partial{W}}\\
=&amp;\delta_{t-1}^T\frac{\partial{\mathrm{net}_{t-1}}}{\partial{W}}\\
\end{align}</p>
<p>于是，我们得到了如下递推公式：</p>
<p>\begin{align}
\nabla_WE=&amp;\frac{\partial{E}}{\partial{W}}\\
=&amp;\frac{\partial{E}}{\partial{\mathrm{net}_t}}\frac{\partial{\mathrm{net}_t}}{\partial{W}}\\
=&amp;\nabla_{Wt}E+\delta_{t-1}^T\frac{\partial{\mathrm{net}_{t-1}}}{\partial{W}}\\
=&amp;\nabla_{Wt}E+\nabla_{Wt-1}E+\delta_{t-2}^T\frac{\partial{\mathrm{net}_{t-2}}}{\partial{W}}\\
=&amp;\nabla_{Wt}E+\nabla_{Wt-1}E+...+\nabla_{W1}E\\
=&amp;\sum_{k=1}^t\nabla_{Wk}E
\end{align}</p>
<p>这样，我们就证明了：最终的梯度$\nabla_WE$是各个时刻的梯度之和。</p>
<p>----------数学公式超高能预警解除----------</p>
<p>同权重矩阵W类似，我们可以得到权重矩阵U的计算方法。</p>
<p>$$
\nabla_{U_t}E=\begin{bmatrix}
\delta_1^tx_1^t &amp; \delta_1^tx_2^t &amp; ... &amp;  \delta_1^tx_m^t\\
\delta_2^tx_1^t &amp; \delta_2^tx_2^t &amp; ... &amp;  \delta_2^tx_m^t\\
.\\.\\
\delta_n^tx_1^t &amp; \delta_n^tx_2^t &amp; ... &amp;  \delta_n^tx_m^t\\
\end{bmatrix}\qquad\label{eq:eq008}
$$</p>
<p>式\ref{eq:eq008}是误差函数在t时刻对权重矩阵U的梯度。和权重矩阵W一样，最终的梯度也是各个时刻的梯度之和：</p>
<p>$$\nabla_UE=\sum_{i=1}^t\nabla_{U_i}E$$</p>
<p>具体的证明这里就不再赘述了，感兴趣的读者可以练习推导一下。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="RNN的梯度爆炸和消失问题">RNN的梯度爆炸和消失问题<a class="anchor-link" href="#RNN的梯度爆炸和消失问题">¶</a></h3><p>不幸的是，实践中前面介绍的几种RNNs并不能很好的处理较长的序列。一个主要的原因是，RNN在训练中很容易发生梯度爆炸和梯度消失，这导致训练时梯度不能在较长序列中一直传递下去，从而使RNN无法捕捉到长距离的影响。</p>
<p>为什么RNN会产生梯度爆炸和消失问题呢？我们接下来将详细分析一下原因。我们根据式\ref{eq:eq003}可得：</p>
<p>\begin{align}
\delta_k^T=&amp;\delta_t^T\prod_{i=k}^{t-1}Wdiag[f'(\mathrm{net}_{i})]\\
\|\delta_k^T\|\leqslant&amp;\|\delta_t^T\|\prod_{i=k}^{t-1}\|W\|\|diag[f'(\mathrm{net}_{i})]\|\\
\leqslant&amp;\|\delta_t^T\|(\beta_W\beta_f)^{t-k}
\end{align}</p>
<p>上式的$\beta$定义为矩阵的模的上界。因为上式是一个指数函数，如果t-k很大的话（也就是向前看很远的时候），会导致对应的误差项的值增长或缩小的非常快，这样就会导致相应的梯度爆炸和梯度消失问题（取决于$\beta$大于1还是小于1）。</p>
<p>通常来说，梯度爆炸更容易处理一些。因为梯度爆炸的时候，我们的程序会收到NaN错误。我们也可以设置一个梯度阈值，当梯度超过这个阈值的时候可以直接截取。</p>
<p>梯度消失更难检测，而且也更难处理一些。总的来说，我们有三种方法应对梯度消失问题：</p>
<ul>
<li>合理的初始化权重值。初始化权重，使每个神经元尽可能不要取极大或极小值，以躲开梯度消失的区域。</li>
<li>使用relu代替sigmoid和tanh作为激活函数。原理请参考上一篇文章零基础入门深度学习(4) - 卷积神经网络的激活函数一节。</li>
<li>使用其他结构的RNNs，比如长短时记忆网络（LTSM）和Gated Recurrent Unit（GRU），这是最流行的做法。我们将在以后的文章中介绍这两种网络。</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="RNN的应用举例——基于RNN的语言模型">RNN的应用举例——基于RNN的语言模型<a class="anchor-link" href="#RNN的应用举例——基于RNN的语言模型">¶</a></h2><p>现在，我们介绍一下基于RNN语言模型。我们首先把词依次输入到循环神经网络中，每输入一个词，循环神经网络就输出截止到目前为止，下一个最可能的词。例如，当我们依次输入：</p>
<blockquote><p>我 昨天 上学 迟到 了</p>
</blockquote>
<p>神经网络的输出如下图所示：
<img alt="基于RNN的语言模型" src="http://upload-images.jianshu.io/upload_images/2256672-a69765380a75f860.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/480"/></p>
<p>其中，s和e是两个特殊的词，分别表示一个序列的开始和结束。</p>
<h3 id="向量化">向量化<a class="anchor-link" href="#向量化">¶</a></h3><p>我们知道，神经网络的输入和输出都是向量，为了让语言模型能够被神经网络处理，我们必须把词表达为向量的形式，这样神经网络才能处理它。</p>
<p>神经网络的输入是词，我们可以用下面的步骤对输入进行向量化：</p>
<p>建立一个包含所有词的词典，每个词在词典里面有一个唯一的编号。
任意一个词都可以用一个N维的one-hot向量来表示。其中，N是词典中包含的词的个数。假设一个词在词典中的编号是i，v是表示这个词的向量，$v_j$是向量的第j个元素，则：</p>
<p>$$ v_j=\begin{cases}1\qquad\qquad j=i\\0 \qquad\qquad j\ne i\end{cases} $$</p>
<p>上面这个公式的含义，可以用下面的图来直观的表示：</p>
<p><img alt="nothing" src="http://upload-images.jianshu.io/upload_images/2256672-14ae8b4f92e90c5c.png"/></p>
<p>使用这种向量化方法，我们就得到了一个高维、稀疏的向量（稀疏是指绝大部分元素的值都是0）。处理这样的向量会导致我们的神经网络有很多的参数，带来庞大的计算量。因此，往往会需要使用一些降维方法，将高维的稀疏向量转变为低维的稠密向量。不过这个话题我们就不再这篇文章中讨论了。</p>
<p>语言模型要求的输出是下一个最可能的词，我们可以让循环神经网络计算计算词典中每个词是下一个词的概率，这样，概率最大的词就是下一个最可能的词。因此，神经网络的输出向量也是一个N维向量，向量中的每个元素对应着词典中相应的词是下一个词的概率。如下图所示：</p>
<p><img alt="nothing" src="http://upload-images.jianshu.io/upload_images/2256672-3e1562c7031309f1.png"/></p>
<h3 id="Softmax层">Softmax层<a class="anchor-link" href="#Softmax层">¶</a></h3><p>前面提到，语言模型是对下一个词出现的概率进行建模。那么，怎样让神经网络输出概率呢？方法就是用softmax层作为神经网络的输出层。</p>
<p>我们先来看一下softmax函数的定义：</p>
<p>$$g(z_i)=\frac{e^{z_i}}{\sum_{k}e^{z_k}}$$</p>
<p>这个公式看起来可能很晕，我们举一个例子。Softmax层如下图所示：</p>
<p><img alt="nothing" src="http://upload-images.jianshu.io/upload_images/2256672-5a3219fab80ab45f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/360"/></p>
<p>从上图我们可以看到，softmax layer的输入是一个向量，输出也是一个向量，两个向量的维度是一样的（在这个例子里面是4）。输入向量x=[1 2 3 4]经过softmax层之后，经过上面的softmax函数计算，转变为输出向量y=[0.03 0.09 0.24 0.64]。计算过程为：</p>
<p>\begin{align}
y_1&amp;=\frac{e^{x_1}}{\sum_{k}e^{x_k}}\\
&amp;=\frac{e^1}{e^1+e^2+e^3+e^4}\\
&amp;=0.03\\
y_2&amp;=\frac{e^2}{e^1+e^2+e^3+e^4}\\
&amp;=0.09\\
y_3&amp;=\frac{e^3}{e^1+e^2+e^3+e^4}\\
&amp;=0.24\\
y_4&amp;=\frac{e^4}{e^1+e^2+e^3+e^4}\\
&amp;=0.64\\
\end{align}</p>
<p>我们来看看输出向量y的特征：</p>
<ul>
<li>每一项为取值为0-1之间的正数；</li>
<li>所有项的总和是1。
我们不难发现，这些特征和概率的特征是一样的，因此我们可以把它们看做是概率。对于语言模型来说，我们可以认为模型预测下一个词是词典中第一个词的概率是0.03，是词典中第二个词的概率是0.09，以此类推。</li>
</ul>
<h3 id="语言模型的训练">语言模型的训练<a class="anchor-link" href="#语言模型的训练">¶</a></h3><p>可以使用监督学习的方法对语言模型进行训练，首先，需要准备训练数据集。接下来，我们介绍怎样把语料</p>
<blockquote><p>我 昨天 上学 迟到 了</p>
</blockquote>
<p>转换成语言模型的训练数据集。</p>
<p>首先，我们获取输入-标签对：</p>
<table>
<thead><tr>
<th style="text-align:center">输入</th>
<th style="text-align:center">标签</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">s</td>
<td style="text-align:center">我</td>
</tr>
<tr>
<td style="text-align:center">我</td>
<td style="text-align:center">昨天</td>
</tr>
<tr>
<td style="text-align:center">昨天</td>
<td style="text-align:center">上学</td>
</tr>
<tr>
<td style="text-align:center">上学</td>
<td style="text-align:center">迟到</td>
</tr>
<tr>
<td style="text-align:center">迟到</td>
<td style="text-align:center">了</td>
</tr>
<tr>
<td style="text-align:center">了</td>
<td style="text-align:center">e</td>
</tr>
</tbody>
</table>
<p>然后，使用前面介绍过的向量化方法，对输入x和标签y进行向量化。这里面有意思的是，对标签y进行向量化，其结果也是一个one-hot向量。例如，我们对标签『我』进行向量化，得到的向量中，只有第2019个元素的值是1，其他位置的元素的值都是0。它的含义就是下一个词是『我』的概率是1，是其它词的概率都是0。</p>
<p>最后，我们使用交叉熵误差函数作为优化目标，对模型进行优化。</p>
<p>在实际工程中，我们可以使用大量的语料来对模型进行训练，获取训练数据和训练的方法都是相同的。</p>
<h3 id="交叉熵误差">交叉熵误差<a class="anchor-link" href="#交叉熵误差">¶</a></h3><p>一般来说，当神经网络的输出层是softmax层时，对应的误差函数E通常选择交叉熵误差函数，其定义如下：</p>
<p>$$L(y,o)=-\frac{1}{N}\sum_{n\in{N}}{y_nlogo_n}$$</p>
<p>在上式中，N是训练样本的个数，向量$y_n$是样本的标记，向量$o_n$是网络的输出。标记$y_n$是一个one-hot向量，例如$y_1=[1,0,0,0]$，如果网络的输出$o=[0.03,0.09,0.24,0.64]$，那么，交叉熵误差是（假设只有一个训练样本，即N=1）：</p>
<p>\begin{align}
L&amp;=-\frac{1}{N}\sum_{n\in{N}}{y_nlogo_n}\\
&amp;=-y_1logo_1\\
&amp;=-(1*log0.03+0*log0.09+0*log0.24+0*log0.64)\\
&amp;=3.51
\end{align}</p>
<p>我们当然可以选择其他函数作为我们的误差函数，比如最小平方误差函数(MSE)。不过对概率进行建模时，选择交叉熵误差函数更make sense。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="RNN的实现">RNN的实现<a class="anchor-link" href="#RNN的实现">¶</a></h2><blockquote><p>完整代码请参考GitHub: <a href="https://github.com/hanbt/learn_dl/blob/master/rnn.py">https://github.com/hanbt/learn_dl/blob/master/rnn.py</a> (python2.7)</p>
</blockquote>
<p>为了加深我们对前面介绍的知识的理解，我们来动手实现一个RNN层。我们复用了上一篇文章零基础入门深度学习(4) - 卷积神经网络中的一些代码，所以先把它们导入进来。</p>
</div>
</div>
</div>
</body>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['$','$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        " linebreaks: { automatic: true, width: '95% container' }, " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
</div>


            <footer>
<span class="byline author vcard">
    Posted by
    <span class="fn">
            niult
    </span>
</span><time datetime="2018-01-01T00:00:00+08:00" pubdate>2018-01-01 00:00</time><span class="categories">
        <a href="../../../category/ling ji chu ru men shen du xue xi.html">零基础入门深度学习</a>
</span>

    <span class="categories">
            <a class="category" href="../../../tag/python.html">python</a>, 
            <a class="category" href="../../../tag/shen-du-xue-xi.html">深度学习</a>, 
            <a class="category" href="../../../tag/xun-huan-shen-jing-wang-luo.html">循环神经网络</a>
    </span>

<div class="sharing">
</div>            </footer>
        </article>

    </div>
<aside class="sidebar">
    <section>
        <h1>Recent Posts</h1>
        <ul id="recent_posts">
                <li class="post">
                    <a href="../../../pages/2019/01/21-a-first-look-at-a-neural-network.html">2.1-a-first-look-at-a-neural-network</a>
                </li>
                <li class="post">
                    <a href="../../../pages/2019/01/35-classifying-movie-reviews.html">3.5-classifying-movie-reviews</a>
                </li>
                <li class="post">
                    <a href="../../../pages/2019/01/36-classifying-newswires.html">3.6-classifying-newswires</a>
                </li>
                <li class="post">
                    <a href="../../../pages/2019/01/37-predicting-house-prices.html">3.7-predicting-house-prices</a>
                </li>
                <li class="post">
                    <a href="../../../pages/2019/01/44-overfitting-and-underfitting.html">4.4-overfitting-and-underfitting</a>
                </li>
        </ul>
    </section>
        <section>

            <h1>Categories</h1>
            <ul id="recent_posts">
                    <li><a href="../../../category/01chang yong gong ju.html">01常用工具</a></li>
                    <li><a href="../../../category/02.wo ai du shu.html">02.我爱读书</a></li>
                    <li><a href="../../../category/algorithms.html">algorithms</a></li>
                    <li><a href="../../../category/book.html">book</a></li>
                    <li><a href="../../../category/book-pydata.html">book-pydata</a></li>
                    <li><a href="../../../category/deep-learning-with-python.html">deep-learning-with-python</a></li>
                    <li><a href="../../../category/ji qi xue xi shi zhan.html">机器学习实战</a></li>
                    <li><a href="../../../category/ke wai du wu.html">课外读物</a></li>
                    <li><a href="../../../category/ling ji chu ru men shen du xue xi.html">零基础入门深度学习</a></li>
                    <li><a href="../../../category/shen du xue xi.html">深度学习</a></li>
                    <li><a href="../../../category/shen jing wang luo.html">神经网络</a></li>
                    <li><a href="../../../category/shu ju wa jue.html">数据挖掘</a></li>
                    <li><a href="../../../category/shu xue ji chu.html">数学基础</a></li>
                    <li><a href="../../../category/tf-example.html">tf-example</a></li>
                    <li><a href="../../../category/tool1.html">tool1</a></li>
                    <li><a href="../../../category/tool2.html">tool2</a></li>
                    <li><a href="../../../category/tools.html">tools</a></li>
                    <li><a href="../../../category/tui jian xi tong.html">推荐系统</a></li>
                    <li><a href="../../../category/wen ben wa jue.html">文本挖掘</a></li>
            </ul>
        </section>


    <section>
        <h1>Tags</h1>
            <a href="../../../tag/python.html">python</a>, 
            <a href="../../../tag/numpy.html">numpy</a>, 
            <a href="../../../tag/deep-learning.html">deep-learning</a>, 
            <a href="../../../tag/algorithms.html">algorithms</a>, 
            <a href="../../../tag/wen-ben-wa-jue.html">文本挖掘</a>, 
            <a href="../../../tag/shen-jing-wang-luo.html">神经网络</a>, 
            <a href="../../../tag/shu-xue-ji-chu.html">数学基础</a>, 
            <a href="../../../tag/nlp.html">nlp</a>, 
            <a href="../../../tag/tf-example.html">tf-example</a>, 
            <a href="../../../tag/tui-jian-xi-tong.html">推荐系统</a>, 
            <a href="../../../tag/tf.html">tf</a>, 
            <a href="../../../tag/ji-huo-han-shu.html">激活函数</a>, 
            <a href="../../../tag/mapreduce.html">mapreduce</a>, 
            <a href="../../../tag/spark.html">spark</a>, 
            <a href="../../../tag/handbook.html">handbook</a>, 
            <a href="../../../tag/matplotlib.html">matplotlib</a>, 
            <a href="../../../tag/scikit-learn.html">scikit-learn</a>, 
            <a href="../../../tag/latex.html">latex</a>, 
            <a href="../../../tag/pandas.html">pandas</a>, 
            <a href="../../../tag/jupyter.html">jupyter</a>, 
            <a href="../../../tag/plot.html">plot</a>, 
            <a href="../../../tag/pip.html">pip</a>, 
            <a href="../../../tag/geng-xin-suo-you-mo-kuai.html">更新所有模块</a>, 
            <a href="../../../tag/shen-du-xue-xi.html">深度学习</a>, 
            <a href="../../../tag/xun-huan-shen-jing-wang-luo.html">循环神经网络</a>, 
            <a href="../../../tag/pangrank.html">PangRank</a>, 
            <a href="../../../tag/book.html">book</a>, 
            <a href="../../../tag/pydata.html">pydata</a>, 
            <a href="../../../tag/shell.html">shell</a>, 
            <a href="../../../tag/pyhton.html">pyhton</a>
    </section>



    <section>
        <h1>GitHub Repos</h1>
            <a href="https://github.com/1007530194">@1007530194</a> on GitHub
    </section>

</aside>    </div>
</div>
<footer role="contentinfo"><p>
        活到老，学到老，玩到老
    <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>

    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-132396898-1', 'auto');

    ga('require', 'displayfeatures');
    ga('send', 'pageview');
    </script>
</body>
</html>