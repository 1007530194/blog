<!DOCTYPE html>
<html lang="en">
  

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width,minimum-scale=1">

  <title>Search the site</title>
  <meta name="description" content="This is my book.">

  <link rel="canonical" href="https://jupyter.org/jupyter-book/search">
  <link rel="alternate" type="application/rss+xml" title="MyBook" href="https://jupyter.org/jupyter-book/feed.xml">

  <meta property="og:url"         content="https://jupyter.org/jupyter-book/search" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Search the site" />
<meta property="og:description" content="This is my book." />
<meta property="og:image"       content="" />


  <script type="application/ld+json">
  {
  "@context": "http://schema.org",
  "@type": "NewsArticle",
  "mainEntityOfPage":
    "https://jupyter.org/jupyter-book/search",
  "headline":
    "Search the site",
  "datePublished":
    "2019-06-10T17:08:49+08:00",
  "dateModified":
    "2019-06-10T17:08:49+08:00",
  "description":
    "This is my book.",
  "author": {
    "@type": "Person",
    "name": "niult"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Data 100 at UC Berkeley",
    "logo": {
      "@type": "ImageObject",
      "url": "https://jupyter.org/jupyter-book",
      "width": 60,
      "height": 60
    }
  },
  "image": {
    "@type": "ImageObject",
    "url": "https://jupyter.org/jupyter-book",
    "height": 60,
    "width": 60
  }
}

  </script>
  <link rel="stylesheet" href="/jupyter-book/assets/css/styles.css">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css ">
  <link rel="apple-touch-icon" sizes="57x57" href="/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon-180x180.png">

  <!-- <link rel="manifest" href="/manifest.json"> -->
  <!-- <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#efae0a"> -->
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/mstile-144x144.png">
  <meta name="theme-color" content="#233947">

  <!-- Favicon -->
  <link rel="shortcut icon" type="image/x-icon" href="/jupyter-book/images/logo/favicon.ico">

  <!-- MathJax Config -->
  <!-- Allow inline math using $ and automatically break long math lines -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true,
        processEnvironments: true
    },
    CommonHTML: {
        linebreaks: {
            automatic: true,
        },
    },
});
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML' async></script>

  <!-- DOM updating function -->
  <script>
const runWhenDOMLoaded = cb => {
  if (document.readyState != 'loading') {
    cb()
  } else if (document.addEventListener) {
    document.addEventListener('DOMContentLoaded', cb)
  } else {
    document.attachEvent('onreadystatechange', function() {
      if (document.readyState == 'complete') cb()
    })
  }
}

// Helper function to init things quickly
initFunction = function(myfunc) {
  runWhenDOMLoaded(myfunc);
  document.addEventListener('turbolinks:load', myfunc);
};
</script>

  <!-- Define some javascript variables that will be useful in other javascript -->
  <script>
    const site_basename = '/jupyter-book';
  </script>

  <!-- Add AnchorJS to let headers be linked -->
  <script src="/jupyter-book/assets/js/anchor.min.js"  type="text/javascript"></script>
  <script>

initFunction(function () {
    anchors.add("main h1, main h2, main h3, main h4")
});

</script>

  <!-- Include Turbolinks to make page loads fast -->
  <!-- https://github.com/turbolinks/turbolinks -->
  <script src="/jupyter-book/assets/js/turbolinks.js" async></script>
  <meta name="turbolinks-cache-control" content="no-cache">

  <!-- Load nbinteract for widgets -->
  

  <!-- Load Thebelab for interactive widgets -->
  <!-- Include Thebelab for interactive code if it's enabled -->


<!-- Display Thebelab button in each code cell -->
<script>
/**
 * Set up thebelab button for code blocks
 */

const thebelabCellButton = id =>
  `<a id="thebelab-cell-button-${id}" class="btn thebebtn o-tooltip--left" data-tooltip="Interactive Mode">
    <img src="/jupyter-book/assets/images/edit-button.svg" alt="Start interactive mode">
  </a>`


const addThebelabButtonToCodeCells =  () => {

  const codeCells = document.querySelectorAll('div.input_area > div.highlighter-rouge:not(.output) pre')
  codeCells.forEach((codeCell, index) => {
    const id = codeCellId(index)
    codeCell.setAttribute('id', id)
    if (document.getElementById("thebelab-cell-button-" + id) == null) {
      codeCell.insertAdjacentHTML('afterend', thebelabCellButton(id));
    }
  })
}

initFunction(addThebelabButtonToCodeCells);
</script>



<script type="text/x-thebe-config">
    {
      requestKernel: true,
      binderOptions: {
        repo: 'jupyter/jupyter-book',
        ref: 'gh-pages',
      },
      codeMirrorConfig: {
        theme: "abcdef"
      },
      kernelOptions: {
        name: 'python3',
      }
    }
</script>
<script src="https://unpkg.com/thebelab@0.4.0/lib/index.js"></script>
<script>
    /**
     * Add attributes to Thebelab blocks
     */

    const initThebelab = () => {
        const addThebelabToCodeCells = () => {
            console.log("Adding thebelab to code cells...");
            // If Thebelab hasn't loaded, wait a bit and try again. This
            // happens because we load ClipboardJS asynchronously.
            if (window.thebelab === undefined) {
                setTimeout(addThebelabToCodeCells, 250)
            return
            }

            // If we already detect a Thebelab cell, don't re-run
            if (document.querySelectorAll('div.thebelab-cell').length > 0) {
                return;
            }

            // Find all code cells, replace with Thebelab interactive code cells
            const codeCells = document.querySelectorAll('.input_area pre')
            codeCells.forEach((codeCell, index) => {
                const id = codeCellId(index)
                codeCell.setAttribute('data-executable', 'true')

                // Figure out the language it uses and add this too
                var parentDiv = codeCell.parentElement.parentElement;
                var arrayLength = parentDiv.classList.length;
                for (var ii = 0; ii < arrayLength; ii++) {
                    var parts = parentDiv.classList[ii].split('language-');
                    if (parts.length === 2) {
                        // If found, assign dataLanguage and break the loop
                        var dataLanguage = parts[1];
                        break;
                    }
                }
                codeCell.setAttribute('data-language', dataLanguage)

                // If the code cell is hidden, show it
                var inputCheckbox = document.querySelector(`input#hidebtn${codeCell.id}`);
                if (inputCheckbox !== null) {
                    setCodeCellVisibility(inputCheckbox, 'visible');
                }
            });

            // Remove the event listener from the page so keyboard press doesn't
            // Change page
            document.removeEventListener('keydown', initPageNav)
            keyboardListener = false;

            // Init thebelab
            thebelab.bootstrap();

            // Remove copy buttons since they won't work anymore
            const copyAndThebeButtons = document.querySelectorAll('.copybtn, .thebebtn')
            copyAndThebeButtons.forEach((button, index) => {
                button.remove();
            });

            // Remove outputs since they'll be stale
            const outputs = document.querySelectorAll('.output *, .output')
            outputs.forEach((output, index) => {
                output.remove();
            });
        }

        // Add event listener for the function to modify code cells
        const thebelabButtons = document.querySelectorAll('[id^=thebelab], [id$=thebelab]')
        thebelabButtons.forEach((thebelabButton,index) => {
            if (thebelabButton === null) {
                setTimeout(initThebelab, 250)
                return
            };
            thebelabButton.addEventListener('click', addThebelabToCodeCells);
        });
    }

    // Initialize Thebelab
    initFunction(initThebelab);
</script>



  <!-- Load the auto-generating TOC -->
  <script src="/jupyter-book/assets/js/tocbot.min.js"  type="text/javascript"></script>
  <script>
var initToc = function () {
  tocbot.init({
    tocSelector: 'nav.onthispage',
    contentSelector: '.c-textbook__content',
    headingSelector: 'h2, h3',
    orderedList: false,
    collapseDepth: 6,
    listClass: 'toc__menu',
    activeListItemClass: "",  // Not using
    activeLinkClass: "", // Not using
  });
  tocbot.refresh();
}
initFunction(initToc);
</script>

  <!-- Google analytics -->
  <script src="/jupyter-book/assets/js/ga.js" async></script>

  <!-- Clipboard copy button -->
  <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" async></script>

  <!-- Load JS that depends on site variables -->
  <script>
/**
 * Set up copy/paste for code blocks
 */
const codeCellId = index => `codecell${index}`

const clipboardButton = id =>
  `<a id="copy-button-${id}" class="btn copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#${id}">
    <img src="/jupyter-book/assets/images/copy-button.svg" alt="Copy to clipboard">
  </a>`

// Clears selected text since ClipboardJS will select the text when copying
const clearSelection = () => {
  if (window.getSelection) {
    window.getSelection().removeAllRanges()
  } else if (document.selection) {
    document.selection.empty()
  }
}

// Changes tooltip text for two seconds, then changes it back
const temporarilyChangeTooltip = (el, newText) => {
  const oldText = el.getAttribute('data-tooltip')
  el.setAttribute('data-tooltip', newText)
  setTimeout(() => el.setAttribute('data-tooltip', oldText), 2000)
}

const addCopyButtonToCodeCells = () => {
  // If ClipboardJS hasn't loaded, wait a bit and try again. This
  // happens because we load ClipboardJS asynchronously.
  if (window.ClipboardJS === undefined) {
    setTimeout(addCopyButtonToCodeCells, 250)
    return
  }

  const codeCells = document.querySelectorAll('div.c-textbook__content > div.highlighter-rouge > div.highlight > pre, div.input_area pre')
  codeCells.forEach((codeCell, index) => {
    const id = codeCellId(index)
    codeCell.setAttribute('id', id)
    if (document.getElementById("copy-button" + id) == null) {
      codeCell.insertAdjacentHTML('afterend', clipboardButton(id));
    }
  })

  const clipboard = new ClipboardJS('.copybtn')
  clipboard.on('success', event => {
    clearSelection()
    temporarilyChangeTooltip(event.trigger, 'Copied!')
  })

  clipboard.on('error', event => {
    temporarilyChangeTooltip(event.trigger, 'Failed to copy')
  })

  // Get rid of clipboard before the next page visit to avoid memory leak
  document.addEventListener('turbolinks:before-visit', () =>
    clipboard.destroy()
  )
}

initFunction(addCopyButtonToCodeCells);
</script>


  <!-- Hide cell code -->
  
<script>
/**
Add buttons to hide code cells
*/


var setCodeCellVisibility = function(inputField, kind) {
    // Update the image and class for hidden
    var id = inputField.getAttribute('data-id');
    var codeCell = document.querySelector(`#${id} div.highlight`);

    if (kind === "visible") {
        codeCell.classList.remove('hidden');
        inputField.checked = true;
    } else {
        codeCell.classList.add('hidden');
        inputField.checked = false;
    }
}

var toggleCodeCellVisibility = function (event) {
    // The label is clicked, and now we decide what to do based on the input field's clicked status
    if (event.target.tagName === "LABEL") {
        var inputField = event.target.previousElementSibling;
    } else {
        // It is the span inside the target
        var inputField = event.target.parentElement.previousElementSibling;
    }

    if (inputField.checked === true) {
        setCodeCellVisibility(inputField, "visible");
    } else {
        setCodeCellVisibility(inputField, "hidden");
    }
}


// Button constructor
const hideCodeButton = id => `<input class="hidebtn" type="checkbox" id="hidebtn${id}" data-id="${id}"><label title="Toggle cell" for="hidebtn${id}" class="plusminus"><span class="pm_h"></span><span class="pm_v"></span></label>`

var addHideButton = function () {
  // If a hide button is already added, don't add another
  if (document.querySelector('div.hidecode input') !== null) {
      return;
  }

  // Find the input cells and add a hide button
  document.querySelectorAll('div.input_area').forEach(function (item, index) {
    if (!item.classList.contains("hidecode")) {
        // Skip the cell if it doesn't have a hidecode class
        return;
    }

    const id = codeCellId(index)
    item.setAttribute('id', id);
    // Insert the button just inside the end of the next div
    item.querySelector('div').insertAdjacentHTML('beforeend', hideCodeButton(id))

    // Set up the visibility toggle
    hideLink = document.querySelector(`#${id} div.highlight + input + label`);
    hideLink.addEventListener('click', toggleCodeCellVisibility)
  });
}


// Initialize the hide buttos
var initHiddenCells = function () {
    // Add hide buttons to the cells
    addHideButton();

    // Toggle the code cells that should be hidden
    document.querySelectorAll('div.hidecode input').forEach(function (item) {
        setCodeCellVisibility(item, 'hidden');
        item.checked = true;
    })
}

initFunction(initHiddenCells);

</script>


  <!-- Load custom website scripts -->
  <script src="/jupyter-book/assets/js/scripts.js" async></script>

  <!-- Load custom user CSS and JS  -->
  <script src="/jupyter-book/assets/custom/custom.js" async></script>
  <link rel="stylesheet" href="/jupyter-book/assets/custom/custom.css">

  <!-- Update interact links w/ REST param, is defined in includes so we can use templates -->
  
<script>
/**
  * To auto-embed hub URLs in interact links if given in a RESTful fashion
 */

function getJsonFromUrl(url) {
  var query = url.split('?');
  if (query.length < 2) {
    // No queries so just return false
    return false;
  }
  query = query[1];
  // Collect REST params into a dictionary
  var result = {};
  query.split("&").forEach(function(part) {
    var item = part.split("=");
    result[item[0]] = decodeURIComponent(item[1]);
  });
  return result;
}
    
function dict2param(dict) {
    params = Object.keys(dict).map(function(k) {
        return encodeURIComponent(k) + '=' + encodeURIComponent(dict[k])
    });
    return params.join('&')
}

// Parse a Binder URL, converting it to the string needed for JupyterHub
function binder2Jupyterhub(url) {
  newUrl = {};
  parts = url.split('v2/gh/')[1];
  // Grab the base repo information
  repoinfo = parts.split('?')[0];
  var [org, repo, ref] = repoinfo.split('/');
  newUrl['repo'] = ['https://github.com', org, repo].join('/');
  newUrl['branch'] = ref
  // Grab extra parameters passed
  params = getJsonFromUrl(url);
  if (params['filepath'] !== undefined) {
    newUrl['subPath'] = params['filepath']
  }
  return dict2param(newUrl);
}

// Filter out potentially unsafe characters to prevent xss
function safeUrl(url)
{
   return String(encodeURIComponent(url))
            .replace(/&/g, '&amp;')
            .replace(/"/g, '&quot;')
            .replace(/'/g, '&#39;')
            .replace(/</g, '&lt;')
            .replace(/>/g, '&gt;');
}

function addParamToInternalLinks(hub) {
  var links = document.querySelectorAll("a").forEach(function(link) {
    var href = link.href;
    // If the link is an internal link...
    if (href.search("https://jupyter.org") !== -1 || href.startsWith('/') || href.search("127.0.0.1:") !== -1) {
      // Assume we're an internal link, add the hub param to it
      var params = getJsonFromUrl(href);
      if (params !== false) {
        // We have REST params, so append a new one
        params['jupyterhub'] = hub;
      } else {
        // Create the REST params
        params = {'jupyterhub': hub};
      }
      // Update the link
      var newHref = href.split('?')[0] + '?' + dict2param(params);
      link.setAttribute('href', decodeURIComponent(newHref));
    }
  });
  return false;
}


// Update interact links
function updateInteractLink() {
    // hack to make this work since it expects a ? in the URL
    rest = getJsonFromUrl("?" + location.search.substr(1));
    jupyterHubUrl = rest['jupyterhub'];
    var hubType = null;
    var hubUrl = null;
    if (jupyterHubUrl !== undefined) {
      hubType = 'jupyterhub';
      hubUrl = jupyterHubUrl;
    }

    if (hubType !== null) {
      // Sanitize the hubUrl
      hubUrl = safeUrl(hubUrl);

      // Add HTTP text if omitted
      if (hubUrl.indexOf('http') < 0) {hubUrl = 'http://' + hubUrl;}
      var interactButtons = document.querySelectorAll("button.interact-button")
      var lastButton = interactButtons[interactButtons.length-1];
      var link = lastButton.parentElement;

      // If we've already run this, skip the link updating
      if (link.nextElementSibling !== null) {
        return;
      }

      // Update the link and add context div
      var href = link.getAttribute('href');
      if (lastButton.id === 'interact-button-binder') {
        // If binder links exist, we need to re-work them for jupyterhub
        if (hubUrl.indexOf('http%3A%2F%2Flocalhost') > -1) {
          // If localhost, assume we're working from a local Jupyter server and remove `/hub`
          first = [hubUrl, 'git-sync'].join('/')
        } else {
          first = [hubUrl, 'hub', 'user-redirect', 'git-sync'].join('/')
        }
        href = first + '?' + binder2Jupyterhub(href);
      } else {
        // If interact button isn't binderhub, assume it's jupyterhub
        // If JupyterHub links, we only need to replace the hub url
        href = href.replace("", hubUrl);
        if (hubUrl.indexOf('http%3A%2F%2Flocalhost') > -1) {
          // Assume we're working from a local Jupyter server and remove `/hub`
          href = href.replace("/hub/user-redirect", "");
        }
      }
      link.setAttribute('href', decodeURIComponent(href));

      // Add text after interact link saying where we're launching
      hubUrlNoHttp = decodeURIComponent(hubUrl).replace('http://', '').replace('https://', '');
      link.insertAdjacentHTML('afterend', '<div class="interact-context">on ' + hubUrlNoHttp + '</div>');

      // Update internal links so we retain the hub url
      addParamToInternalLinks(hubUrl);
    }
}

runWhenDOMLoaded(updateInteractLink)
document.addEventListener('turbolinks:load', updateInteractLink)
</script>


  <!-- Lunr search code - will only be executed on the /search page -->
  <script src="/jupyter-book/assets/js/lunr/lunr.min.js" type="text/javascript"></script>
  <script>var initQuery = function() {
  // See if we have a search box
  var searchInput = document.querySelector('input#lunr_search');
  if (searchInput === null) {
    return;
  }

  // Function to parse our lunr cache
  var idx = lunr(function () {
    this.field('title')
    this.field('excerpt')
    this.field('categories')
    this.field('tags')
    this.ref('id')

    this.pipeline.remove(lunr.trimmer)

    for (var item in store) {
      this.add({
        title: store[item].title,
        excerpt: store[item].excerpt,
        categories: store[item].categories,
        tags: store[item].tags,
        id: item
      })
    }
  });

  // Run search upon keyup
  searchInput.addEventListener('keyup', function () {
    var resultdiv = document.querySelector('#results');
    var query = document.querySelector("input#lunr_search").value.toLowerCase();
    var result =
      idx.query(function (q) {
        query.split(lunr.tokenizer.separator).forEach(function (term) {
          q.term(term, { boost: 100 })
          if(query.lastIndexOf(" ") != query.length-1){
            q.term(term, {  usePipeline: false, wildcard: lunr.Query.wildcard.TRAILING, boost: 10 })
          }
          if (term != ""){
            q.term(term, {  usePipeline: false, editDistance: 1, boost: 1 })
          }
        })
      });

      // Empty the results div
      while (resultdiv.firstChild) {
        resultdiv.removeChild(resultdiv.firstChild);
      }

    resultdiv.insertAdjacentHTML('afterbegin', '<p class="results__found">'+result.length+' Result(s) found</p>');
    for (var item in result) {
      var ref = result[item].ref;
      if(store[ref].teaser){
        var searchitem =
          '<div class="list__item">'+
            '<article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">'+
              '<h2 class="archive__item-title" itemprop="headline">'+
                '<a href="'+store[ref].url+'" rel="permalink">'+store[ref].title+'</a>'+
              '</h2>'+
              '<div class="archive__item-teaser">'+
                '<img src="'+store[ref].teaser+'" alt="">'+
              '</div>'+
              '<p class="archive__item-excerpt" itemprop="description">'+store[ref].excerpt.split(" ").splice(0,20).join(" ")+'...</p>'+
            '</article>'+
          '</div>';
      }
      else{
    	  var searchitem =
          '<div class="list__item">'+
            '<article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">'+
              '<h2 class="archive__item-title" itemprop="headline">'+
                '<a href="'+store[ref].url+'" rel="permalink">'+store[ref].title+'</a>'+
              '</h2>'+
              '<p class="archive__item-excerpt" itemprop="description">'+store[ref].excerpt.split(" ").splice(0,20).join(" ")+'...</p>'+
            '</article>'+
          '</div>';
      }
      resultdiv.insertAdjacentHTML('beforeend', searchitem);
    }
  });
};

initFunction(initQuery);
</script>
</head>

  <body>
    <!-- .js-show-sidebar shows sidebar by default -->
    <div id="js-textbook" class="c-textbook js-show-sidebar">
      



<nav id="js-sidebar" class="c-textbook__sidebar">
  <a href="https://jupyter.org/jupyter-book/intro.html"><img src="/jupyter-book/images/logo/logo.png" class="textbook_logo" id="sidebar-logo" data-turbolinks-permanent/></a>
  <h2 class="c-sidebar__title">MyBook</h2>
  <ul class="c-sidebar__chapters">
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/jupyter-book/00Example/index.html"
        >
          
            1.
          
          00Example
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/00Example/1/index.html"
                >
                  
                    1.1
                  
                  1
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00Example/1/1/index.html"
                    >
                      
                        1.1.1
                        
                      
                      1
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00Example/1/2/index.html"
                    >
                      
                        1.1.2
                        
                      
                      2
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00Example/1/intro.html"
                    >
                      
                        1.1.3
                        
                      
                      intro
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/00Example/2/index.html"
                >
                  
                    1.2
                  
                  2
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00Example/2/why-data-science.html"
                    >
                      
                        1.2.1
                        
                      
                      why-data-science
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/00Example/3/index.html"
                >
                  
                    1.3
                  
                  3
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00Example/3/Plotting_the_Classics.html"
                    >
                      
                        1.3.1
                        
                      
                      Plotting_the_Classics
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00Example/3/subsection/index.html"
                    >
                      
                        1.3.2
                        
                      
                      subsection
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/00Example/4/index.html"
                >
                  
                    1.4
                  
                  4
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00Example/4/endnote.html"
                    >
                      
                        1.4.1
                        
                      
                      endnote
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00Example/4/establishing-causality.html"
                    >
                      
                        1.4.2
                        
                      
                      establishing-causality
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00Example/4/observation-and-visualization-john-snow-and-the-broad-street-pump.html"
                    >
                      
                        1.4.3
                        
                      
                      observation-and-visualization-john-snow-and-the-broad-street-pump
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00Example/4/randomization.html"
                    >
                      
                        1.4.4
                        
                      
                      randomization
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00Example/4/snow-s-grand-experiment.html"
                    >
                      
                        1.4.5
                        
                      
                      snow-s-grand-experiment
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/00Example/Calls.html"
                >
                  
                    1.5
                  
                  Calls
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/00Example/causality-and-experiments.html"
                >
                  
                    1.6
                  
                  causality-and-experiments
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/00Example/Expressions.html"
                >
                  
                    1.7
                  
                  Expressions
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/00Example/features/index.html"
                >
                  
                    1.8
                  
                  features
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00Example/features/citations.html"
                    >
                      
                        1.8.1
                        
                      
                      citations
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00Example/features/features.html"
                    >
                      
                        1.8.2
                        
                      
                      features
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00Example/features/hiding.html"
                    >
                      
                        1.8.3
                        
                      
                      hiding
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00Example/features/interact.html"
                    >
                      
                        1.8.4
                        
                      
                      interact
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00Example/features/interactive_cells.html"
                    >
                      
                        1.8.5
                        
                      
                      interactive_cells
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00Example/features/limits.html"
                    >
                      
                        1.8.6
                        
                      
                      limits
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00Example/features/markdown.html"
                    >
                      
                        1.8.7
                        
                      
                      markdown
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00Example/features/notebooks.html"
                    >
                      
                        1.8.8
                        
                      
                      notebooks
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00Example/features/search.html"
                    >
                      
                        1.8.9
                        
                      
                      search
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/00Example/Growth.html"
                >
                  
                    1.9
                  
                  Growth
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/00Example/Introduction_to_Tables.html"
                >
                  
                    1.10
                  
                  Introduction_to_Tables
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/00Example/Names.html"
                >
                  
                    1.11
                  
                  Names
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/00Example/Numbers.html"
                >
                  
                    1.12
                  
                  Numbers
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/00Example/programming-in-python.html"
                >
                  
                    1.13
                  
                  programming-in-python
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/00Example/Strings.html"
                >
                  
                    1.14
                  
                  Strings
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/00Example/Types.html"
                >
                  
                    1.15
                  
                  Types
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/00Example/what-is-data-science.html"
                >
                  
                    1.16
                  
                  what-is-data-science
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/jupyter-book/00TensorFlow/index.html"
        >
          
            2.
          
          00TensorFlow
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/00TensorFlow/DNN%E5%AE%9E%E4%BE%8B%E4%BB%A3%E7%A0%81%E5%9D%97.html"
                >
                  
                    2.1
                  
                  DNN实例代码块
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/00TensorFlow/DSSM%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B.html"
                >
                  
                    2.2
                  
                  DSSM双塔模型
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/00TensorFlow/keras-RNN%E5%AE%9E%E4%BE%8B.html"
                >
                  
                    2.3
                  
                  keras-RNN实例
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/00TensorFlow/KerasExample/index.html"
                >
                  
                    2.4
                  
                  KerasExample
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/KerasExample/addition_rnn.html"
                    >
                      
                        2.4.1
                        
                      
                      addition_rnn
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/KerasExample/antirectifier.html"
                    >
                      
                        2.4.2
                        
                      
                      antirectifier
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/KerasExample/babi_memnn.html"
                    >
                      
                        2.4.3
                        
                      
                      babi_memnn
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/KerasExample/babi_rnn.html"
                    >
                      
                        2.4.4
                        
                      
                      babi_rnn
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/KerasExample/image_ocr.html"
                    >
                      
                        2.4.5
                        
                      
                      image_ocr
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/KerasExample/imdb_bidirectional_lstm.html"
                    >
                      
                        2.4.6
                        
                      
                      imdb_bidirectional_lstm
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/KerasExample/imdb_cnn.html"
                    >
                      
                        2.4.7
                        
                      
                      imdb_cnn
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/KerasExample/imdb_cnn_lstm.html"
                    >
                      
                        2.4.8
                        
                      
                      imdb_cnn_lstm
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/KerasExample/imdb_lstm.html"
                    >
                      
                        2.4.9
                        
                      
                      imdb_lstm
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/KerasExample/lstm-seq2seq.html"
                    >
                      
                        2.4.10
                        
                      
                      lstm-seq2seq
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/KerasExample/mnist_acgan.html"
                    >
                      
                        2.4.11
                        
                      
                      mnist_acgan
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/KerasExample/mnist_mlp.html"
                    >
                      
                        2.4.12
                        
                      
                      mnist_mlp
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/KerasExample/README.html"
                    >
                      
                        2.4.13
                        
                      
                      README
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/KerasExample/reuters_mlp.html"
                    >
                      
                        2.4.14
                        
                      
                      reuters_mlp
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/KerasExample/tensorboard_embeddings_mnist.html"
                    >
                      
                        2.4.15
                        
                      
                      tensorboard_embeddings_mnist
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/00TensorFlow/keras%E6%97%A5%E5%B8%B8%E5%B0%8F%E5%B7%A5%E5%85%B7.html"
                >
                  
                    2.5
                  
                  keras日常小工具
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/00TensorFlow/kera%E5%AD%A6%E4%B9%A0-%E6%9B%B4%E6%96%B0.html"
                >
                  
                    2.6
                  
                  kera学习-更新
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/00TensorFlow/kera%E5%AE%9E%E4%BE%8B%E4%BA%8C.html"
                >
                  
                    2.7
                  
                  kera实例二
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/00TensorFlow/recommendation/index.html"
                >
                  
                    2.8
                  
                  recommendation
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/recommendation/DeepFM%E6%A8%A1%E5%9E%8B%E7%90%86%E8%AE%BA%E5%92%8C%E5%AE%9E%E8%B7%B5.html"
                    >
                      
                        2.8.1
                        
                      
                      DeepFM模型理论和实践
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/recommendation/DIN%E6%A8%A1%E5%9E%8B%E7%90%86%E8%AE%BA%E5%92%8C%E5%AE%9E%E8%B7%B5.html"
                    >
                      
                        2.8.2
                        
                      
                      DIN模型理论和实践
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/recommendation/LR.html"
                    >
                      
                        2.8.3
                        
                      
                      LR
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/recommendation/MLR%E6%A8%A1%E5%9E%8B%E7%90%86%E8%AE%BA%E5%92%8C%E5%AE%9E%E8%B7%B5.html"
                    >
                      
                        2.8.4
                        
                      
                      MLR模型理论和实践
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/00TensorFlow/tensorflowExample/index.html"
                >
                  
                    2.9
                  
                  tensorflowExample
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0001-ml_introduction.html"
                    >
                      
                        2.9.1
                        
                      
                      0001-ml_introduction
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0002-mnist_dataset_intro.html"
                    >
                      
                        2.9.2
                        
                      
                      0002-mnist_dataset_intro
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0101-helloworld.html"
                    >
                      
                        2.9.3
                        
                      
                      0101-helloworld
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0102-basic_eager_api.html"
                    >
                      
                        2.9.4
                        
                      
                      0102-basic_eager_api
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0103-basic_operations.html"
                    >
                      
                        2.9.5
                        
                      
                      0103-basic_operations
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0201-linear_regression_eager_api.html"
                    >
                      
                        2.9.6
                        
                      
                      0201-linear_regression_eager_api
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0202-linear_regression.html"
                    >
                      
                        2.9.7
                        
                      
                      0202-linear_regression
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0203-logistic_regression_eager_api.html"
                    >
                      
                        2.9.8
                        
                      
                      0203-logistic_regression_eager_api
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0204-nearest_neighbor.html"
                    >
                      
                        2.9.9
                        
                      
                      0204-nearest_neighbor
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0205-kmeans.html"
                    >
                      
                        2.9.10
                        
                      
                      0205-kmeans
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0206-LR.html"
                    >
                      
                        2.9.11
                        
                      
                      0206-LR
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0207-GBDT.html"
                    >
                      
                        2.9.12
                        
                      
                      0207-GBDT
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0208-word2vec.html"
                    >
                      
                        2.9.13
                        
                      
                      0208-word2vec
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0209-random_forest.html"
                    >
                      
                        2.9.14
                        
                      
                      0209-random_forest
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0301-autoencoder.html"
                    >
                      
                        2.9.15
                        
                      
                      0301-autoencoder
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0302-bidirectional_rnn.html"
                    >
                      
                        2.9.16
                        
                      
                      0302-bidirectional_rnn
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0303-convolutional_network_raw.html"
                    >
                      
                        2.9.17
                        
                      
                      0303-convolutional_network_raw
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0304-convolutional_network.html"
                    >
                      
                        2.9.18
                        
                      
                      0304-convolutional_network
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0305-dcgan.html"
                    >
                      
                        2.9.19
                        
                      
                      0305-dcgan
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0306-dynamic_rnn.html"
                    >
                      
                        2.9.20
                        
                      
                      0306-dynamic_rnn
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0307-gan.html"
                    >
                      
                        2.9.21
                        
                      
                      0307-gan
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0308-neural_network_eager_api.html"
                    >
                      
                        2.9.22
                        
                      
                      0308-neural_network_eager_api
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0309-neural_network_raw.html"
                    >
                      
                        2.9.23
                        
                      
                      0309-neural_network_raw
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0310-neural_network.html"
                    >
                      
                        2.9.24
                        
                      
                      0310-neural_network
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0311-recurrent_network.html"
                    >
                      
                        2.9.25
                        
                      
                      0311-recurrent_network
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0312-variational_autoencoder.html"
                    >
                      
                        2.9.26
                        
                      
                      0312-variational_autoencoder
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0401-save_restore_model.html"
                    >
                      
                        2.9.27
                        
                      
                      0401-save_restore_model
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0402-tensorboard_advanced.html"
                    >
                      
                        2.9.28
                        
                      
                      0402-tensorboard_advanced
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0403-tensorboard_basic.html"
                    >
                      
                        2.9.29
                        
                      
                      0403-tensorboard_basic
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0501-build_an_image_dataset.html"
                    >
                      
                        2.9.30
                        
                      
                      0501-build_an_image_dataset
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0502-tensorflow_dataset_api.html"
                    >
                      
                        2.9.31
                        
                      
                      0502-tensorflow_dataset_api
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0601-multigpu_basics.html"
                    >
                      
                        2.9.32
                        
                      
                      0601-multigpu_basics
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/00TensorFlow/tensorflowExample/0602-multigpu_cnn.html"
                    >
                      
                        2.9.33
                        
                      
                      0602-multigpu_cnn
                    </a>
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/jupyter-book/13%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/index.html"
        >
          
            3.
          
          13推荐系统
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/13%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/test.html"
                >
                  
                    3.1
                  
                  test
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/index.html"
        >
          
            4.
          
          14常见算法
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/adaBoost/index.html"
                >
                  
                    4.1
                  
                  adaBoost
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/adaBoost/AdaBoost.html"
                    >
                      
                        4.1.1
                        
                      
                      AdaBoost
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/adaBoost/AdaBoost%E5%85%83%E7%AE%97%E6%B3%95%E6%8F%90%E9%AB%98%E5%88%86%E7%B1%BB%E6%80%A7%E8%83%BD.html"
                    >
                      
                        4.1.2
                        
                      
                      AdaBoost元算法提高分类性能
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/apriori/index.html"
                >
                  
                    4.2
                  
                  apriori
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/apriori/apriori.html"
                    >
                      
                        4.2.1
                        
                      
                      apriori
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/bayes/index.html"
                >
                  
                    4.3
                  
                  bayes
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/bayes/naiveBayes.html"
                    >
                      
                        4.3.1
                        
                      
                      naiveBayes
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/boost/index.html"
                >
                  
                    4.4
                  
                  boost
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/boost/Boost%E7%AE%80%E4%BB%8B.html"
                    >
                      
                        4.4.1
                        
                      
                      Boost简介
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/ES%E6%9F%A5%E8%AF%A2.html"
                >
                  
                    4.5
                  
                  ES查询
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/fp-growth/index.html"
                >
                  
                    4.6
                  
                  fp-growth
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/fp-growth/fp-growth.html"
                    >
                      
                        4.6.1
                        
                      
                      fp-growth
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/k-Nearest%20Neighbor/index.html"
                >
                  
                    4.7
                  
                  k-Nearest Neighbor
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/k-Nearest%20Neighbor/knn.html"
                    >
                      
                        4.7.1
                        
                      
                      knn
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/k-Nearest%20Neighbor/knn_%E5%8E%9F%E7%89%88.html"
                    >
                      
                        4.7.2
                        
                      
                      knn_原版
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/kmeans/index.html"
                >
                  
                    4.8
                  
                  kmeans
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/kmeans/kmeans.html"
                    >
                      
                        4.8.1
                        
                      
                      kmeans
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/line-regression/index.html"
                >
                  
                    4.9
                  
                  line-regression
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/line-regression/line_regression.html"
                    >
                      
                        4.9.1
                        
                      
                      line_regression
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/line-regression/LR.html"
                    >
                      
                        4.9.2
                        
                      
                      LR
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/logistic/index.html"
                >
                  
                    4.10
                  
                  logistic
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/logistic/logistic-code.html"
                    >
                      
                        4.10.1
                        
                      
                      logistic-code
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/logistic/logistic-%E7%90%86%E8%AE%BA.html"
                    >
                      
                        4.10.2
                        
                      
                      logistic-理论
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/logistic/temo.html"
                    >
                      
                        4.10.3
                        
                      
                      temo
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/pca/index.html"
                >
                  
                    4.11
                  
                  pca
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/pca/pca.html"
                    >
                      
                        4.11.1
                        
                      
                      pca
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/pca%E4%B8%8Esvd%E7%9A%84%E6%AF%94%E8%BE%83/index.html"
                >
                  
                    4.12
                  
                  pca与svd的比较
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/pca%E4%B8%8Esvd%E7%9A%84%E6%AF%94%E8%BE%83/pca.html"
                    >
                      
                        4.12.1
                        
                      
                      pca
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/README.html"
                >
                  
                    4.13
                  
                  README
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/regressionTree/index.html"
                >
                  
                    4.14
                  
                  regressionTree
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/regressionTree/CART.html"
                    >
                      
                        4.14.1
                        
                      
                      CART
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/svd/index.html"
                >
                  
                    4.15
                  
                  svd
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/svd/collaborative_filter.html"
                    >
                      
                        4.15.1
                        
                      
                      collaborative_filter
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/svd/svd.html"
                    >
                      
                        4.15.2
                        
                      
                      svd
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/svm/index.html"
                >
                  
                    4.16
                  
                  svm
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/svm/smo.html"
                    >
                      
                        4.16.1
                        
                      
                      smo
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/Untitled.html"
                >
                  
                    4.17
                  
                  Untitled
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/xgboost/index.html"
                >
                  
                    4.18
                  
                  xgboost
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/xgboost/xgboost.html"
                    >
                      
                        4.18.1
                        
                      
                      xgboost
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/xgboost/%E5%8D%81%E5%88%86%E9%92%9F%E4%B8%8A%E6%89%8Bxgboost.html"
                    >
                      
                        4.18.2
                        
                      
                      十分钟上手xgboost
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E5%86%B3%E7%AD%96%E6%A0%91(DecisionTree)/index.html"
                >
                  
                    4.19
                  
                  决策树(DecisionTree)
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E5%86%B3%E7%AD%96%E6%A0%91(DecisionTree)/decisionTree.html"
                    >
                      
                        4.19.1
                        
                      
                      decisionTree
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E5%86%B3%E7%AD%96%E6%A0%91(DecisionTree)/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%80%E4%BB%8B.html"
                    >
                      
                        4.19.2
                        
                      
                      决策树简介
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E5%BB%BA%E6%A8%A1%E8%BF%87%E7%A8%8B.html"
                >
                  
                    4.20
                  
                  建模过程
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/index.html"
                >
                  
                    4.21
                  
                  推荐算法
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93.html"
                    >
                      
                        4.21.1
                        
                      
                      协同过滤推荐算法总结
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3.html"
                    >
                      
                        4.21.2
                        
                      
                      矩阵分解
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98/index.html"
                >
                  
                    4.22
                  
                  文本挖掘
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98/LDA%E6%A8%A1%E5%9E%8B.html"
                    >
                      
                        4.22.1
                        
                      
                      LDA模型
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98/Word2Vec.html"
                    >
                      
                        4.22.2
                        
                      
                      Word2Vec
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%E4%B9%8B%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E7%B4%A2%E5%BC%95(LSI).html"
                    >
                      
                        4.22.3
                        
                      
                      文本主题模型之潜在语义索引(LSI)
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E9%A2%84%E5%A4%84%E7%90%86%E4%B9%8BTF-IDF.html"
                    >
                      
                        4.22.4
                        
                      
                      文本挖掘预处理之TF-IDF
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8BHMM.html"
                    >
                      
                        4.22.5
                        
                      
                      隐马尔科夫模型HMM
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.html"
                >
                  
                    4.23
                  
                  深度学习
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/LSTM.html"
                    >
                      
                        4.23.1
                        
                      
                      LSTM
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/RNN.html"
                    >
                      
                        4.23.2
                        
                      
                      RNN
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Seq2Seq.html"
                    >
                      
                        4.23.3
                        
                      
                      Seq2Seq
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/TensorFlow-%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%A4%84%E7%90%86.html"
                    >
                      
                        4.23.4
                        
                      
                      TensorFlow-字符串处理
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/tensorflow%E5%AD%A6%E4%B9%A0.html"
                    >
                      
                        4.23.5
                        
                      
                      tensorflow学习
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/TensorFlow%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E5%9D%97.html"
                    >
                      
                        4.23.6
                        
                      
                      TensorFlow常见代码块
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/tf%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95.html"
                    >
                      
                        4.23.7
                        
                      
                      tf优化方法
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/tf%E5%87%BD%E6%95%B0.html"
                    >
                      
                        4.23.8
                        
                      
                      tf函数
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html"
                    >
                      
                        4.23.9
                        
                      
                      人工神经网络
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html"
                >
                  
                    4.24
                  
                  神经网络
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/AutoEncoder-%E8%87%AA%E7%BC%96%E7%A0%81%E5%AE%9E%E8%B7%B5.html"
                    >
                      
                        4.24.1
                        
                      
                      AutoEncoder-自编码实践
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/AutoEncoder-%E8%87%AA%E7%BC%96%E7%A0%81%E7%90%86%E8%AE%BA.html"
                    >
                      
                        4.24.2
                        
                      
                      AutoEncoder-自编码理论
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%90%86%E8%AE%BA.html"
                    >
                      
                        4.24.3
                        
                      
                      CNN-卷积神经网络理论
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/DNN-%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html"
                    >
                      
                        4.24.4
                        
                      
                      DNN-深度神经网络
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/LSTM-%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C%E7%90%86%E8%AE%BA.html"
                    >
                      
                        4.24.5
                        
                      
                      LSTM-长短期记忆网络理论
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/PCA-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90.html"
                    >
                      
                        4.24.6
                        
                      
                      PCA-主成分分析
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/RBM-%E5%8F%97%E9%99%90%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E6%9C%BA%E5%AE%9E%E8%B7%B5.html"
                    >
                      
                        4.24.7
                        
                      
                      RBM-受限玻尔兹曼机实践
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/RBM-%E5%8F%97%E9%99%90%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E6%9C%BA%E7%90%86%E8%AE%BA.html"
                    >
                      
                        4.24.8
                        
                      
                      RBM-受限玻尔兹曼机理论
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/RNN-%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%90%86%E8%AE%BA.html"
                    >
                      
                        4.24.9
                        
                      
                      RNN-递归神经网络理论
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%90%91%E9%87%8F%E9%99%8D%E7%BB%B4.html"
                    >
                      
                        4.24.10
                        
                      
                      向量降维
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%B8%B8%E8%A7%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%B1%87%E6%80%BB.html"
                    >
                      
                        4.24.11
                        
                      
                      常见损失函数汇总
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%B8%B8%E8%A7%81%E6%BF%80%E5%8A%B1%E5%87%BD%E6%95%B0%E6%B1%87%E6%80%BB.html"
                    >
                      
                        4.24.12
                        
                      
                      常见激励函数汇总
                    </a>
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/jupyter-book/15tools/index.html"
        >
          
            5.
          
          15tools
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/15tools/latex/index.html"
                >
                  
                    5.1
                  
                  latex
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/15tools/latex/LaTeX%E5%85%AC%E5%BC%8F%E5%AE%9E%E4%BE%8B.html"
                    >
                      
                        5.1.1
                        
                      
                      LaTeX公式实例
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/15tools/latex/latex%E5%85%AC%E5%BC%8F%E7%BC%96%E5%8F%B7.html"
                    >
                      
                        5.1.2
                        
                      
                      latex公式编号
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/15tools/latex/%E5%8D%81%E5%88%86%E9%92%9F%E4%B8%8A%E6%89%8BLaTeX.html"
                    >
                      
                        5.1.3
                        
                      
                      十分钟上手LaTeX
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/15tools/numpy/index.html"
                >
                  
                    5.2
                  
                  numpy
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/15tools/numpy/numpy%E6%95%99%E7%A8%8B00.html"
                    >
                      
                        5.2.1
                        
                      
                      numpy教程00
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/15tools/numpy/numpy%E6%95%99%E7%A8%8B01.html"
                    >
                      
                        5.2.2
                        
                      
                      numpy教程01
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/15tools/numpy/numpy%E6%95%99%E7%A8%8B02.html"
                    >
                      
                        5.2.3
                        
                      
                      numpy教程02
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/15tools/numpy/numpy%E6%95%99%E7%A8%8B03.html"
                    >
                      
                        5.2.4
                        
                      
                      numpy教程03
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/15tools/pandas/index.html"
                >
                  
                    5.3
                  
                  pandas
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/15tools/pandas/pandas.html"
                    >
                      
                        5.3.1
                        
                      
                      pandas
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/15tools/pandas/pandas%E5%87%BD%E6%95%B0%E6%B7%B1%E5%85%A5.html"
                    >
                      
                        5.3.2
                        
                      
                      pandas函数深入
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/15tools/pandas/pandas%E6%95%99%E7%A8%8B01.html"
                    >
                      
                        5.3.3
                        
                      
                      pandas教程01
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/15tools/pandas/pandas%E6%95%99%E7%A8%8B02.html"
                    >
                      
                        5.3.4
                        
                      
                      pandas教程02
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/15tools/pandas/pandas%E6%95%99%E7%A8%8B03.html"
                    >
                      
                        5.3.5
                        
                      
                      pandas教程03
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/15tools/pandas/%E5%8D%81%E5%88%86%E9%92%9F%E4%B8%8A%E6%89%8BPandas.html"
                    >
                      
                        5.3.6
                        
                      
                      十分钟上手Pandas
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/15tools/pandas/%E5%8D%81%E5%88%86%E9%92%9F%E4%B8%8A%E6%89%8BPandas%E5%87%BD%E6%95%B0.html"
                    >
                      
                        5.3.7
                        
                      
                      十分钟上手Pandas函数
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/15tools/%E7%94%BB%E5%9B%BE/index.html"
                >
                  
                    5.4
                  
                  画图
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/15tools/%E7%94%BB%E5%9B%BE/python%E7%94%BB%E5%9B%BE%E4%B9%8Bfolium%E5%9C%B0%E5%9B%BE.html"
                    >
                      
                        5.4.1
                        
                      
                      python画图之folium地图
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/15tools/%E7%94%BB%E5%9B%BE/python%E7%94%BB%E5%9B%BE%E4%B9%8Bpycharts-3D%E5%9B%BE.html"
                    >
                      
                        5.4.2
                        
                      
                      python画图之pycharts-3D图
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/15tools/%E7%94%BB%E5%9B%BE/python%E7%94%BB%E5%9B%BE%E4%B9%8Bpycharts-kline.html"
                    >
                      
                        5.4.3
                        
                      
                      python画图之pycharts-kline
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/15tools/%E7%94%BB%E5%9B%BE/python%E7%94%BB%E5%9B%BE%E4%B9%8Bpycharts-%E5%85%B6%E4%BB%96.html"
                    >
                      
                        5.4.4
                        
                      
                      python画图之pycharts-其他
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/15tools/%E7%94%BB%E5%9B%BE/python%E7%94%BB%E5%9B%BE%E4%B9%8Bpycharts-%E5%85%B6%E4%BB%96%E5%9D%90%E6%A0%87%E7%B3%BB.html"
                    >
                      
                        5.4.5
                        
                      
                      python画图之pycharts-其他坐标系
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/15tools/%E7%94%BB%E5%9B%BE/python%E7%94%BB%E5%9B%BE%E4%B9%8Bpycharts-%E5%9C%B0%E5%9B%BE.html"
                    >
                      
                        5.4.6
                        
                      
                      python画图之pycharts-地图
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/15tools/%E7%94%BB%E5%9B%BE/python%E7%94%BB%E5%9B%BE%E4%B9%8Bpycharts-%E7%9B%B4%E8%A7%92%E5%9D%90%E6%A0%87%E7%B3%BB.html"
                    >
                      
                        5.4.7
                        
                      
                      python画图之pycharts-直角坐标系
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/15tools/%E7%94%BB%E5%9B%BE/python%E7%94%BB%E5%9B%BE%E4%B9%8Bpycharts-%E7%AE%80%E4%BB%8B.html"
                    >
                      
                        5.4.8
                        
                      
                      python画图之pycharts-简介
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/15tools/%E7%94%BB%E5%9B%BE/Untitled.html"
                    >
                      
                        5.4.9
                        
                      
                      Untitled
                    </a>
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/jupyter-book/colabHub/index.html"
        >
          
            6.
          
          colabHub
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/colabHub/README.html"
                >
                  
                    6.1
                  
                  README
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/colabHub/%E5%88%9D%E5%A7%8B%E5%8C%96.html"
                >
                  
                    6.2
                  
                  初始化
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/jupyter-book/DownLoadPicture.html"
        >
          
            7.
          
          DownLoadPicture
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/jupyter-book/Example.html"
        >
          
            8.
          
          Example
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/jupyter-book/guide/index.html"
        >
          
            9.
          
          guide
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/guide/01_overview.html"
                >
                  
                    9.1
                  
                  01_overview
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/guide/02_create.html"
                >
                  
                    9.2
                  
                  02_create
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/guide/03_build.html"
                >
                  
                    9.3
                  
                  03_build
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/guide/04_faq.html"
                >
                  
                    9.4
                  
                  04_faq
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/guide/05_advanced.html"
                >
                  
                    9.5
                  
                  05_advanced
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/jupyter-book/intro.html"
        >
          
            10.
          
          intro
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/jupyter-book/jupyter-book-start.html"
        >
          
            11.
          
          jupyter-book-start
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/jupyter-book/LICENSE.html"
        >
          
            12.
          
          LICENSE
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/jupyter-book/math/index.html"
        >
          
            13.
          
          math
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/math/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90.html"
                >
                  
                    13.1
                  
                  主成分分析
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/math/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95.html"
                >
                  
                    13.2
                  
                  最小二乘法
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/math/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC.html"
                >
                  
                    13.3
                  
                  矩阵求导
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/jupyter-book/notebooks/index.html"
        >
          
            14.
          
          notebooks
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/notebooks/JupyterNotebookTips.html"
                >
                  
                    14.1
                  
                  JupyterNotebookTips
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/notebooks/LinearRegression.html"
                >
                  
                    14.2
                  
                  LinearRegression
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/jupyter-book/notechats/index.html"
        >
          
            15.
          
          notechats
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/notechats/notedata%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE.html"
                >
                  
                    15.1
                  
                  notedata测试数据
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/jupyter-book/package/index.html"
        >
          
            16.
          
          package
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/package/folium.html"
                >
                  
                    16.1
                  
                  folium
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/package/graphviz%E4%B9%8B%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8.html"
                >
                  
                    16.2
                  
                  graphviz之安装使用
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/package/pygithub.html"
                >
                  
                    16.3
                  
                  pygithub
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/package/word2vec.html"
                >
                  
                    16.4
                  
                  word2vec
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/package/%E5%90%91%E9%87%8F%E5%BC%95%E6%93%8Efaiss.html"
                >
                  
                    16.5
                  
                  向量引擎faiss
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/package/%E8%AF%8D%E5%90%91%E9%87%8F%E8%AE%AD%E7%BB%83.html"
                >
                  
                    16.6
                  
                  词向量训练
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/jupyter-book/paper/index.html"
        >
          
            17.
          
          paper
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/paper/atrank/index.html"
                >
                  
                    17.1
                  
                  atrank
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/paper/atrank/Atrank.html"
                    >
                      
                        17.1.1
                        
                      
                      Atrank
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/jupyter-book/paper/atrank/Attention.html"
                    >
                      
                        17.1.2
                        
                      
                      Attention
                    </a>
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/jupyter-book/temp/index.html"
        >
          
            18.
          
          temp
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/temp/merge.html"
                >
                  
                    18.1
                  
                  merge
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/jupyter-book/TempDir/index.html"
        >
          
            19.
          
          TempDir
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/TempDir/Test1.html"
                >
                  
                    19.1
                  
                  Test1
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/TempDir/%E4%B8%AA%E7%A8%8E%E8%AE%A1%E7%AE%97%E5%99%A8.html"
                >
                  
                    19.2
                  
                  个税计算器
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/jupyter-book/TempDir/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html"
                >
                  
                    19.3
                  
                  强化学习实践
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/jupyter-book/Todo.html"
        >
          
            20.
          
          Todo
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/jupyter-book/Untitled.html"
        >
          
            21.
          
          Untitled
        </a>

        
      </li>

      
    
  </ul>
  <p class="sidebar_footer">Powered by <a href="https://github.com/jupyter/jupyter-book">Jupyter Book</a></p>
</nav>

      
      <main class="c-textbook__page" tabindex="-1">
          <div class="o-wrapper">
            <div class="c-sidebar-toggle">
  <!-- We show the sidebar by default so we use .is-active -->
  <button
    id="js-sidebar-toggle"
    class="hamburger hamburger--arrowalt is-active"
  >
    <span class="hamburger-box">
      <span class="hamburger-inner"></span>
    </span>
    <span class="c-sidebar-toggle__label">Toggle Sidebar</span>
  </button>
</div>

            

            <div class="c-textbook__content">
              <div class="search-content__inner-wrap">
    <input type="text" id="lunr_search" class="search-input" tabindex="-1" placeholder="'Enter your search term...''" />
    <div id="results" class="results"></div>
</div>

<script>
    // Add the lunr store since we will now search it
    var store = [{
        "title": "hello jekyll!",
        
        "excerpt":
            "You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated. To add new posts, simply add a file in the _posts directory that follows the convention YYYY-MM-DD-name-of-post.ext and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works. Jekyll also offers powerful support for...",
        "categories": ["gaohaoyangBlog"],
        "tags": ["jekyll"],
        "url": "https://jupyter.org/jupyter-book/12%E6%88%91%E7%88%B1%E5%AD%A6%E4%B9%A0-pass/%E8%BD%AC%E8%BD%BDpass/example2/welcome-to-jekyll.html",
        "teaser":null},{
        "title": "1",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/1/1/index.html",
        "teaser":null},{
        "title": "2",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/1/2/index.html",
        "teaser":null},{
        "title": "1",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/1/index.html",
        "teaser":null},{
        "title": "intro",
        
        "excerpt":
            "Chapter 1: Introduction Data are descriptions of the world around us, collected through observation and stored on computers. Computers enable us to infer properties of the world from these descriptions. Data science is the discipline of drawing conclusions from data using computation. There are three core aspects of effective data analysis: exploration, prediction, and inference. This text develops a consistent approach to all three, introducing statistical ideas and fundamental ideas in computer science concurrently. We focus on a minimal set of core techniques that they apply to a vast range of real-world applications. A foundation in data science requires not...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/1/intro.html",
        "teaser":null},{
        "title": "2",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/2/index.html",
        "teaser":null},{
        "title": "why-data-science",
        
        "excerpt":
            "Why Data Science? Most important decisions are made with only partial information and uncertain outcomes. However, the degree of uncertainty for many decisions can be reduced sharply by public access to large data sets and the computational tools required to analyze them effectively. Data-driven decision making has already transformed a tremendous breadth of industries, including finance, advertising, manufacturing, and real estate. At the same time, a wide range of academic disciplines are evolving rapidly to incorporate large-scale data analysis into their theory and practice. Studying data science enables individuals to bring these techniques to bear on their work, their scientific...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/2/why-data-science.html",
        "teaser":null},{
        "title": "Plotting_the_Classics",
        
        "excerpt":
            "from datascience import * from datascience.predicates import are path_data = '../../data/' import numpy as np import matplotlib matplotlib.use('Agg', warn=False) %matplotlib inline import matplotlib.pyplot as plots plots.style.use('fivethirtyeight') import warnings warnings.simplefilter(action=\"ignore\", category=FutureWarning) from urllib.request import urlopen import re def read_url(url): return re.sub('\\\\s+', ' ', urlopen(url).read().decode()) In this example, we will explore statistics for two classic novels: The Adventures of Huckleberry Finn by Mark Twain, and Little Women by Louisa May Alcott. The text of any book can be read by a computer at great speed. Books published before 1923 are currently in the public domain, meaning that everyone has the right to...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/3/Plotting_the_Classics.html",
        "teaser":null},{
        "title": "3",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/3/index.html",
        "teaser":null},{
        "title": "subsection",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/3/subsection/index.html",
        "teaser":null},{
        "title": "endnote",
        
        "excerpt":
            "Endnote In the terminology of that we have developed, John Snow conducted an observational study, not a randomized experiment. But he called his study a “grand experiment” because, as he wrote, “No fewer than three hundred thousand people … were divided into two groups without their choice, and in most cases, without their knowledge …” Studies such as Snow’s are sometimes called “natural experiments.” However, true randomization does not simply mean that the treatment and control groups are selected “without their choice.” The method of randomization can be as simple as tossing a coin. It may also be quite a...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/4/endnote.html",
        "teaser":null},{
        "title": "establishing-causality",
        
        "excerpt":
            "Establishing Causality In the language developed earlier in the section, you can think of the people in the S&amp;V houses as the treatment group, and those in the Lambeth houses at the control group. A crucial element in Snow’s analysis was that the people in the two groups were comparable to each other, apart from the treatment. In order to establish whether it was the water supply that was causing cholera, Snow had to compare two groups that were similar to each other in all but one aspect–their water supply. Only then would he be able to ascribe the differences...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/4/establishing-causality.html",
        "teaser":null},{
        "title": "4",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/4/index.html",
        "teaser":null},{
        "title": "observation-and-visualization-john-snow-and-the-broad-street-pump",
        
        "excerpt":
            "Observation and Visualization: John Snow and the Broad Street Pump One of the earliest examples of astute observation eventually leading to the establishment of causality dates back more than 150 years. To get your mind into the right timeframe, try to imagine London in the 1850’s. It was the world’s wealthiest city but many of its people were desperately poor. Charles Dickens, then at the height of his fame, was writing about their plight. Disease was rife in the poorer parts of the city, and cholera was among the most feared. It was not yet known that germs cause disease;...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/4/observation-and-visualization-john-snow-and-the-broad-street-pump.html",
        "teaser":null},{
        "title": "randomization",
        
        "excerpt":
            "Randomization An excellent way to avoid confounding is to assign individuals to the treatment and control groups at random, and then administer the treatment to those who were assigned to the treatment group. Randomization keeps the two groups similar apart from the treatment. If you are able to randomize individuals into the treatment and control groups, you are running a randomized controlled experiment, also known as a randomized controlled trial (RCT). Sometimes, people’s responses in an experiment are influenced by their knowing which group they are in. So you might want to run a blind experiment in which individuals do...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/4/randomization.html",
        "teaser":null},{
        "title": "snow-s-grand-experiment",
        
        "excerpt":
            "Snow’s “Grand Experiment” Encouraged by what he had learned in Soho, Snow completed a more thorough analysis of cholera deaths. For some time, he had been gathering data on cholera deaths in an area of London that was served by two water companies. The Lambeth water company drew its water upriver from where sewage was discharged into the River Thames. Its water was relatively clean. But the Southwark and Vauxhall (S&amp;V) company drew its water below the sewage discharge, and thus its supply was contaminated. The map below shows the areas served by the two companies. Snow honed in on...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/4/snow-s-grand-experiment.html",
        "teaser":null},{
        "title": "Calls",
        
        "excerpt":
            "Call expressions invoke functions, which are named operations. The name of the function appears first, followed by expressions in parentheses. abs(-12) 12 round(5 - 1.3) 4 max(2, 2 + 3, 4) 5 In this last example, the max function is called on three arguments: 2, 5, and 4. The value of each expression within parentheses is passed to the function, and the function returns the final value of the full call expression. The max function can take any number of arguments and returns the maximum. A few functions are available by default, such as abs and round, but most functions...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/Calls.html",
        "teaser":null},{
        "title": "Expressions",
        
        "excerpt":
            "3.1 Expressions Programming languages are much simpler than human languages. Nonetheless, there are some rules of grammar to learn in any language, and that is where we will begin. In this text, we will use the Python programming language. Learning the grammar rules is essential, and the same rules used in the most basic programs are also central to more sophisticated programs. Programs are made up of expressions, which describe to the computer how to combine pieces of data. For example, a multiplication expression consists of a * symbol between two numerical expressions. Expressions, such as 3 * 4, are...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/Expressions.html",
        "teaser":null},{
        "title": "Growth",
        
        "excerpt":
            "3.2.1 Growth The relationship between two measurements of the same quantity taken at different times is often expressed as a growth rate. For example, the United States federal government employed 2,766,000 people in 2002 and 2,814,000 people in 2012. To compute a growth rate, we must first decide which value to treat as the initial amount. For values over time, the earlier value is a natural choice. Then, we divide the difference between the changed and initial amount by the initial amount. initial = 2766000 changed = 2814000 (changed - initial) / initial 0.01735357917570499 It is also typical to subtract...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/Growth.html",
        "teaser":null},{
        "title": "Introduction_to_Tables",
        
        "excerpt":
            "from datascience import * path_data = '../../data/' import numpy as np %matplotlib inline import matplotlib.pyplot as plots plots.style.use('fivethirtyeight') cones = Table.read_table(path_data + 'cones.csv') nba = Table.read_table(path_data + 'nba_salaries.csv').relabeled(3, 'SALARY') movies = Table.read_table(path_data + 'movies_by_year.csv') We can now apply Python to analyze data. We will work with data stored in Table structures. Tables are a fundamental way of representing data sets. A table can be viewed in two ways: a sequence of named columns that each describe a single attribute of all entries in a data set, or a sequence of rows that each contain all information about a single individual...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/Introduction_to_Tables.html",
        "teaser":null},{
        "title": "Names",
        
        "excerpt":
            "3.2 Names Names are given to values in Python using an assignment statement. In an assignment, a name is followed by =, which is followed by any expression. The value of the expression to the right of = is assigned to the name. Once a name has a value assigned to it, the value will be substituted for that name in future expressions. a = 10 b = 20 a + b 30 A previously assigned name can be used in the expression to the right of =. quarter = 1/4 half = 2 * quarter half 0.5 However, only...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/Names.html",
        "teaser":null},{
        "title": "Numbers",
        
        "excerpt":
            "Computers are designed to perform numerical calculations, but there are some important details about working with numbers that every programmer working with quantitative data should know. Python (and most other programming languages) distinguishes between two different types of numbers: Integers are called int values in the Python language. They can only represent whole numbers (negative, zero, or positive) that don’t have a fractional component Real numbers are called float values (or floating point values) in the Python language. They can represent whole or fractional numbers but have some limitations. The type of a number is evident from the way it...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/Numbers.html",
        "teaser":null},{
        "title": "Strings",
        
        "excerpt":
            "Much of the world’s data is text, and a piece of text represented in a computer is called a string. A string can represent a word, a sentence, or even the contents of every book in a library. Since text can include numbers (like this: 5) or truth values (True), a string can also describe those things. The meaning of an expression depends both upon its structure and the types of values that are being combined. So, for instance, adding two strings together produces another string. This expression is still an addition expression, but it is combining a different type...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/Strings.html",
        "teaser":null},{
        "title": "Types",
        
        "excerpt":
            "Every value has a type, and the built-in type function returns the type of the result of any expression.   One type we have encountered already is a built-in function. Python indicates that the type is a builtin_function_or_method; the distinction between a function and a method is not important at this stage.           type(abs)                         builtin_function_or_method                   This chapter will explore many useful types of data.  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/Types.html",
        "teaser":null},{
        "title": "causality-and-experiments",
        
        "excerpt":
            "Causality and Experiments “These problems are, and will probably ever remain, among the inscrutable secrets of nature. They belong to a class of questions radically inaccessible to the human intelligence.” —The Times of London, September 1849, on how cholera is contracted and spread Does the death penalty have a deterrent effect? Is chocolate good for you? What causes breast cancer? All of these questions attempt to assign a cause to an effect. A careful examination of data can help shed light on questions like these. In this section you will learn some of the fundamental concepts involved in establishing causality....",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/causality-and-experiments.html",
        "teaser":null},{
        "title": "citations",
        
        "excerpt":
            "Including citations in your book Because jupyter-book is built on top of Jekyll, we can use the excellent jekyll-scholar book to include citations and a bibliography with your book. Note that this only works if you’re building your book HTML locally and hosting the HTML files online somewhere. This can still use GitHub pages, but not the auto-generation of a cite from markdown files feature of GitHub pages. This is because GitHub pages doesn’t include the jekyll-scholar plugin. How to use citations Including citations with your markdown files or notebooks is done in the following way. Modify the file in...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/features/citations.html",
        "teaser":null},{
        "title": "features",
        
        "excerpt":
            "Features   This is a short demonstration textbook to show the general layout / style of textbooks built with Jupyter and Jekyll.   To begin, click on one of the chapter sections in the sidebar to the left. Alternatively, click on the “next” button below in order to read further.   The first sections demonstrate some simple functionality of Jupyter Books, while the final chapters contain a subset of content from the Foundations in Data Science textbook.  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/features/features.html",
        "teaser":null},{
        "title": "hiding",
        
        "excerpt":
            "Hiding code blocks or entire cells It’s possible to control which cells show up in your final book pages. For example, you may want to display a complex visualization to illustrate an idea, but don’t want the page to be cluttered with a large code cell that generated the viz. In other cases, you may want to remove a code cell entirely. This page explains how to accomplish this with Jupyter Book. Hiding code cells and displaying a button to show them Jupyter Books uses notebook cell tags to determine which code cells to hide. If you add the tag...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/features/hiding.html",
        "teaser":null},{
        "title": "features",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/features/index.html",
        "teaser":null},{
        "title": "interact",
        
        "excerpt":
            "Connecting content with JupyterHub and Binder Because Jupyter Books are built with Jupyter Notebooks, you can connect your online book with a Jupyter kernel running in the cloud. This lets readers quickly interact with your content in a traditional coding interface using either JupyterHub or BinderHub. This page describes a few ways to accomplish this. Creating interact buttons for BinderHub BinderHub can be used to build the environment needed to run a repository, and provides a link that lets others interact with that repository. If your Jupyter Book is hosted online on GitHub, you can automatically insert buttons that link...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/features/interact.html",
        "teaser":null},{
        "title": "interactive_cells",
        
        "excerpt":
            "Interactive code in your book Sometimes you’d rather let people interact with code directly on the page instead of sending them off to a Binder or a JupyterHub. There are currently a few ways to make this happen in Jupyter Book (both of which are experimental). This page describes how to bring interactivity to your book. Both of these tools use MyBinder to provide a remote kernel. Making your page inputs interactive ✨experimental✨ If you’d like to provide interactivity for your content without making your readers leave the Jupyter Book site, you can use a project called Thebelab. This provides...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/features/interactive_cells.html",
        "teaser":null},{
        "title": "limits",
        
        "excerpt":
            "Demo page to showcase markdown NOTE: None of the information on this page applies to Jupyter Books. It is taken from the Markdown documentation, and is meant to give an idea of how Jupyter Book renders really long pages of very diverse content! Amount of Content You should be able to put as much content, and as many headings, within each chapter as you want. In order to enable this we allow the table of contents on the right hand side to be scrollable. All of the sections on here allow us to test that. And they’ll give you a...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/features/limits.html",
        "teaser":null},{
        "title": "markdown",
        
        "excerpt":
            "Creating book content The two kinds of files that contain course content are: Jupyter Notebooks Markdown files Each are contained in the content/ folder and referenced from _data/toc.yml. If the file is markdown, it will be copied over with front-matter YAML added so that Jekyll can parse it. print(\"Python (and any language-specific) code still works as expected\") As does non-language code. Sidebars with Jekyll You may notice that there’s a sidebar to the right (if your screen is wide enough). These are automatically generated from the headers that are present in your page. The sidebar will automatically capture all 2nd...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/features/markdown.html",
        "teaser":null},{
        "title": "notebooks",
        
        "excerpt":
            "Content with notebooks You can also create content with Jupyter Notebooks. The content for the current page is contained in a Jupyter Notebook in the notebooks/ folder of the repository. This means that we can include code blocks and their outputs, and export them to Jekyll markdown. You can find the original notebook for this page at this address Markdown + notebooks As it is markdown, you can embed images, HTML, etc into your posts! You an also $add_{math}$ and or But make sure you \\$Escape \\$your \\$dollar signs \\$you want to keep! Code blocks and image outputs Textbooks with...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/features/notebooks.html",
        "teaser":null},{
        "title": "search",
        
        "excerpt":
            "Searching your book It’s possible to enable search in your book so that users can find the content they’re looking for. This uses a nifty package called lunr.js. To add search to your book, simple add the following entry to _data/toc.yml. - title: Search (whatever title you like) search: true The result will be a link in your Table of Contents that directs users to a search page. For example, click the Search link in the TOC of this demo book. Customizing how much text is stored in search The Jupyter-Book search works by storing excerpts of text from each...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/features/search.html",
        "teaser":null},{
        "title": "00Example",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/index.html",
        "teaser":null},{
        "title": "programming-in-python",
        
        "excerpt":
            "Programming in Python   Programming can dramatically improve our ability to collect and analyze information about the world, which in turn can lead to discoveries through the kind of careful reasoning demonstrated in the previous section. In data science, the purpose of writing a program is to instruct a computer to carry out the steps of an analysis. Computers cannot study the world on their own. People must describe precisely what steps the computer should take in order to collect and analyze data, and those steps are expressed through programs.   ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/programming-in-python.html",
        "teaser":null},{
        "title": "what-is-data-science",
        
        "excerpt":
            "What is Data Science Data Science is about drawing useful conclusions from large and diverse data sets through exploration, prediction, and inference. Exploration involves identifying patterns in information. Prediction involves using information we know to make informed guesses about values we wish we knew. Inference involves quantifying our degree of certainty: will those patterns we found also appear in new observations? How accurate are our predictions? Our primary tools for exploration are visualizations and descriptive statistics, for prediction are machine learning and optimization, and for inference are statistical tests and models. Statistics is a central component of data science because...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00Example/what-is-data-science.html",
        "teaser":null},{
        "title": "DNN实例代码块",
        
        "excerpt":
            "title: DNN实例 author: niult date: 2019-03-16 category: tf-example tags: python,numpy,tf-example DNN 实例介绍 Build a 2-hidden layers fully connected neural network (a.k.a multilayer perceptron) with TensorFlow. 网络结构 MNIST数据集介绍 This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flattened and converted to a 1-D numpy array of 784 features (28*28). More info: http://yann.lecun.com/exdb/mnist/ 输入数据介绍 from __future__ import print_function import tensorflow as tf import keras from tensorflow.examples.tutorials.mnist import...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/DNN%E5%AE%9E%E4%BE%8B%E4%BB%A3%E7%A0%81%E5%9D%97.html",
        "teaser":null},{
        "title": "DSSM双塔模型",
        
        "excerpt":
            "from keras import * input1 = Input(shape=(maxlen,)) input2 = Input(shape=(maxlen,)) input3 = Input(shape=(5,)) embed1 = Embedding(wordnum,embedsize) lstm0 = CuDNNLSTM(lstmsize,return_sequences = True) lstm1 = Bidirectional(CuDNNLSTM(lstmsize)) lstm2 = CuDNNLSTM(lstmsize) att1 = Attention(10) den = Dense(64,activation = 'tanh') # att1 = Lambda(lambda x: K.max(x,axis = 1)) v1 = embed1(input1) v2 = embed1(input2) v3 = embed1(input3) v11 = lstm1(v1) v22 = lstm1(v2) v1ls = lstm2(lstm0(v1)) v2ls = lstm2(lstm0(v2)) v1 = Concatenate(axis=1)([att1(v1),v11]) v2 = Concatenate(axis=1)([att1(v2),v22]) input1c = Input(shape=(maxlen2,)) input2c = Input(shape=(maxlen2,)) embed1c = Embedding(charnum,embedsize) lstm1c = Bidirectional(CuDNNLSTM(6)) att1c = Attention(10) v1c = embed1(input1c) v2c = embed1(input2c) v11c = lstm1c(v1c) v22c = lstm1c(v2c) v1c = Concatenate(axis=1)([att1c(v1c),v11c])...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/DSSM%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B.html",
        "teaser":null},{
        "title": "README",
        
        "excerpt":
            "Keras examples directory Vision models examples mnist_mlp.py Trains a simple deep multi-layer perceptron on the MNIST dataset. mnist_cnn.py Trains a simple convnet on the MNIST dataset. cifar10_cnn.py Trains a simple deep CNN on the CIFAR10 small images dataset. cifar10_cnn_capsule.py Trains a simple CNN-Capsule Network on the CIFAR10 small images dataset. cifar10_resnet.py Trains a ResNet on the CIFAR10 small images dataset. conv_lstm.py Demonstrates the use of a convolutional LSTM network. image_ocr.py Trains a convolutional stack followed by a recurrent stack and a CTC logloss function to perform optical character recognition (OCR). mnist_acgan.py Implementation of AC-GAN (Auxiliary Classifier GAN) on the MNIST...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/KerasExample/README.html",
        "teaser":null},{
        "title": "addition_rnn",
        
        "excerpt":
            "An implementation of sequence to sequence learning for performing addition Input: “535+61” Output: “596” Padding is handled by using a repeated sentinel character (space) Input may optionally be reversed, shown to increase performance in many tasks in: “Learning to Execute” “Sequence to Sequence Learning with Neural Networks” Theoretically it introduces shorter term dependencies between source and target. Two digits reversed: One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs Three digits reversed: One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs Four digits reversed: One layer LSTM (128...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/KerasExample/addition_rnn.html",
        "teaser":null},{
        "title": "antirectifier",
        
        "excerpt":
            "#This example demonstrates how to write custom layers for Keras. We build a custom activation layer called ‘Antirectifier’, which modifies the shape of the tensor that passes through it. We need to specify two methods: compute_output_shape and call. Note that the same result can also be achieved via a Lambda layer. Because our custom layer is written with primitives from the Keras backend (K), our code can run both on TensorFlow and Theano. from __future__ import print_function import keras from keras.models import Sequential from keras import layers from keras.datasets import mnist from keras import backend as K class Antirectifier(layers.Layer): '''This...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/KerasExample/antirectifier.html",
        "teaser":null},{
        "title": "babi_memnn",
        
        "excerpt":
            "#Trains a memory network on the bAbI dataset. References: Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush, “Towards AI-Complete Question Answering:A Set of Prerequisite Toy Tasks” Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus, “End-To-End Memory Networks” Reaches 98.6% accuracy on task ‘single_supporting_fact_10k’ after 120 epochs. Time per epoch: 3s on CPU (core i7). babi_tasks_1-20_v1-2 from __future__ import print_function from keras.models import Sequential, Model from keras.layers.embeddings import Embedding from keras.layers import Input, Activation, Dense, Permute, Dropout from keras.layers import add, dot, concatenate from keras.layers import LSTM from keras.utils.data_utils import get_file from keras.preprocessing.sequence import pad_sequences from functools import...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/KerasExample/babi_memnn.html",
        "teaser":null},{
        "title": "babi_rnn",
        
        "excerpt":
            "Trains two recurrent neural networks based upon a story and a question. The resulting merged vector is then queried to answer a range of bAbI tasks. The results are comparable to those for an LSTM model provided in Weston et al.: “Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks” http://arxiv.org/abs/1502.05698 Task Number FB LSTM Baseline Keras QA QA1 - Single Supporting Fact 50 52.1 QA2 - Two Supporting Facts 20 37.0 QA3 - Three Supporting Facts 20 20.5 QA4 - Two Arg. Relations 61 62.9 QA5 - Three Arg. Relations 70 61.9 QA6 - yes/No Questions 48 50.7...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/KerasExample/babi_rnn.html",
        "teaser":null},{
        "title": "image_ocr",
        
        "excerpt":
            "# -*- coding: utf-8 -*- ''' # Optical character recognition This example uses a convolutional stack followed by a recurrent stack and a CTC logloss function to perform optical character recognition of generated text images. I have no evidence of whether it actually learns general shapes of text, or just is able to recognize all the different fonts thrown at it...the purpose is more to demonstrate CTC inside of Keras. Note that the font list may need to be updated for the particular OS in use. This starts off with 4 letter words. For the first 12 epochs, the difficulty...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/KerasExample/image_ocr.html",
        "teaser":null},{
        "title": "imdb_bidirectional_lstm",
        
        "excerpt":
            "#Trains a Bidirectional LSTM on the IMDB sentiment classification task. Output after 4 epochs on CPU: ~0.8146 Time per epoch on CPU (Core i7): ~150s. from __future__ import print_function import numpy as np from keras.preprocessing import sequence from keras.models import Sequential from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional from keras.datasets import imdb max_features = 20000 # cut texts after this number of words # (among top max_features most common words) maxlen = 100 batch_size = 32 print('Loading data...') (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) print(len(x_train), 'train sequences') print(len(x_test), 'test sequences') print('Pad sequences (samples x time)') x_train = sequence.pad_sequences(x_train, maxlen=maxlen)...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/KerasExample/imdb_bidirectional_lstm.html",
        "teaser":null},{
        "title": "imdb_cnn",
        
        "excerpt":
            "''' #This example demonstrates the use of Convolution1D for text classification. Gets to 0.89 test accuracy after 2 epochs. &lt;/br&gt; 90s/epoch on Intel i5 2.4Ghz CPU. &lt;/br&gt; 10s/epoch on Tesla K40 GPU. ''' from __future__ import print_function from keras.preprocessing import sequence from keras.models import Sequential from keras.layers import Dense, Dropout, Activation from keras.layers import Embedding from keras.layers import Conv1D, GlobalMaxPooling1D from keras.datasets import imdb # set parameters: max_features = 5000 maxlen = 400 batch_size = 32 embedding_dims = 50 filters = 250 kernel_size = 3 hidden_dims = 250 epochs = 2 print('Loading data...') (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) print(len(x_train),...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/KerasExample/imdb_cnn.html",
        "teaser":null},{
        "title": "imdb_cnn_lstm",
        
        "excerpt":
            "''' #Train a recurrent convolutional network on the IMDB sentiment classification task. Gets to 0.8498 test accuracy after 2 epochs. 41 s/epoch on K520 GPU. ''' from __future__ import print_function from keras.preprocessing import sequence from keras.models import Sequential from keras.layers import Dense, Dropout, Activation from keras.layers import Embedding from keras.layers import LSTM from keras.layers import Conv1D, MaxPooling1D from keras.datasets import imdb # Embedding max_features = 20000 maxlen = 100 embedding_size = 128 # Convolution kernel_size = 5 filters = 64 pool_size = 4 # LSTM lstm_output_size = 70 # Training batch_size = 30 epochs = 2 ''' Note: batch_size is...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/KerasExample/imdb_cnn_lstm.html",
        "teaser":null},{
        "title": "imdb_lstm",
        
        "excerpt":
            "#Trains an LSTM model on the IMDB sentiment classification task. The dataset is actually too small for LSTM to be of any advantage compared to simpler, much faster methods such as TF-IDF + LogReg. Notes RNNs are tricky. Choice of batch size is important,choice of loss and optimizer is critical, etc.Some configurations won’t converge. LSTM loss decrease patterns during training can be quite different from what you see with CNNs/MLPs/etc. from __future__ import print_function from keras.preprocessing import sequence from keras.models import Sequential from keras.layers import Dense, Embedding from keras.layers import LSTM from keras.datasets import imdb max_features = 20000 # cut...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/KerasExample/imdb_lstm.html",
        "teaser":null},{
        "title": "KerasExample",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/KerasExample/index.html",
        "teaser":null},{
        "title": "lstm-seq2seq",
        
        "excerpt":
            "#Sequence to sequence example in Keras (character-level). This script demonstrates how to implement a basic character-level sequence-to-sequence model. We apply it to translating short English sentences into short French sentences, character-by-character. Note that it is fairly unusual to do character-level machine translation, as word-level models are more common in this domain. Summary of the algorithm We start with input sequences from a domain (e.g. English sentences) and corresponding target sequences from another domain(e.g. French sentences). An encoder LSTM turns input sequences to 2 state vectors(we keep the last LSTM state and discard the outputs). A decoder LSTM is trained to...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/KerasExample/lstm-seq2seq.html",
        "teaser":null},{
        "title": "mnist_acgan",
        
        "excerpt":
            "#Train an Auxiliary Classifier GAN (ACGAN) on the MNIST dataset. More details on Auxiliary Classifier GANs. You should start to see reasonable images after ~5 epochs, and good images by ~15 epochs. You should use a GPU, as the convolution-heavy operations are very slow on the CPU. Prefer the TensorFlow backend if you plan on iterating, as the compilation time can be a blocker using Theano. Timings: Hardware Backend Time / Epoch CPU TF 3 hrs Titan X (maxwell) TF 4 min Titan X (maxwell) TH 7 min Consult Auxiliary Classifier Generative Adversarial Networks in Keras for more information and...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/KerasExample/mnist_acgan.html",
        "teaser":null},{
        "title": "mnist_mlp",
        
        "excerpt":
            "'''Trains a simple deep NN on the MNIST dataset. Gets to 98.40% test accuracy after 20 epochs (there is *a lot* of margin for parameter tuning). 2 seconds per epoch on a K520 GPU. ''' from __future__ import print_function import keras from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense, Dropout from keras.optimizers import RMSprop batch_size = 128 num_classes = 10 epochs = 20 # the data, split between train and test sets (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.reshape(60000, 784) x_test = x_test.reshape(10000, 784) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/KerasExample/mnist_mlp.html",
        "teaser":null},{
        "title": "reuters_mlp",
        
        "excerpt":
            "Trains and evaluate a simple MLP on the Reuters newswire topic classification task. from __future__ import print_function import numpy as np import keras from keras.datasets import reuters from keras.models import Sequential from keras.layers import Dense, Dropout, Activation from keras.preprocessing.text import Tokenizer max_words = 1000 batch_size = 32 epochs = 5 print('Loading data...') (x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,test_split=0.2) print(len(x_train), 'train sequences') print(len(x_test), 'test sequences') num_classes = np.max(y_train) + 1 print(num_classes, 'classes') print('Vectorizing sequence data...') tokenizer = Tokenizer(num_words=max_words) x_train = tokenizer.sequences_to_matrix(x_train, mode='binary') x_test = tokenizer.sequences_to_matrix(x_test, mode='binary') print('x_train shape:', x_train.shape) print('x_test shape:', x_test.shape) print('Convert class vector to binary class matrix (for use...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/KerasExample/reuters_mlp.html",
        "teaser":null},{
        "title": "tensorboard_embeddings_mnist",
        
        "excerpt":
            "'''Trains a simple convnet on the MNIST dataset and embeds test data. The test data is embedded using the weights of the final dense layer, just before the classification head. This embedding can then be visualized using TensorBoard's Embedding Projector. ''' from __future__ import print_function from os import makedirs from os.path import exists, join import keras from keras.callbacks import TensorBoard from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense, Dropout, Flatten from keras.layers import Conv2D, MaxPooling2D from keras import backend as K import numpy as np batch_size = 128 num_classes = 10 epochs = 2 log_dir =...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/KerasExample/tensorboard_embeddings_mnist.html",
        "teaser":null},{
        "title": "00TensorFlow",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/index.html",
        "teaser":null},{
        "title": "keras-RNN实例",
        
        "excerpt":
            "Bitcoin and Ethereum price prediction with RNN LSTM Full Code Importing necessary libraries from keras.layers import SimpleRNN, Activation, Dense import numpy as np np.random.seed(1337) # for reproducibility from keras.datasets import mnist from keras.utils import np_utils from keras.models import Sequential from keras.optimizers import Adam from keras.datasets import mnist # download the mnist to the path '~/.keras/datasets/' if it is the first time to be called # X shape (60,000 28x28), y shape (10,000, ) (X_train, y_train), (X_test, y_test) = mnist.load_data() # data pre-processing X_train = X_train.reshape(-1,28,28) / 255. X_test = X_test.reshape(-1,28,28) / 255. # normalize y_train = np_utils.to_categorical(y_train, num_classes=10) y_test =...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/keras-RNN%E5%AE%9E%E4%BE%8B.html",
        "teaser":null},{
        "title": "keras日常小工具",
        
        "excerpt":
            "Batch Normalization   本文是谷歌机器学习工程师 Chris Rawles 撰写的一篇技术博文，探讨了如何在 TensorFlow 和 tf.keras 上利用 Batch Normalization 加快深度神经网络的训练。我们知道，深度神经网络一般非常复杂，即使是在当前高性能GPU的加持下，要想快速训练深度神经网络依然不容易。Batch Normalization 也许是一个不错的加速方法，本文介绍了它如何帮助解决梯度消失和梯度爆炸问题，并讨论了ReLu激活以及其他激活函数对于抵消梯度消失问题的作用。   Batch Normalization: 如何更快地训练深度神经网络  原文   keras输出中间层结果的2种方法  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/keras%E6%97%A5%E5%B8%B8%E5%B0%8F%E5%B7%A5%E5%85%B7.html",
        "teaser":null},{
        "title": "kera学习-更新",
        
        "excerpt":
            "参考资料 keras中文文档（官方） keras中文文档（非官方） 莫烦keras教程代码 莫烦keras视频教程 一些keras的例子 Keras开发者的github keras在imagenet以及VGG19上的应用 一个不负责任的Keras介绍（上） 一个不负责任的Keras介绍（中） 一个不负责任的Keras介绍（下） 使用keras构建流行的深度学习模型 Keras FAQ: Frequently Asked Keras Questions GPU并行训练 常见CNN结构的keras实现 实例 第一个例子：回归模型 首先我们在Keras中定义一个单层全连接网络，进行线性回归模型的训练： import numpy as np np.random.seed(1337) from keras.models import Sequential from keras.layers import Dense import matplotlib.pyplot as plt %matplotlib inline # 创建数据集 X = np.linspace(-1, 1, 200) np.random.shuffle(X) # 将数据集随机化 Y = 0.5 * X + 2 + np.random.normal(0, 0.05, (200, )) # 假设我们真实模型为：Y=0.5X+2 # 绘制数据集plt.scatter(X, Y) plt.show() X_train, Y_train = X[:160], Y[:160] # 把前160个数据放到训练集 X_test, Y_test = X[160:], Y[160:] # 把后40个点放到测试集 # 定义一个model， # Keras有两种类型的模型，序贯模型（Sequential）和函数式模型 # 比较常用的是Sequential，它是单输入单输出的 model = Sequential() # 通过add()方法一层层添加模型 # Dense是全连接层，第一层需要定义输入， # 第二层无需指定输入，一般第二层把第一层的输出作为输入 model.add(Dense(output_dim=1,...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/kera%E5%AD%A6%E4%B9%A0-%E6%9B%B4%E6%96%B0.html",
        "teaser":null},{
        "title": "kera实例二",
        
        "excerpt":
            "from keras.preprocessing.text import one_hot from keras.preprocessing.sequence import pad_sequences from keras.models import Sequential from keras.layers import Dense from keras.layers import Flatten from keras.layers.embeddings import Embedding # define documents docs = ['Well done!', 'Good work', 'Great effort', 'nice work', 'Excellent!', 'Weak', 'Poor effort!', 'not good', 'poor work', 'Could have done better.' ] # define class labels labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0] # integer encode the documents vocab_size = 50 encoded_docs = [one_hot(d, vocab_size) for d in docs] print(encoded_docs) # pad documents to a max length of 4 words max_length = 4 padded_docs = pad_sequences(encoded_docs, maxlen=max_length,...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/kera%E5%AE%9E%E4%BE%8B%E4%BA%8C.html",
        "teaser":null},{
        "title": "DIN模型理论和实践",
        
        "excerpt":
            "简介 Code实例 Data from notedata.dataset.ElectronicsData import ElectronicsData import notedata.utils as utils ele = ElectronicsData() utils.exists(ele.pkl_dataset) True DataInput &amp; DataInputTest import numpy as np class DataInput: def __init__(self, data, batch_size): self.batch_size = batch_size self.data = data self.epoch_size = len(self.data) // self.batch_size if self.epoch_size * self.batch_size &lt; len(self.data): self.epoch_size += 1 self.i = 0 def __iter__(self): return self def __next__(self): if self.i == self.epoch_size: raise StopIteration ts = self.data[self.i * self.batch_size: min((self.i + 1) * self.batch_size, len(self.data))] self.i += 1 u, i, y, sl = [], [], [], [] for t in ts: u.append(t[0]) i.append(t[2]) y.append(t[3]) sl.append(len(t[1])) max_sl = max(sl) hist_i =...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/recommendation/DIN%E6%A8%A1%E5%9E%8B%E7%90%86%E8%AE%BA%E5%92%8C%E5%AE%9E%E8%B7%B5.html",
        "teaser":null},{
        "title": "DeepFM模型理论和实践",
        
        "excerpt":
            " ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/recommendation/DeepFM%E6%A8%A1%E5%9E%8B%E7%90%86%E8%AE%BA%E5%92%8C%E5%AE%9E%E8%B7%B5.html",
        "teaser":null},{
        "title": "LR",
        
        "excerpt":
            "import tensorflow as tf import time from sklearn.metrics import roc_auc_score from notecode.data import data import pandas as pd x=tf.placeholder(tf.float32,shape=[None,108]) y=tf.placeholder(tf.float32,shape=[None]) m=1 learning_rate=0.3 w=tf.Variable(tf.random_normal([108,m], 0.0, 0.5),name='u') W=tf.matmul(x,w) p2=tf.reduce_sum(tf.nn.sigmoid(W),1) pred=p2 cost1=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=pred, labels=y)) cost=tf.add_n([cost1]) train_op = tf.train.FtrlOptimizer(learning_rate).minimize(cost) init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer()) sess = tf.Session() sess.run(init_op) train_x,train_y,test_x,test_y = data.get_adult_data() result = [] time_s=time.time() for epoch in range(0,10000): f_dict = {x: train_x, y: train_y} _, cost_, predict_= sess.run([train_op, cost, pred],feed_dict=f_dict) auc=roc_auc_score(train_y, predict_) time_t=time.time() if epoch % 100 == 0: f_dict = {x: test_x, y: test_y} _, cost_, predict_test = sess.run([train_op, cost, pred], feed_dict=f_dict) test_auc = roc_auc_score(test_y, predict_test) print(\"%d %ld cost:%f,train_auc:%f,test_auc:%f\" % (epoch, (time_t-time_s),cost_,auc,test_auc)) result.append([epoch, (time_t -...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/recommendation/LR.html",
        "teaser":null},{
        "title": "MLR模型理论和实践",
        
        "excerpt":
            "算法介绍 现阶段各CTR预估算法的不足 我们这里的现阶段，不是指的今时今日，而是阿里刚刚公开此算法的时间，大概就是去年的三四月份吧。 业界常用的CTR预估算法的不足如下表所示： 方法 简介 不足 逻辑回归 使用了Sigmoid函数将函数值映射到0~1区间作为CTR的预估值。LR这种线性模型很容易并行化，处理上亿条训练样本不是问题。 线性模型的学习能力有限，需要引入大量的领域知识来人工设计特征以及特征之间的交叉组合来间接补充算法的非线性学习能力，非常消耗人力和机器资源，迁移性不够友好。 Kernel方法 将低维特征映射到高维特征空间 复杂度太高而不易实现 树模型 如Facebook的GBDT+LR算法，有效地解决了LR模型的特征组合问题 是对历史行为的记忆，缺乏推广性，树模型只能学习到历史数据中的特定规则，对于新规则缺乏推广性 FM模型 自动学习高阶属性的权值，不用通过人工的方式选取特征来做交叉 FM模型只能拟合特定的非线性模式，常用的就是二阶FM 深度神经网络 使用神经网络拟合数据之间的高阶非线性关系，非线性拟合能力足够强 适合数据规律的、具备推广性的网络结构业界依然在探索中，尤其是要做到端到端规模化上线，这里面的技术挑战依然很大 那么挑战来了，如何设计算法从大规模数据中挖掘出具有推广性的非线性模式？ MLR算法 2011-2012年期间，阿里妈妈资深专家盖坤创新性地提出了MLR(mixed logistic regression)算法，引领了广告领域CTR预估算法的全新升级。MLR算法创新地提出并实现了直接在原始空间学习特征之间的非线性关系，基于数据自动发掘可推广的模式，相比于人工来说效率和精度均有了大幅提升。 MLR可以看做是对LR的一个自然推广，它采用分而治之的思路，用分片线性的模式来拟合高维空间的非线性分类面，其形式化表达如下： 其中u是聚类参数，决定了空间的划分，w是分类参数，决定空间内的预测。这里面超参数分片数m可以较好地平衡模型的拟合与推广能力。当m=1时MLR就退化为普通的LR，m越大模型的拟合能力越强，但是模型参数规模随m线性增长，相应所需的训练样本也随之增长。因此实际应用中m需要根据实际情况进行选择。例如，在阿里的场景中，m一般选择为12。下图中MLR模型用4个分片可以完美地拟合出数据中的菱形分类面。 在实际中，MLR算法常用的形式如下，使用softmax作为分片函数： 在这种情况下，MLR模型可以看作是一个FOE model： 关于损失函数的设计，阿里采用了 neg-likelihood loss function以及L1，L2正则，形式如下： 由于加入了正则项，MLR算法变的不再是平滑的凸函数，梯度下降法不再适用，因此模型参数的更新使用LBFGS和OWLQN的结合，具体的优化细节大家可以参考论文. MLR算法适合于工业级的大规模稀疏数据场景问题，如广告CTR预估。背后的优势体现在两个方面： 端到端的非线性学习：从模型端自动挖掘数据中蕴藏的非线性模式，省去了大量的人工特征设计，这 使得MLR算法可以端到端地完成训练，在不同场景中的迁移和应用非常轻松。 稀疏性：MLR在建模时引入了L1和L2,1范数正则，可以使得最终训练出来的模型具有较高的稀疏度， 模型的学习和在线预测性能更好。当然，这也对算法的优化求解带来了巨大的挑战。 import tensorflow as tf import time from sklearn.metrics import roc_auc_score import pandas as pd from notecode.data import data x = tf.placeholder(tf.float32,shape=[None,108]) y = tf.placeholder(tf.float32,shape=[None]) m = 2 learning_rate = 0.3 u = tf.Variable(tf.random_normal([108,m],0.0,0.5),name='u') w = tf.Variable(tf.random_normal([108,m],0.0,0.5),name='w') U = tf.matmul(x,u) p1 = tf.nn.softmax(U) W = tf.matmul(x,w) p2 = tf.nn.sigmoid(W) pred = tf.reduce_sum(tf.multiply(p1,p2),1) cost1=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=pred, labels=y)) cost=tf.add_n([cost1]) train_op = tf.train.FtrlOptimizer(learning_rate).minimize(cost) train_x,train_y,test_x,test_y...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/recommendation/MLR%E6%A8%A1%E5%9E%8B%E7%90%86%E8%AE%BA%E5%92%8C%E5%AE%9E%E8%B7%B5.html",
        "teaser":null},{
        "title": "recommendation",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/recommendation/index.html",
        "teaser":null},{
        "title": "0001-ml_introduction",
        
        "excerpt":
            "title: tf-example-ml介绍 author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example Machine Learning Prior to start browsing the examples, it may be useful that you get familiar with machine learning, as TensorFlow is mostly used for machine learning tasks (especially Neural Networks). You can find below a list of useful links, that can give you the basic knowledge required for this TensorFlow Tutorial. Machine Learning An Introduction to Machine Learning Theory and Its Applications: A Visual Tutorial with Examples A Gentle Guide to Machine Learning A Visual Introduction to Machine Learning Introduction to Machine Learning Deep Learning &amp; Neural Networks An...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0001-ml_introduction.html",
        "teaser":null},{
        "title": "0002-mnist_dataset_intro",
        
        "excerpt":
            "title: tf-example-mnist数据集介绍 author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example MNIST Dataset Introduction Most examples are using MNIST dataset of handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flatten and converted to a 1-D numpy array of 784 features (28*28). Overview Usage In our examples, we are using TensorFlow input_data.py script to load that dataset. It is quite useful for managing our data, and handle: Dataset downloading Loading the...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0002-mnist_dataset_intro.html",
        "teaser":null},{
        "title": "0101-helloworld",
        
        "excerpt":
            "   title: tf-example-01.01-helloworld   author: niult   date: 2019-01-16   category: tf-example   tags: python,numpy,tf-example           import tensorflow as tf                    # Simple hello world using TensorFlow  # Create a Constant op # The op is added as a node to the default graph. # # The value returned by the constructor represents the output # of the Constant op.  hello = tf.constant('Hello, TensorFlow!')                    # Start tf session sess = tf.Session()                    # Run graph print(sess.run(hello))                        Hello, TensorFlow!                 ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0101-helloworld.html",
        "teaser":null},{
        "title": "0102-basic_eager_api",
        
        "excerpt":
            "title: tf-example-01.02-basic-eager-api author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example Basic introduction to TensorFlow’s Eager API A simple introduction to get started with TensorFlow’s Eager API. Author: Aymeric Damien Project: https://github.com/aymericdamien/TensorFlow-Examples/ What is TensorFlow’s Eager API ? Eager execution is an imperative, define-by-run interface where operations are executed immediately as they are called from Python. This makes it easier to get started with TensorFlow, and can make research and development more intuitive. A vast majority of the TensorFlow API remains the same whether eager execution is enabled or not. As a result, the exact same code that constructs TensorFlow graphs...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0102-basic_eager_api.html",
        "teaser":null},{
        "title": "0103-basic_operations",
        
        "excerpt":
            "title: tf-example-01.03-basic-operations author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example # Basic Operations example using TensorFlow library. # Author: Aymeric Damien # Project: https://github.com/aymericdamien/TensorFlow-Examples/ import tensorflow as tf # Basic constant operations # The value returned by the constructor represents the output # of the Constant op. a = tf.constant(2) b = tf.constant(3) # Launch the default graph. with tf.Session() as sess: print \"a: %i\" % sess.run(a), \"b: %i\" % sess.run(b) print \"Addition with constants: %i\" % sess.run(a+b) print \"Multiplication with constants: %i\" % sess.run(a*b) a=2, b=3 Addition with constants: 5 Multiplication with constants: 6 # Basic Operations with variable...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0103-basic_operations.html",
        "teaser":null},{
        "title": "0201-linear_regression_eager_api",
        
        "excerpt":
            "title: tf-example-02.01-linear_regression_eager_api author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example Linear Regression with Eager API A linear regression implemented using TensorFlow’s Eager API. Author: Aymeric Damien Project: https://github.com/aymericdamien/TensorFlow-Examples/ from __future__ import absolute_import, division, print_function import matplotlib.pyplot as plt import numpy as np import tensorflow as tf # Set Eager API tf.enable_eager_execution() tfe = tf.contrib.eager # Training Data train_X = [3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59, 2.167, 7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1] train_Y = [1.7, 2.76, 2.09, 3.19, 1.694, 1.573, 3.366, 2.596, 2.53, 1.221, 2.827, 3.465, 1.65, 2.904, 2.42, 2.94, 1.3] n_samples = len(train_X) # Parameters...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0201-linear_regression_eager_api.html",
        "teaser":null},{
        "title": "0202-linear_regression",
        
        "excerpt":
            "title: tf-example-02.02-linear_regression author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example Linear Regression Example A linear regression learning algorithm example using TensorFlow library. import tensorflow as tf import numpy import matplotlib.pyplot as plt rng = numpy.random # Parameters learning_rate = 0.01 training_epochs = 1000 display_step = 50 # Training Data train_X = numpy.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167, 7.042,10.791,5.313,7.997,5.654,9.27,3.1]) train_Y = numpy.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221, 2.827,3.465,1.65,2.904,2.42,2.94,1.3]) n_samples = train_X.shape[0] # tf Graph Input X = tf.placeholder(\"float\") Y = tf.placeholder(\"float\") # Set model weights W = tf.Variable(rng.randn(), name=\"weight\") b = tf.Variable(rng.randn(), name=\"bias\") # Construct a linear model pred = tf.add(tf.multiply(X, W), b) # Mean squared error cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0202-linear_regression.html",
        "teaser":null},{
        "title": "0203-logistic_regression_eager_api",
        
        "excerpt":
            "title: tf-example-02.03-logistic_regression_eager_api author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example Logistic Regression with Eager API A logistic regression implemented using TensorFlow’s Eager API. Author: Aymeric Damien Project: https://github.com/aymericdamien/TensorFlow-Examples/ MNIST Dataset Overview This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flattened and converted to a 1-D numpy array of 784 features (28*28). More info: http://yann.lecun.com/exdb/mnist/ from __future__ import absolute_import, division, print_function import tensorflow as tf...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0203-logistic_regression_eager_api.html",
        "teaser":null},{
        "title": "0204-nearest_neighbor",
        
        "excerpt":
            "title: tf-example-02.04-nearest_neighbor author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example Nearest Neighbor Example A nearest neighbor learning algorithm example using TensorFlow library. This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/) Author: Aymeric Damien Project: https://github.com/aymericdamien/TensorFlow-Examples/ import numpy as np import tensorflow as tf # Import MINST data from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True) Extracting MNIST_data/train-images-idx3-ubyte.gz Extracting MNIST_data/train-labels-idx1-ubyte.gz Extracting MNIST_data/t10k-images-idx3-ubyte.gz Extracting MNIST_data/t10k-labels-idx1-ubyte.gz # In this example, we limit mnist data Xtr, Ytr = mnist.train.next_batch(5000) #5000 for training (nn candidates) Xte, Yte = mnist.test.next_batch(200) #200 for testing # tf Graph Input xtr = tf.placeholder(\"float\", [None, 784]) xte...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0204-nearest_neighbor.html",
        "teaser":null},{
        "title": "0205-kmeans",
        
        "excerpt":
            "title: tf-example-02.05-kmeans author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example K-Means Example Implement K-Means algorithm with TensorFlow, and apply it to classify handwritten digit images. This example is using the MNIST database of handwritten digits as training samples (http://yann.lecun.com/exdb/mnist/). Note: This example requires TensorFlow v1.1.0 or over. Author: Aymeric Damien Project: https://github.com/aymericdamien/TensorFlow-Examples/ from __future__ import print_function import numpy as np import tensorflow as tf from tensorflow.contrib.factorization import KMeans # Ignore all GPUs, tf random forest does not benefit from it. import os os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" # Import MNIST data from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(\"tmp/data/\", one_hot=True) full_data_x = mnist.train.images...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0205-kmeans.html",
        "teaser":null},{
        "title": "0206-LR",
        
        "excerpt":
            "title: tf-example-02.06-LR author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example Logistic Regression Example A logistic regression learning algorithm example using TensorFlow library. Author: Aymeric Damien Project: https://github.com/aymericdamien/TensorFlow-Examples/ MNIST Dataset Overview This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flattened and converted to a 1-D numpy array of 784 features (28*28). More info: http://yann.lecun.com/exdb/mnist/ import tensorflow as tf # Import MINST data from tensorflow.examples.tutorials.mnist import...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0206-LR.html",
        "teaser":null},{
        "title": "0207-GBDT",
        
        "excerpt":
            "title: tf-example-02.07-GBDT author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example Gradient Boosted Decision Tree 利用张力流实现梯度提升决策树(GBDT)，对手写数字图像进行分类。这个示例使用手写数字的MNIST数据库作为训练样本 (http://yann.lecun.com/exdb/mnist/). from __future__ import print_function import tensorflow as tf from tensorflow.contrib.boosted_trees.estimator_batch.estimator import GradientBoostedDecisionTreeClassifier from tensorflow.contrib.boosted_trees.proto import learner_pb2 as gbdt_learner # Ignore all GPUs (current TF GBDT does not support GPU). import os os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" # Import MNIST data # Set verbosity to display errors only (Remove this line for showing warnings) tf.logging.set_verbosity(tf.logging.ERROR) from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(\"tmp/data/\", one_hot=False,source_url='http://yann.lecun.com/exdb/mnist/') Extracting tmp/data/train-images-idx3-ubyte.gz Extracting tmp/data/train-labels-idx1-ubyte.gz Extracting tmp/data/t10k-images-idx3-ubyte.gz Extracting tmp/data/t10k-labels-idx1-ubyte.gz # Parameters batch_size = 4096 # The number of samples per batch num_classes = 10 #...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0207-GBDT.html",
        "teaser":null},{
        "title": "0208-word2vec",
        
        "excerpt":
            "title: tf-example-02.08-word2vec author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example Word2Vec (Word Embedding) Implement Word2Vec algorithm to compute vector representations of words. This example is using a small chunk of Wikipedia articles to train from. More info: Mikolov, Tomas et al. “Efficient Estimation of Word Representations in Vector Space.”, 2013 from __future__ import division, print_function, absolute_import import collections import os import random import urllib import zipfile import numpy as np import tensorflow as tf # Training Parameters learning_rate = 0.1 batch_size = 128 num_steps = 3000000 display_step = 10000 eval_step = 200000 # Evaluation Parameters eval_words = ['five', 'of', 'going',...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0208-word2vec.html",
        "teaser":null},{
        "title": "0209-random_forest",
        
        "excerpt":
            "title: tf-example-02.09-random_forest author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example Random Forest Example Implement Random Forest algorithm with TensorFlow, and apply it to classify handwritten digit images. This example is using the MNIST database of handwritten digits as training samples (http://yann.lecun.com/exdb/mnist/). Project: https://github.com/aymericdamien/TensorFlow-Examples/ from __future__ import print_function import tensorflow as tf from tensorflow.python.ops import resources from tensorflow.contrib.tensor_forest.python import tensor_forest # Ignore all GPUs, tf random forest does not benefit from it. import os os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" # Import MNIST data from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(\"tmp/data/\", one_hot=False) WARNING:tensorflow:From &lt;ipython-input-2-0a3c50fadd0e&gt;:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0209-random_forest.html",
        "teaser":null},{
        "title": "0301-autoencoder",
        
        "excerpt":
            "title: tf-example-03.01-autoencoder author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example Auto-Encoder Example Build a 2 layers auto-encoder with TensorFlow to compress images to a lower latent space and then reconstruct them. Project: https://github.com/aymericdamien/TensorFlow-Examples/ Auto-Encoder Overview References: Gradient-based learning applied to document recognition. Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Proceedings of the IEEE, 86(11):2278-2324, November 1998. MNIST Dataset Overview This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0301-autoencoder.html",
        "teaser":null},{
        "title": "0302-bidirectional_rnn",
        
        "excerpt":
            "title: tf-example-03.02-bidirectional_rnn author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example Bi-directional Recurrent Neural Network Example Build a bi-directional recurrent neural network (LSTM) with TensorFlow. Project: https://github.com/aymericdamien/TensorFlow-Examples/ BiRNN Overview References: Long Short Term Memory, Sepp Hochreiter &amp; Jurgen Schmidhuber, Neural Computation 9(8): 1735-1780, 1997. MNIST Dataset Overview This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flattened and converted to a 1-D numpy array of 784...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0302-bidirectional_rnn.html",
        "teaser":null},{
        "title": "0303-convolutional_network_raw",
        
        "excerpt":
            "title: tf-example-03.03-convolutional_network_raw author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example Convolutional Neural Network Example Build a convolutional neural network with TensorFlow. Project: https://github.com/aymericdamien/TensorFlow-Examples/ CNN Overview MNIST Dataset Overview This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flattened and converted to a 1-D numpy array of 784 features (28*28). More info: http://yann.lecun.com/exdb/mnist/ from __future__ import division, print_function, absolute_import import tensorflow as tf # Import MNIST...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0303-convolutional_network_raw.html",
        "teaser":null},{
        "title": "0304-convolutional_network",
        
        "excerpt":
            "title: tf-example-03.04-convolutional_network author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example Convolutional Neural Network Example CNN Overview MNIST Dataset Overview This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flattened and converted to a 1-D numpy array of 784 features (28*28). More info: http://yann.lecun.com/exdb/mnist/ from __future__ import division, print_function, absolute_import # Import MNIST data from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False) import tensorflow as tf...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0304-convolutional_network.html",
        "teaser":null},{
        "title": "0305-dcgan",
        
        "excerpt":
            "title: tf-example-03.05-dcgan author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example Deep Convolutional Generative Adversarial Network Example Build a deep convolutional generative adversarial network (DCGAN) to generate digit images from a noise distribution with TensorFlow. Author: Aymeric Damien Project: https://github.com/aymericdamien/TensorFlow-Examples/ DCGAN Overview References: Unsupervised representation learning with deep convolutional generative adversarial networks. A Radford, L Metz, S Chintala, 2016. Understanding the difficulty of training deep feedforward neural networks. X Glorot, Y Bengio. Aistats 9, 249-256 Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. Sergey Ioffe, Christian Szegedy. 2015. MNIST Dataset Overview This example is using MNIST handwritten digits....",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0305-dcgan.html",
        "teaser":null},{
        "title": "0306-dynamic_rnn",
        
        "excerpt":
            "title: tf-example-03.06-dynamic_rnn author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example Dynamic Recurrent Neural Network. TensorFlow implementation of a Recurrent Neural Network (LSTM) that performs dynamic computation over sequences with variable length. This example is using a toy dataset to classify linear sequences. The generated sequences have variable length. Author: Aymeric Damien Project: https://github.com/aymericdamien/TensorFlow-Examples/ RNN Overview References: Long Short Term Memory, Sepp Hochreiter &amp; Jurgen Schmidhuber, Neural Computation 9(8): 1735-1780, 1997. from __future__ import print_function import tensorflow as tf import random # ==================== # TOY DATA GENERATOR # ==================== class ToySequenceData(object): \"\"\" Generate sequence of data with dynamic length. This class...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0306-dynamic_rnn.html",
        "teaser":null},{
        "title": "0307-gan",
        
        "excerpt":
            "title: tf-example-03.07-gan author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example Generative Adversarial Network Example Build a generative adversarial network (GAN) to generate digit images from a noise distribution with TensorFlow. Author: Aymeric Damien Project: https://github.com/aymericdamien/TensorFlow-Examples/ GAN Overview References: Generative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, Y. Bengio. Advances in neural information processing systems, 2672-2680. Understanding the difficulty of training deep feedforward neural networks. X Glorot, Y Bengio. Aistats 9, 249-256 Other tutorials: Generative Adversarial Networks Explained. Kevin Frans. MNIST Dataset Overview This example is using MNIST handwritten digits. The dataset contains 60,000...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0307-gan.html",
        "teaser":null},{
        "title": "0308-neural_network_eager_api",
        
        "excerpt":
            "title: tf-example-03.08-neural_network_eager_api author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example Neural Network with Eager API Build a 2-hidden layers fully connected neural network (a.k.a multilayer perceptron) with TensorFlow’s Eager API. This example is using some of TensorFlow higher-level wrappers (tf.estimators, tf.layers, tf.metrics, …), you can check ‘neural_network_raw’ example for a raw, and more detailed TensorFlow implementation. Author: Aymeric Damien Project: https://github.com/aymericdamien/TensorFlow-Examples/ Neural Network Overview MNIST Dataset Overview This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0308-neural_network_eager_api.html",
        "teaser":null},{
        "title": "0309-neural_network_raw",
        
        "excerpt":
            "title: tf-example-03.09-neural_network_raw author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example Neural Network Example Build a 2-hidden layers fully connected neural network (a.k.a multilayer perceptron) with TensorFlow. Author: Aymeric Damien Project: https://github.com/aymericdamien/TensorFlow-Examples/ Neural Network Overview MNIST Dataset Overview This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flattened and converted to a 1-D numpy array of 784 features (28*28). More info: http://yann.lecun.com/exdb/mnist/ from __future__ import print_function...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0309-neural_network_raw.html",
        "teaser":null},{
        "title": "0310-neural_network",
        
        "excerpt":
            "title: tf-example-03.10-neural_network author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example Neural Network Example Build a 2-hidden layers fully connected neural network (a.k.a multilayer perceptron) with TensorFlow. This example is using some of TensorFlow higher-level wrappers (tf.estimators, tf.layers, tf.metrics, …), you can check ‘neural_network_raw’ example for a raw, and more detailed TensorFlow implementation. Author: Aymeric Damien Project: https://github.com/aymericdamien/TensorFlow-Examples/ Neural Network Overview MNIST Dataset Overview This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0310-neural_network.html",
        "teaser":null},{
        "title": "0311-recurrent_network",
        
        "excerpt":
            "title: tf-example-03.11-recurrent_network author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example Recurrent Neural Network Example Build a recurrent neural network (LSTM) with TensorFlow. Author: Aymeric Damien Project: https://github.com/aymericdamien/TensorFlow-Examples/ RNN Overview References: Long Short Term Memory, Sepp Hochreiter &amp; Jurgen Schmidhuber, Neural Computation 9(8): 1735-1780, 1997. MNIST Dataset Overview This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flattened and converted to a 1-D numpy array of...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0311-recurrent_network.html",
        "teaser":null},{
        "title": "0312-variational_autoencoder",
        
        "excerpt":
            "title: tf-example-03.12-variational_autoencoder author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example Variational Auto-Encoder Example Build a variational auto-encoder (VAE) to generate digit images from a noise distribution with TensorFlow. Author: Aymeric Damien Project: https://github.com/aymericdamien/TensorFlow-Examples/ VAE Overview References: Auto-Encoding Variational Bayes The International Conference on Learning Representations (ICLR), Banff, 2014. D.P. Kingma, M. Welling Understanding the difficulty of training deep feedforward neural networks. X Glorot, Y Bengio. Aistats 9, 249-256 Other tutorials: Variational Auto Encoder Explained. Kevin Frans. MNIST Dataset Overview This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0312-variational_autoencoder.html",
        "teaser":null},{
        "title": "0401-save_restore_model",
        
        "excerpt":
            "title: tf-example-04.01-save_restore_model author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example Save &amp; Restore a Model Save and Restore a model using TensorFlow. This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/). from __future__ import print_function # Import MINST data from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True) import tensorflow as tf Extracting MNIST_data/train-images-idx3-ubyte.gz Extracting MNIST_data/train-labels-idx1-ubyte.gz Extracting MNIST_data/t10k-images-idx3-ubyte.gz Extracting MNIST_data/t10k-labels-idx1-ubyte.gz # Parameters learning_rate = 0.001 batch_size = 100 display_step = 1 model_path = \"/tmp/model.ckpt\" # Network Parameters n_hidden_1 = 256 # 1st layer number of features n_hidden_2 = 256 # 2nd layer number of features n_input = 784 #...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0401-save_restore_model.html",
        "teaser":null},{
        "title": "0402-tensorboard_advanced",
        
        "excerpt":
            "title: tf-example-04.02-tensorboard_advanced author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example Tensorboard Advanced Advanced visualization using Tensorboard (weights, gradient, …). This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/). from __future__ import print_function import tensorflow as tf # Import MNIST data from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True) Extracting /tmp/data/train-images-idx3-ubyte.gz Extracting /tmp/data/train-labels-idx1-ubyte.gz Extracting /tmp/data/t10k-images-idx3-ubyte.gz Extracting /tmp/data/t10k-labels-idx1-ubyte.gz # Parameters learning_rate = 0.01 training_epochs = 25 batch_size = 100 display_step = 1 logs_path = '/tmp/tensorflow_logs/example/' # Network Parameters n_hidden_1 = 256 # 1st layer number of features n_hidden_2 = 256 # 2nd layer number of features n_input = 784 #...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0402-tensorboard_advanced.html",
        "teaser":null},{
        "title": "0403-tensorboard_basic",
        
        "excerpt":
            "title: tf-example-04.03-tensorboard_basic author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example Tensorboard Basics Graph and Loss visualization using Tensorboard. This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/). from __future__ import print_function import tensorflow as tf # Import MINST data from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True) # Parameters learning_rate = 0.01 training_epochs = 25 batch_size = 100 display_epoch = 1 logs_path = '/tmp/tensorflow_logs/example/' # tf Graph Input # mnist data image of shape 28*28=784 x = tf.placeholder(tf.float32, [None, 784], name='InputData') # 0-9 digits recognition =&gt; 10 classes y = tf.placeholder(tf.float32, [None, 10], name='LabelData') # Set model weights...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0403-tensorboard_basic.html",
        "teaser":null},{
        "title": "0501-build_an_image_dataset",
        
        "excerpt":
            "title: tf-example-05.01-build_an_image_dataset author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example Build an Image Dataset in TensorFlow. For this example, you need to make your own set of images (JPEG). We will show 2 different ways to build that dataset: From a root folder, that will have a sub-folder containing images for each class ROOT_FOLDER |-------- SUBFOLDER (CLASS 0) | | | | ----- image1.jpg | | ----- image2.jpg | | ----- etc... | |-------- SUBFOLDER (CLASS 1) | | | | ----- image1.jpg | | ----- image2.jpg | | ----- etc... From a plain text file, that will list all...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0501-build_an_image_dataset.html",
        "teaser":null},{
        "title": "0502-tensorflow_dataset_api",
        
        "excerpt":
            "title: tf-example-05.02-tensorflow_dataset_api author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example TensorFlow Dataset API In this example, we will show how to load numpy array data into the new TensorFlow ‘Dataset’ API. The Dataset API implements an optimized data pipeline with queues, that make data processing and training faster (especially on GPU). import tensorflow as tf # Import MNIST data (Numpy format) from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True) Extracting /tmp/data/train-images-idx3-ubyte.gz Extracting /tmp/data/train-labels-idx1-ubyte.gz Extracting /tmp/data/t10k-images-idx3-ubyte.gz Extracting /tmp/data/t10k-labels-idx1-ubyte.gz # Parameters learning_rate = 0.01 num_steps = 1000 batch_size = 128 display_step = 100 # Network Parameters n_input = 784 # MNIST data...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0502-tensorflow_dataset_api.html",
        "teaser":null},{
        "title": "0601-multigpu_basics",
        
        "excerpt":
            "title: tf-example-06.01-multigpu_basics author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example Multi-GPU Basics Basic Multi-GPU computation example using TensorFlow library. This tutorial requires your machine to have 2 GPUs “/cpu:0”: The CPU of your machine. “/gpu:0”: The first GPU of your machine “/gpu:1”: The second GPU of your machine For this example, we are using 2 GTX-980 import numpy as np import tensorflow as tf import datetime #Processing Units logs log_device_placement = True #num of multiplications to perform n = 10 # Example: compute A^n + B^n on 2 GPUs # Create random large matrix A = np.random.rand(1e4, 1e4).astype('float32') B =...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0601-multigpu_basics.html",
        "teaser":null},{
        "title": "0602-multigpu_cnn",
        
        "excerpt":
            "title: tf-example-06.02-multigpu_cnn author: niult date: 2019-01-16 category: tf-example tags: python,numpy,tf-example Multi-GPU Training Example Training with multiple GPU cards In this example, we are using data parallelism to split the training accross multiple GPUs. Each GPU has a full replica of the neural network model, and the weights (i.e. variables) are updated synchronously by waiting that each GPU process its batch of data. First, each GPU process a distinct batch of data and compute the corresponding gradients, then, all gradients are accumulated in the CPU and averaged. The model weights are finally updated with the gradients averaged, and the new model...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/0602-multigpu_cnn.html",
        "teaser":null},{
        "title": "tensorflowExample",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/00TensorFlow/tensorflowExample/index.html",
        "teaser":null},{
        "title": "13推荐系统",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/13%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/index.html",
        "teaser":null},{
        "title": "test",
        
        "excerpt":
            "@hanbingtao 2017-08-28 19:54 字数 23656 阅读 125226 零基础入门深度学习(5) - 循环神经网络 机器学习 深度学习入门 无论即将到来的是大数据时代还是人工智能时代，亦或是传统行业使用人工智能在云上处理大数据的时代，作为一个有理想有追求的程序员，不懂深度学习（Deep Learning）这个超热的技术，会不会感觉马上就out了？现在救命稻草来了，《零基础入门深度学习》系列文章旨在讲帮助爱编程的你从零基础达到入门级水平。零基础意味着你不需要太多的数学知识，只要会写程序就行了，没错，这是专门为程序员写的文章。虽然文中会有很多公式你也许看不懂，但同时也会有更多的代码，程序员的你一定能看懂的（我周围是一群狂热的Clean Code程序员，所以我写的代码也不会很差）。 文章列表 零基础入门深度学习(1) - 感知器 零基础入门深度学习(2) - 线性单元和梯度下降 零基础入门深度学习(3) - 神经网络和反向传播算法 零基础入门深度学习(4) - 卷积神经网络 零基础入门深度学习(5) - 循环神经网络 零基础入门深度学习(6) - 长短时记忆网络(LSTM) 零基础入门深度学习(7) - 递归神经网络 往期回顾 在前面的文章系列文章中，我们介绍了全连接神经网络和卷积神经网络，以及它们的训练和使用。他们都只能单独的取处理一个个的输入，前一个输入和后一个输入是完全没有关系的。但是，某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的。比如，当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个序列；当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要分析这些帧连接起来的整个序列。这时，就需要用到深度学习领域中另一类非常重要神经网络：循环神经网络(Recurrent Neural Network)。RNN种类很多，也比较绕脑子。不过读者不用担心，本文将一如既往的对复杂的东西剥茧抽丝，帮助您理解RNNs以及它的训练算法，并动手实现一个循环神经网络。 语言模型 RNN是在自然语言处理领域中最先被用起来的，比如，RNN可以为语言模型来建模。那么，什么是语言模型呢？ 我们可以和电脑玩一个游戏，我们写出一个句子前面的一些词，然后，让电脑帮我们写下接下来的一个词。比如下面这句： 我昨天上学迟到了，老师批评了____。 我们给电脑展示了这句话前面这些词，然后，让电脑写下接下来的一个词。在这个例子中，接下来的这个词最有可能是『我』，而不太可能是『小明』，甚至是『吃饭』。 语言模型就是这样的东西：给定一个一句话前面的部分，预测接下来最有可能的一个词是什么。 语言模型是对一种语言的特征进行建模，它有很多很多用处。比如在语音转文本(STT)的应用中，声学模型输出的结果，往往是若干个可能的候选词，这时候就需要语言模型来从这些候选词中选择一个最可能的。当然，它同样也可以用在图像到文本的识别中(OCR)。 使用RNN之前，语言模型主要是采用N-Gram。N可以是一个自然数，比如2或者3。它的含义是，假设一个词出现的概率只与前面N个词相关。我们以2-Gram为例。首先，对前面的一句话进行切词： 我 昨天 上学 迟到 了 ，老师 批评 了 __。 如果用2-Gram进行建模，那么电脑在预测的时候，只会看到前面的『了』，然后，电脑会在语料库中，搜索『了』后面最可能的一个词。不管最后电脑选的是不是『我』，我们都知道这个模型是不靠谱的，因为『了』前面说了那么一大堆实际上是没有用到的。如果是3-Gram模型呢，会搜索『批评了』后面最可能的词，感觉上比2-Gram靠谱了不少，但还是远远不够的。因为这句话最关键的信息『我』，远在9个词之前！ 现在读者可能会想，可以提升继续提升N的值呀，比如4-Gram、5-Gram…….。实际上，这个想法是没有实用性的。因为我们想处理任意长度的句子，N设为多少都不合适；另外，模型的大小和N的关系是指数级的，4-Gram模型就会占用海量的存储空间。 所以，该轮到RNN出场了，RNN理论上可以往前看(往后看)任意多个词。 循环神经网络是啥 循环神经网络种类繁多，我们先从最简单的基本循环神经网络开始吧。 基本循环神经网络 下图是一个简单的循环神经网络如，它由输入层、一个隐藏层和一个输出层组成： 纳尼？！相信第一次看到这个玩意的读者内心和我一样是崩溃的。因为循环神经网络实在是太难画出来了，网上所有大神们都不得不用了这种抽象艺术手法。不过，静下心来仔细看看的话，其实也是很好理解的。如果把上面有W的那个带箭头的圈去掉，它就变成了最普通的全连接神经网络。x是一个向量，它表示输入层的值（这里面没有画出来表示神经元节点的圆圈）；s是一个向量，它表示隐藏层的值（这里隐藏层面画了一个节点，你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；U是输入层到隐藏层的权重矩阵（读者可以回到第三篇文章零基础入门深度学习(3) - 神经网络和反向传播算法，看看我们是怎样用矩阵来表示全连接神经网络的计算的）；o也是一个向量，它表示输出层的值；V是隐藏层到输出层的权重矩阵。那么，现在我们来看看W是什么。循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层的值s。权重矩阵 W就是隐藏层上一次的值作为这一次的输入的权重。 如果我们把上面的图展开，循环神经网络也可以画成下面这个样子： 现在看上去就比较清楚了，这个网络在t时刻接收到输入之后，隐藏层的值是，输出值是。关键一点是，的值不仅仅取决于，还取决于。我们可以用下面的公式来表示循环神经网络的计算方法： 式 式 式1是输出层的计算公式，输出层是一个全连接层，也就是它的每个节点都和隐藏层的每个节点相连。V是输出层的权重矩阵，g是激活函数。式2是隐藏层的计算公式，它是循环层。U是输入x的权重矩阵，W是上一次的值作为这一次的输入的权重矩阵，f是激活函数。 从上面的公式我们可以看出，循环层和全连接层的区别就是循环层多了一个权重矩阵 W。 如果反复把式2带入到式1，我们将得到： 从上面可以看出，循环神经网络的输出值，是受前面历次输入值、、、、…影响的，这就是为什么循环神经网络可以往前看任意多个输入值的原因。 双向循环神经网络 对于语言模型来说，很多时候光看前面的词是不够的，比如下面这句话： 我的手机坏了，我打算____一部新手机。 可以想象，如果我们只看横线前面的词，手机坏了，那么我是打算修一修？换一部新的？还是大哭一场？这些都是无法确定的。但如果我们也看到了横线后面的词是『一部新手机』，那么，横线上的词填『买』的概率就大得多了。 在上一小节中的基本循环神经网络是无法对此进行建模的，因此，我们需要双向循环神经网络，如下图所示： 当遇到这种从未来穿越回来的场景时，难免处于懵逼的状态。不过我们还是可以用屡试不爽的老办法：先分析一个特殊场景，然后再总结一般规律。我们先考虑上图中，的计算。 从上图可以看出，双向卷积神经网络的隐藏层要保存两个值，一个A参与正向计算，另一个值A’参与反向计算。最终的输出值取决于和。其计算方法为： 和则分别计算： 现在，我们已经可以看出一般的规律：正向计算时，隐藏层的值与有关；反向计算时，隐藏层的值与有关；最终的输出取决于正向和反向计算的加和。现在，我们仿照式1和式2，写出双向循环神经网络的计算方法： 从上面三个公式我们可以看到，正向计算和反向计算不共享权重，也就是说U和U’、W和W’、V和V’都是不同的权重矩阵。 深度循环神经网络 前面我们介绍的循环神经网络只有一个隐藏层，我们当然也可以堆叠两个以上的隐藏层，这样就得到了深度循环神经网络。如下图所示： 我们把第i个隐藏层的值表示为、，则深度循环神经网络的计算方式可以表示为： 循环神经网络的训练 循环神经网络的训练算法：BPTT BPTT算法是针对循环层的训练算法，它的基本原理和BP算法是一样的，也包含同样的三个步骤： 前向计算每个神经元的输出值； 反向计算每个神经元的误差项值，它是误差函数E对神经元j的加权输入的偏导数； 计算每个权重的梯度。 最后再用随机梯度下降算法更新权重。 循环层如下图所示： 前向计算...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/13%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/test.html",
        "teaser":null},{
        "title": "ES查询",
        
        "excerpt":
            "Elasticsearch 是功能强大的全文搜索引擎，用它的目的就是为了能快速的查询你想好要的数据 基本查询：利用Elasticsearch内置查询条件进行查询 组合查询：把多个基本查询组合在一起的复合性查询 过滤：查询的同时，通过filter条件 在不影响打分的情况下筛选出想要的数据 # 基本查询： 1.term,terms查询 2.from,size 3.返回版本号_version 4.match查询 5.升序降序 6.prefix前缀匹配查询 7.range 范围查询 8.wildcard 通配符查询 9.fuzzy 模糊查询 10.more_like &amp; more_like_this_field查询 查询某一个索引的映射信息： # term 查询 term查询：查询 preview 字段里有某个关键词的文档 GET /library/books/_search { “query”:{ “term”:” “preview”:”elasticsearch” } } } terms 查询：查询某个字段里有多个关键词的文档 miniumum_match:最小匹配集： 设置为1.说明两个关键词里面最少有一个 设置为2. 说明文档里这2个关键词都得存在 GET /library/books/_search { “query”{ “terms”{ “preview”:[“elasticsearch”,”book”] “miniumu_match”:2 } } } from,size 控制查询返回的数量 #相当于mysql 里面的limit from：从哪个结果开始返回 size:定义返回最大的结果数 GET /library/books/_search { “from”:1, ##从第二个开始 “size”:2, ##返回2个 “query”:{ “term”:” “preview”:”elasticsearch” } } } 返回版本号_version GET /library/books/_search { “version”:true, #设置为true 显示版本 “query”:{ “term”:” “preview”:”elasticsearch” } } } match查询 match 查询可以接受文字，数字日期等数据类型 match 和term的区别是，match 查询的时候，elasticsearch会根据你给定的字段提供合适的分析器，而term查询时不会有分析器分析的过程 GET /library/books/_search { “query”{ “match”{ “preview”:”elasticsearch” ##字符串类型...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/ES%E6%9F%A5%E8%AF%A2.html",
        "teaser":null},{
        "title": "README",
        
        "excerpt":
            "Machine-learning-in-action  个人使用jupyter notebook整理的peter的《机器学习实战》代码，使其更有层次感，更加连贯，也根据自己的代码习惯，加了一些自己的修改，以及注释   这是给自己做的笔记，贴出来，也是希望大家一起学习!   注：原版所有代码点击这里        GitHub整理的资源apachecn/MachineLearning   内容包括：   adaBoost文件夹：AdaBoost元算法提高分类性能   apriori文件夹：Apriori算法进行关联分析   bayes文件夹：bayes算法用于垃圾邮件分类   decisionTree文件夹：使用决策树算法，进行数据分类   fp-growth文件夹：FP-growth算法加速发现频繁项集   kmeans文件夹：kmeans + 二分kmeans算法   k-Nearest Neighbor文件夹：k近邻算法 + 数值归一化   logistic文件夹：batch GD + SGD   pca文件夹：pca降维   pca和svd的比较：关于pca和svd的区别和联系，理论参见博客   regress文件夹：线性回归 + 局部加权线性回归 + 岭回归 + 向前逐步回归    regressionTree文件夹：回归树+模型树   svd文件夹：svd降维 + 协同过滤算法进行物品推荐   svm文件夹：简化版smo实现svm(支持向量机)分类器   完结  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/README.html",
        "teaser":null},{
        "title": "Untitled",
        
        "excerpt":
            "g = [ [0,0,0,0,0,0,0,0,0], [0,0,0,0,0,3,0,8,5], [0,0,1,0,2,0,0,0,0], [0,0,0,5,0,7,0,0,0], [0,0,4,0,0,0,1,0,0], [0,9,0,0,0,0,0,0,0], [5,0,0,0,0,0,0,7,3], [0,0,2,0,1,0,0,0,0], [0,0,0,0,4,0,0,0,9]] from pymprog import model, iprod p = model(\"sudoku\") I = range(1,10) J = range(1,10) K = range(1,10) T = iprod(I,J,K) #create Indice tuples x = p.var('x', T, bool) #x[i,j,k] = 1 means cell [i,j] is assigned number k #assign pre-defined numbers using the \"givens\" [x[i,j,k] == (1 if g[i-1][j-1] == k else 0) for (i,j,k) in T if g[i-1][j-1] &gt; 0 ] #each cell must be assigned exactly one number [sum(x[i,j,k] for k in K)==1 for i in I for j in J] #cells in the same row...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/Untitled.html",
        "teaser":null},{
        "title": "AdaBoost",
        
        "excerpt":
            "algorithms-AdaBoost #adaptive boosting:是一种集成方法，通过组合多个弱分类器的分类结果，进行加权求和的分类结果 import numpy as np import matplotlib.pyplot as plt def loadSimpData(): datMat = np.matrix( [[ 1. , 2.1], [ 2. , 1.1], [ 1.3, 1. ], [ 1. , 1. ], [ 2. , 1. ]]) classLabels = [1.0, 1.0, -1.0, -1.0, 1.0] return datMat,classLabels datMat,classLabels = loadSimpData() #绘制一下 xcord0 = [] ycord0 = [] xcord1 = [] ycord1 = [] markers =[] colors =[] for i in range(len(classLabels)): if classLabels[i]==1.0: xcord1.append(datMat[i,0]), ycord1.append(datMat[i,1]) else: xcord0.append(datMat[i,0]), ycord0.append(datMat[i,1]) fig = plt.figure() ax = fig.add_subplot(111) ax.scatter(xcord0,ycord0, marker='s', s=90) ax.scatter(xcord1,ycord1, marker='o', s=50, c='red') plt.title('decision stump test data') plt.show() #分类函数 #(数据集，特征，阈值，阈值判定方法) def stumpClassify(dataMatrix,dimen,threshVal,threshIneq):#just...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/adaBoost/AdaBoost.html",
        "teaser":null},{
        "title": "AdaBoost元算法提高分类性能",
        
        "excerpt":
            "algorithms-AdaBoost提高分类性能 #adaptive boosting:是一种集成方法，通过组合多个弱分类器的分类结果，进行加权求和的分类结果 import numpy as np import matplotlib.pyplot as plt def loadSimpData(): datMat = np.matrix( [[ 1. , 2.1], [ 2. , 1.1], [ 1.3, 1. ], [ 1. , 1. ], [ 2. , 1. ]]) classLabels = [1.0, 1.0, -1.0, -1.0, 1.0] return datMat,classLabels datMat,classLabels = loadSimpData() #绘制一下 xcord0 = [] ycord0 = [] xcord1 = [] ycord1 = [] markers =[] colors =[] for i in range(len(classLabels)): if classLabels[i]==1.0: xcord1.append(datMat[i,0]), ycord1.append(datMat[i,1]) else: xcord0.append(datMat[i,0]), ycord0.append(datMat[i,1]) fig = plt.figure() ax = fig.add_subplot(111) ax.scatter(xcord0,ycord0, marker='s', s=90) ax.scatter(xcord1,ycord1, marker='o', s=50, c='red') plt.title('decision stump test data') plt.show() #分类函数 #(数据集，特征，阈值，阈值判定方法) def stumpClassify(dataMatrix,dimen,threshVal,threshIneq):#just...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/adaBoost/AdaBoost%E5%85%83%E7%AE%97%E6%B3%95%E6%8F%90%E9%AB%98%E5%88%86%E7%B1%BB%E6%80%A7%E8%83%BD.html",
        "teaser":null},{
        "title": "adaBoost",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/adaBoost/index.html",
        "teaser":null},{
        "title": "apriori",
        
        "excerpt":
            "title: algorithms-apriori author: niult date: 2019-01-16 category: algorithms tags: python,numpy,algorithms Apriori 频繁项集：经常一起出现的物品集合 关联规则：两种物品存在的某种很强的关系 支持度(support) 数据集中包含该项集的记录所占总交易记录的比例 最小支持度(min_support) 最小的比例，表示一种下界 可信度(confidence) 针对某种关联规则{A}——&gt;{B}的可信度=support({A,B}) / support({A}) 最小可信度(min_confidence) 最小的可信度 Apriori作用 减少可能感兴趣的项集，避免物品很多时，需要计算的项集呈指数增长，N种物品，(2**N-1)种的项集组合 Apriori原理 如果某个项集是频繁的，那么他的所有子集也是频繁的 延伸 如果某个项集是非频繁的，那么他的所有超集也是非频繁的 import os data_root = os.path.join( os.path.abspath(os.curdir), 'datas/datas/algorithms/data') print(data_root) /Users/weidian/Documents/MyDiary/datas/datas/algorithms/data 任务1：发现频繁项集 利用Apriori,只保留k个元素的频繁项集 #交易集合 def loadDataSet(): return [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]] #创建C1，单元素集合，[[1],[2],[3],[4],[5]] def createC1(dataSet): C1 = [] for transaction in dataSet: for item in transaction: if not [item] in C1: C1.append([item]) C1.sort() return map(frozenset, C1) #use frozen set so we #can use it as a key in a dict #对C1中的[i]都做frozenset()操作，改成不可变的集合 dataSet = loadDataSet() print dataSet...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/apriori/apriori.html",
        "teaser":null},{
        "title": "apriori",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/apriori/index.html",
        "teaser":null},{
        "title": "bayes",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/bayes/index.html",
        "teaser":null},{
        "title": "naiveBayes",
        
        "excerpt":
            "title: algorithms-naiveBayes author: niult date: 2019-01-16 category: algorithms tags: python,numpy,algorithms # -*- coding: utf-8 -*- #可以使用中文注释 #朴素贝叶斯用于文本分类 #以网站发帖留言，作为文本数据集合 def loadDataSet(): postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']] classVec = [0,1,0,1,0,1] #1 is abusive, 0 not return postingList,classVec #根据所有的留言，构建词汇表（包含所有的单词token） def createVocabularyList(dataSet): vocabularySet = set([]) #create empty set，利用set去重的特点 for document in dataSet: vocabularySet = vocabularySet | set(document) #union of the two sets ,取或操作(并集) return list(vocabularySet) #词序模型，将某个输入留言inputSet转化为关于词汇表的0/1向量...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/bayes/naiveBayes.html",
        "teaser":null},{
        "title": "Boost简介",
        
        "excerpt":
            "title: algorithms-Boost简介 author: niult date: 2019-01-16 category: algorithms tags: python,numpy,algorithms Boost 简介 写在前面 提升（boosting）方法是一类应用广泛且非常有效的统计学习方法。 在2006年，Caruana和Niculescu-Mizil等人完成了一项实验，比较当今世界上现成的分类器（off-the-shelf classifiers）中哪个最好？实现结果表明Boosted Decision Tree（提升决策树）不管是在misclassification error还是produce well-calibrated probabilities方面都是最好的分离器，以ROC曲线作为衡量指标。（效果第二好的方法是随机森林） 参见paper：《An Empirical Comparison of Supervised Learning Algorithms》ICML2006. 下图给出的是Adaboost算法（Decision Stump as Weak Learner）在处理二类分类问题时，随着弱分类器的个数增加，训练误差与测试误差的曲线图。 损失函数示意图 损失函数示意图 从图中可以看出，Adaboost算法随着模型复杂度的增加，测试误差（红色点线）基本保持稳定，并没有出现过拟合的现象。 其实不仅是Adaboost算法有这种表现，Boosting方法的学习思想和模型结构上可以保证其不容易产生过拟合（除非Weak Learner本身出现过拟合）。 下面我们主要是从损失函数的差异，来介绍Boosting的家族成员；然后我们针对每个具体的家族成员，详细介绍其学习过程和核心公式；最后从算法应用场景和工具方法给出简单的介绍。 Boosting介绍 基本思想 Boosting方法基于这样一种思想： 对于一个复杂任务来说，将多个专家的判定进行适当的综合得出的判断，要比其中任何一个专家单独的判断好。很容易理解，就是”三个臭皮匠顶个诸葛亮”的意思…😄😄😄。 历史由来 历史上，Kearns和Valiant首先提出了”强可学习（strongly learnable）”和“弱可学习（weakly learnable）”的概念。他们指出： 在概率近似正确（probably approximately correct，PAC）学习框架中： ①. 一个概念（一个类，label），如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是强可学习的； ②. 一个概念（一个类，label），如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。 Schapire后来证明了: 强可学习和弱可学习是等价的。 也就是说，在PAC学习的框架下，一个概念是强可学习的 充分必要条件 是 这个概念是弱可学习的。 表示如下： 强可学习⇔弱可学习 如此一来，问题便成为：在学习中，如果已经发现了”弱学习算法”，那么能否将它提升为”强学习算法”？ 通常的，发现弱学习算法通常要比发现强学习算法容易得多。那么如何具体实施提升，便成为开发提升方法时所要解决的问题。关于提升方法的研究很多，最具代表性的当数AdaBoost算法（是1995年由Freund和Schapire提出的）。 Boosting学习思路 对于一个学习问题来说（以分类问题为例），给定训练数据集，求一个弱学习算法要比求一个强学习算法要容易的多。Boosting方法就是从弱学习算法出发，反复学习，得到一系列弱分类器，然后组合弱分类器，得到一个强分类器。Boosting方法在学习过程中通过改变训练数据的权值分布，针对不同的数据分布调用弱学习算法得到一系列弱分类器。 这里面有两个问题需要回答： 1.在每一轮学习之前，如何改变训练数据的权值分布？ 2.如何将一组弱分类器组合成一个强分类器？ 具体不同的boosting实现，主要区别在弱学习算法本身和上面两个问题的回答上。 针对第一个问题，Adaboost算法的做法是： 提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。 如此，那些没有得到正确分类的样本，由于其权值加大而受到后一轮的弱分类器的更大关注。 第二个问题，弱分类器的组合，AdaBoost采取加权多数表决的方法。具体地： 加大 分类误差率小 的弱分类器的权值，使其在表决中起较大的作用；减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。 AdaBoost算法的巧妙之处就在于它将这些学习思路自然并且有效地在一个算法里面实现。 前向分步加法模型 英文名称：Forward Stagewise Additive Modeling 加法模型（addtive model） 其中，$b(x;\\gamma_k)$ 为基函数，$\\gamma_k$为基函数的参数，$\\beta_k$为基函数的系数。 前向分步算法 在给定训练数据及损失函数$L(y,f(x))$的条件下，学习加法模型$f(x)$成为经验风险极小化即损失函数极小化的问题： 通常这是一个复杂的优化问题。前向分布算法（forward stagwise algorithm）求解这一优化问题的思路是：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式，那么就可以简化优化的复杂度。具体地，每步只需优化如下损失函数： 给定训练数据集$D={(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),⋯,(x^{(M)},y^{(M)})},x^{(i)}\\in R^n,y^{(i)}\\in [−1,+1]$。损失函数$L(y,f(x))$和基函数的集合${b(x;\\gamma)}$，学习加法模型$f(x)$的前向分步算法如下： { 输入：训练数据集$D={(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)})}$; $\\qquad$ 损失函数$L(y,f(x))$; $\\qquad$...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/boost/Boost%E7%AE%80%E4%BB%8B.html",
        "teaser":null},{
        "title": "boost",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/boost/index.html",
        "teaser":null},{
        "title": "fp-growth",
        
        "excerpt":
            "title: algorithms-FP-growth author: niult date: 2019-01-16 category: algorithms tags: python,numpy,algorithms #FP树:Frequent Pattern Tree #树节点 class treeNode: def __init__(self, nameValue, numOccur, parentNode): self.name = nameValue self.count = numOccur self.nodeLink = None self.parent = parentNode #needs to be updated self.children = {} #存放子节点的字典 def inc(self, numOccur): self.count += numOccur def disp(self, ind=1): print ' '*ind, self.name, ' ', self.count for child in self.children.values(): child.disp(ind+1) #测试一下 rootNode = treeNode('pyramid',9,None) rootNode.children['eye'] = treeNode('eye',13,None) rootNode.disp() print '*******************' rootNode.children['phoenix'] = treeNode('phoenix',3,None) rootNode.disp() pyramid 9 eye 13 ******************* pyramid 9 eye 13 phoenix 3 #创建FP树 #dataset为字典{frozenset([事务i]):频数} ,minSup:最小支持度（用于过滤非频繁集） def createTree(dataSet, minSup=1): #create FP-tree from dataset but don't mine...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/fp-growth/fp-growth.html",
        "teaser":null},{
        "title": "fp-growth",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/fp-growth/index.html",
        "teaser":null},{
        "title": "14常见算法",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/index.html",
        "teaser":null},{
        "title": "k-Nearest Neighbor",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/k-Nearest%20Neighbor/index.html",
        "teaser":null},{
        "title": "knn",
        
        "excerpt":
            "title: algorithms-KNN2 author: niult date: 2019-01-16 category: algorithms tags: python,numpy,algorithms import numpy as np #load data from file导入txt数据 def load_data(filename): dataset = [] label = [] file = open(filename) for line in file.readlines(): lineArr = line.strip().split('\\t') dataset.append(lineArr[0:3]) label.append(lineArr[-1]) return np.array(dataset,dtype=np.float64),\\ np.array(label,dtype=np.int) import matplotlib.pyplot as plt #导入数据并且可视化一下 data,label = load_data(\"k-Nearest Neighbor/datingTestSet2.txt\") print data.shape,label.shape print data[0],label[0:10] def plot(x,y): label1 = np.where(y.ravel() == 1) plt.scatter(x[label1,0],x[label1,1],marker='x',color = 'r',label = 'didnt like=1') label2 = np.where(y.ravel() == 2) plt.scatter(x[label2,0],x[label2,1],marker='o',color = 'b',label = 'smallDoses=2') label3 = np.where(y.ravel() == 3) plt.scatter(x[label3,0],x[label3,1],marker='.',color = 'y',label = 'largeDoses=3') plt.xlabel('pilot distance') plt.ylabel('game time') plt.legend(loc = 'upper left') plt.show() plot(data,label) (1000L, 3L) (1000L,)...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/k-Nearest%20Neighbor/knn.html",
        "teaser":null},{
        "title": "knn_原版",
        
        "excerpt":
            "title: algorithms-KNN1 author: niult date: 2019-01-16 category: algorithms tags: python,numpy,algorithms #knn算法，机器学习实战源代码 import numpy as np import operator #创建简单的数据用例x，y def createDataSet(): group = np.array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]]) labels = ['A','A','B','B'] return group, labels #knn算法实现 def classify0(inX, dataSet, labels, k): dataSetSize = dataSet.shape[0] diffMat = np.tile(inX, (dataSetSize,1)) - dataSet #tile()用于扩展矩阵 sqDiffMat = diffMat**2 sqDistances = sqDiffMat.sum(axis=1) distances = sqDistances**0.5 #求得距离 sortedDistIndicies = distances.argsort() #返回排序的下标，从小到大 classCount={} for i in range(k): voteIlabel = labels[sortedDistIndicies[i]] classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1 #用字典计数 sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True) #对字典排序，基于值，从大到小(逆序，reverse=True) return sortedClassCount[0][0] #测试一下 group,labels=createDataSet() pred=classify0([0,0],group,labels,3) print pred B #导入数据集 def file2matrix(filename): fr = open(filename) numberOfLines = len(fr.readlines()) #get the number of...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/k-Nearest%20Neighbor/knn_%E5%8E%9F%E7%89%88.html",
        "teaser":null},{
        "title": "kmeans",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/kmeans/index.html",
        "teaser":null},{
        "title": "kmeans",
        
        "excerpt":
            "title: algorithms-Kmeans author: niult date: 2019-01-16 category: algorithms tags: python,numpy,algorithms import numpy as np #load data from file导入txt数据 def load_data(filename): dataset = [] file = open(filename) for line in file.readlines(): lineArr = line.strip().split('\\t') m = len(lineArr) dataset.append(lineArr[0:m]) return np.array(dataset,dtype=np.float64) import matplotlib.pyplot as plt x = load_data(\"kmeans/testSet.txt\") print x.shape #可视化一下 plt.scatter(x[:,0],x[:,1],marker='x',color = 'r') plt.xlabel('x0') plt.ylabel('x1') plt.show() (80L, 2L) #计算距离 def computeDistance(A,B): return np.sqrt(np.sum(np.square(A-B))) #我们一开始应该随机初始化centroids ,随机选择k个样本点作为质心,而不是书上的某个随机值 def randCentroids(x,k=3): m,n = x.shape centroids = np.zeros((k,n)) randIndex = np.random.choice(m,k) centroids = x[randIndex] return centroids #计算x的均值，改变centroids def change_centroids(x,idx,K): m,n = x.shape centroids = np.zeros((K,n)) for i in range(K): index = np.where(idx[:,0].ravel() == i) centroids[i] =...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/kmeans/kmeans.html",
        "teaser":null},{
        "title": "LR",
        
        "excerpt":
            "title: algorithms-LR author: niult date: 2019-01-16 category: algorithms tags: python,numpy,algorithms 例1 import urllib import pandas as pd from __future__ import print_function, division import tensorflow as tf import numpy as np import matplotlib.pyplot as plt import seaborn from sklearn.cross_validation import train_test_split import random url = \"https://raw.githubusercontent.com/LuisM78/Occupancy-detection-data/master/datatraining.txt\" data = urllib.urlopen(url).read() data = data.decode('UTF-8') f = open('test.txt', 'w') # 若是'wb'就表示写二进制文件 f.write(data) f.close() df1= pd.read_csv('test.txt') # 读取数据 data = df1 # 拆分数据 X_train, X_test, y_train, y_test = train_test_split(data[[\"Temperature\", \"Humidity\", \"Light\", \"CO2\", \"HumidityRatio\"]].values, data[\"Occupancy\"].values.reshape(-1, 1), random_state=42) # one-hot 编码 y_train y_train = tf.concat(1, [1 - y_train, y_train]) y_test = tf.concat(1, [1 - y_test, y_test]) #...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/line-regression/LR.html",
        "teaser":null},{
        "title": "line-regression",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/line-regression/index.html",
        "teaser":null},{
        "title": "line_regression",
        
        "excerpt":
            "title: algorithms-LR3 author: niult date: 2019-01-16 category: algorithms tags: python,numpy,algorithms import numpy as np #load data from file导入txt数据 def load_data(filename): dataset = [] label = [] file = open(filename) for line in file.readlines(): lineArr = line.strip().split('\\t') dataset.append(lineArr[0:2]) label.append(lineArr[-1]) return np.array(dataset,dtype=np.float64),\\ np.array(label,dtype=np.float64).reshape(-1,1) import matplotlib.pyplot as plt #导入数据并且可视化一下 x,y = load_data(\"regress/ex0.txt\") print x.shape,y.shape print x[0],y[0] plt.scatter(x[:,1],y[:,0],marker='x',color = 'r') plt.xlabel('x') plt.ylabel('y') plt.show() (200L, 2L) (200L, 1L) [ 1. 0.067732] [ 3.176513] #简单的线性回归，使用求导公式就可以求得w的最优值 def normalEquation(X_train,y_train): w = np.zeros((X_train.shape[0],1)) #这里用的伪逆,所以不用判断矩阵的逆存不存在 w = ((np.linalg.pinv(X_train.T.dot(X_train))).dot(X_train.T)).dot(y_train) return w w = normalEquation(x,y) print w [[ 3.00774324] [ 1.69532264]] #现在可视化一下求得w的效果 plt.scatter(x[:,1],y[:,0],marker='.',color = 'r') plt.plot(x[:,1],x.dot(w),color = \"blue\",linestyle = \"-\") plt.xlabel('x') plt.ylabel('y')...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/line-regression/line_regression.html",
        "teaser":null},{
        "title": "logistic",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/logistic/index.html",
        "teaser":null},{
        "title": "logistic-code",
        
        "excerpt":
            "title: algorithms-logistic-code author: niult date: 2019-01-16 category: algorithms tags: python,numpy,algorithms import numpy as np #load data from file导入txt数据 def load_data(filename): dataset = [] label = [] file = open(filename) for line in file.readlines(): lineArr = line.strip().split('\\t') dataset.append(lineArr[0:2]) label.append(lineArr[-1]) return np.array(dataset,dtype=np.float64),\\ np.array(label,dtype=np.int).reshape(-1,1) import matplotlib.pyplot as plt #导入数据并且可视化一下 x,y = load_data(\"logistic/testSet.txt\") print x.shape,y.shape print x[0],y[0] label1 = np.where(y.ravel() == 0) plt.scatter(x[label1,0],x[label1,1],marker='x',color = 'r',label = '0') label2 = np.where(y.ravel() == 1) plt.scatter(x[label2,0],x[label2,1],marker='o',color = 'b',label = '1') plt.xlabel('x1') plt.ylabel('x2') plt.legend(loc = 'upper left') plt.show() (100L, 2L) (100L, 1L) [ -0.017612 14.053064] [0] #使用batch gradient descent求得最佳参数 def sigmoid(x): return 1.0 / (1+np.exp(-x)) #每次使用全部的数据集去更新一次参数 def bgd(x,y,learn_rate...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/logistic/logistic-code.html",
        "teaser":null},{
        "title": "logistic-理论",
        
        "excerpt":
            "title: algorithms-logistic-理论 author: niult date: 2019-01-16 category: algorithms tags: python,numpy,algorithms 逻辑回归 该算法尝试为给定一个输入特征的线性组合的一个二元变量的结果建模。 举个例子，可以根据候选人为竞选花费的金钱、时间等信息预测选举结果。 逻辑回归工作流程—— 给定： 数据集合 ${(x^{(1)},y^{(1)},\\ldots,(x^{(m)},y^{(m)})}$ 每个 $x^{(i)}$ 都是 $d$ 维向量 $x^{(i)}=(x_1^{(i)},\\ldots,x_d^{(i)})$ 每个 $y^{(i)}$ 都是一个二元目标变量 $y^{(i)} \\in {0,1}$ 逻辑回归模型可以用非常简单的神经网络表达： 拥有一个实值权重向量 $w=(w^{(1)},\\ldots,w^{(d)})$ 拥有一个实值偏置 $b$ 采用sigmoid函数作为激活函数 与线性回归不同，逻辑回归没有封闭解。 但是成本函数是凸的（convex），因此可以采用梯度下降方法来训练该模型。 事实上，梯度下降（或其余任何最优化算法）保证找到全局最小（如果学习率足够小且训练迭代次数足够多）。 训练步骤： 一开始模型参数是初始化了的，接下来重复指定的训练迭代次数或直到参数收敛。 第一步 把权重向量和偏置初始化为零（或小的随机数） 第二步 计算一个输入特性和权重的线性组合。 采用向量化和广播所有训练样本可一次性搞定： $a=X \\cdot w + b$ 这里 $X$ 是矩阵 $(n_{samples},n_{features})$ 保佑全部训练样本，而 $\\cdot$ 表示点积。 第三步 应用sigmoid激活函数，返回值在0和1之间： 第四步 计算整个训练集合的成本 就是对介于0和1之间的目标值的概率建模 因此在训练过程中，要修改（adapt）参数以致把模型输出高值的样本标记为正（1）、把模型输出低值的样本标记为负（0） 在成本函数中体现： 第五步 计算关于权重向量和偏置的成本函数梯度，证明过程 公式： $\\frac{\\partial J}{\\partial w_j}=\\frac{1}{m} \\sum_{i=1}^m \\Big[\\hat y^{(i)} - y^{(i)} \\Big]x_j^{(i)}$ 对于偏置，输入 $x_j^{(i)}$ 则按 $1$ 给予 第六步 更新权重和偏置： 其中 $\\nabla$ 即学习率 正样本的概率： 负样本的概率： 将上面两个公式合成一个，如下： 整个样本集生成的概率，即为所有样本生成概率的乘积： 上式取对数 \\begin{align} \\frac{\\partial}{\\partial \\theta} h_\\theta(x) &amp;= \\frac{\\partial}{\\partial \\theta}g(\\theta^T x) &amp;...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/logistic/logistic-%E7%90%86%E8%AE%BA.html",
        "teaser":null},{
        "title": "temo",
        
        "excerpt":
            " ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/logistic/temo.html",
        "teaser":null},{
        "title": "pca",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/pca/index.html",
        "teaser":null},{
        "title": "pca",
        
        "excerpt":
            "title: algorithms-PCA author: niult date: 2019-01-16 category: algorithms tags: python,numpy,algorithms #导入数据集合 import numpy as np def load_data(filename,splitstyle = \"\\t\"): dataset = [] file = open(filename) for line in file.readlines(): lineArr = line.strip().split(splitstyle) m = len(lineArr) dataset.append(lineArr[0:m]) return np.array(dataset,dtype=np.float64) x = load_data(\"pca/testSet.txt\") print x.shape import matplotlib.pyplot as plt #可视化一下数据集合 plt.scatter(x[:,0],x[:,1],marker='x',color = 'r') plt.xlabel('x0') plt.ylabel('x1') plt.show() (1000L, 2L) def pca(x,k): x_mean = np.mean(x,axis=0) x_nor = x - x_mean x_cov = np.cov(x_nor,rowvar=0) #协方差矩阵 [n,n] eigvals,eigVecs = np.linalg.eig(x_cov) #计算协方差矩阵的特征值和特征向量 eigvals_sortindex = np.argsort(eigvals) #对特征值进行排序 k_index = eigvals_sortindex[: -(k+1):-1] #取出 最后的k个（因为是从小到大排序的）所以倒过来数 k_eigVecs = eigVecs[:,k_index] #只要最大的k个特征向量 [n,k] lowData = x_nor.dot(k_eigVecs) #降维的数据(m.k) =[m,n]*[n,k] recData = lowData.dot(k_eigVecs.T) + x_mean...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/pca/pca.html",
        "teaser":null},{
        "title": "pca与svd的比较",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/pca%E4%B8%8Esvd%E7%9A%84%E6%AF%94%E8%BE%83/index.html",
        "teaser":null},{
        "title": "pca",
        
        "excerpt":
            "title: algorithms-PCA2 author: niult date: 2019-01-16 category: algorithms tags: python,numpy,algorithms #导入数据集合 import numpy as np def load_data(filename,splitstyle = \"\\t\"): dataset = [] file = open(filename) for line in file.readlines(): lineArr = line.strip().split(splitstyle) m = len(lineArr) dataset.append(lineArr[0:m]) return np.array(dataset,dtype=np.float64) x = load_data(\"pca/testSet.txt\") print x.shape import matplotlib.pyplot as plt #可视化一下数据集合 plt.scatter(x[:,0],x[:,1],marker='x',color = 'r') plt.xlabel('x0') plt.ylabel('x1') plt.show() (1000L, 2L) def pca(x,k): x_mean = np.mean(x,axis=0) x_nor = x - x_mean x_cov = np.cov(x_nor,rowvar=0) #协方差矩阵 [n,n] eigvals,eigVecs = np.linalg.eig(x_cov) #计算协方差矩阵的特征值和特征向量 eigvals_sortindex = np.argsort(eigvals) #对特征值进行排序 k_index = eigvals_sortindex[: -(k+1):-1] #取出 最后的k个（因为是从小到大排序的）所以倒过来数 k_eigVecs = eigVecs[:,k_index] #只要最大的k个特征向量 [n,k] lowData = x_nor.dot(k_eigVecs) #降维的数据(m.k) =[m,n]*[n,k] recData = lowData.dot(k_eigVecs.T) + x_mean...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/pca%E4%B8%8Esvd%E7%9A%84%E6%AF%94%E8%BE%83/pca.html",
        "teaser":null},{
        "title": "CART",
        
        "excerpt":
            "#介绍CART(classification and regression tree)分类回归树 #先介绍，决策树对于连续性数值的回归问题————回归树 #1，切分数据：二元切分法，x&gt;a和x&lt;=a,其中的a属于某个特征xi的所有取值集合 #2,选择特征：选择使得，切分数据的总均方差最小 #3,预测：使用切分数据集的平均值，代表预测值 #先导入数据 import numpy as np def loadDataSet(fileName): dataMat = [] #assume last column is target value fr = open(fileName) for line in fr.readlines(): curLine = line.strip().split('\\t') floatLine = map(float,curLine) #map all elements to float() dataMat.append(floatLine) #这里把最后一列作为目标变量，y return np.mat(dataMat) myData= loadDataSet('regressionTree/ex00.txt') print myData.shape (200L, 2L) #可视化一下ex00.txt的数据 import matplotlib.pyplot as plt plt.scatter(myData[:,0].A,myData[:,1].A,color=\"blue\") #np.matrix.A---&gt;matrix转化为array plt.xlabel(\"x\") plt.ylabel(\"y\") plt.show() #二元切分法,x&gt;a和x&lt;=a,其中的a属于某个特征xi的所有取值集合 #notice:切分的特征并没有删除，这与前面ID3算法的决策树分类中，切分数据不同!!! def binSplitDataSet(dataSet, feature, value): mat0 = dataSet[np.nonzero(dataSet[:,feature] &gt; value)[0],:] mat1 = dataSet[np.nonzero(dataSet[:,feature] &lt;= value)[0],:] return mat0,mat1 #测试一下 mat0,mat1 = binSplitDataSet(myData,0,0.5) print mat0.shape,mat1.shape (116L, 2L) (84L, 2L) #树回归的叶子节点————数据集合中y的均值 def regLeaf(dataSet): return np.mean(dataSet[:,-1]) #用y的总均方差代表连续型数据的混乱度 def regErr(dataSet): return np.var(dataSet[:,-1])...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/regressionTree/CART.html",
        "teaser":null},{
        "title": "regressionTree",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/regressionTree/index.html",
        "teaser":null},{
        "title": "collaborative_filter",
        
        "excerpt":
            "title: algorithms-协同过滤SVD author: niult date: 2019-01-16 category: algorithms tags: python,numpy,algorithms #协同过滤的推荐引擎 #比较用户或者物品之间的相似度，进行推荐 #note:用户的数量很多时，我们倾向于，“基于物品相似度的计算方法” #物品之间相似度的计算，不是简单的，利用物品的属性进行距离计算，而是 #利用用户对他们的意见进行相似度计算————根据“评价矩阵”（用户 i 对物品 j 的评分矩阵） #首先定义几个相似度计算函数 import numpy as np #欧式距离计算相似度 #这里，我们希望，相似度在0-1之间。所以用公式：相似度 = 1/(1+相似度) def euclidSimilarity(A,B): #A,B都是列向量,表明是基于物品的相似度 dis = np.linalg.norm(A-B) return 1.0 / (1.0 + dis) #一定要用1.0 #皮尔逊相似系数 def pearsonSimilarity(A,B): if len(A) &lt; 3: return 1.0 #检查是否含有三个及以上的点，否则两个向量完全相关 return 0.5 + 0.5 * np.corrcoef(A,B,rowvar=0)[0,1] #规范到0-1 #余弦相似度 def cosSimilarity(A,B): num = float(A.T.dot(B)) denom = np.linalg.norm(A) * np.linalg.norm(B) return 0.5 + 0.5 * (num / denom) #规范到0-1 #构造一个矩阵测试一下 #行表示人，列表示物品:所以矩阵的列表示：基于物品的相似度计算 a = np.array([[1,1,1,0,0], [2,2,2,0,0], [1,1,1,0,0], [5,5,5,0,0], [1,1,0,2,2], [0,0,0,3,3], [0,0,0,1,1]]) S1 = euclidSimilarity(a[:,0],a[:,4]) print S1 S2 = pearsonSimilarity(a[:,0],a[:,4]) print S2 S3...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/svd/collaborative_filter.html",
        "teaser":null},{
        "title": "svd",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/svd/index.html",
        "teaser":null},{
        "title": "svd",
        
        "excerpt":
            "title: algorithms-SVD author: niult date: 2019-01-16 category: algorithms tags: python,numpy,algorithms #本文使用svd（）函数实现数据降维 #导入数据集合 import numpy as np def load_data(filename,splitstyle = \"\\t\"): dataset = [] file = open(filename) for line in file.readlines(): lineArr = line.strip().split(splitstyle) m = len(lineArr) dataset.append(lineArr[0:m]) return np.array(dataset,dtype=np.float64) x = load_data(\"svd/testSet.txt\") print x.shape import matplotlib.pyplot as plt #可视化一下数据集合 plt.scatter(x[:,0],x[:,1],marker='x',color = 'r') plt.xlabel('x0') plt.ylabel('x1') plt.show() (1000L, 2L) #feature normalize 特征归一化 def featureNormalize(X): X_norm = X mu = np.zeros((1,X.shape[1])) sigma = np.zeros((1,X.shape[1])) mu = np.mean(X,axis=0) #mean value of every feature sigma = np.std(X,axis=0)#std of every feature X_norm = (X - mu) / sigma return X_norm,mu,sigma def SVD(x_norm,k): m = x_norm.shape[0] x_cov...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/svd/svd.html",
        "teaser":null},{
        "title": "svm",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/svm/index.html",
        "teaser":null},{
        "title": "smo",
        
        "excerpt":
            "#smo算法，里面的公式很复杂，这里只是简化版本，详细版本请参见原版代码 #SMO基本思路：1.选取一对需要更新的alpha(i)和alpha(j) 2.固定其他的alpha,更新这两个值直到收敛 #关键：怎么选取两个alpha值？ + 怎么更新？ import numpy as np #load data from file导入txt数据 def load_data(filename): dataset = [] label = [] file = open(filename) for line in file.readlines(): lineArr = line.strip().split('\\t') m = len(lineArr) dataset.append(lineArr[0:m-1]) label.append(lineArr[-1]) return np.array(dataset,dtype=np.float64),\\ np.array(label,dtype=np.int).reshape(-1,1) import matplotlib.pyplot as plt #导入数据并且可视化一下 x,y = load_data(\"svm/testSet.txt\") print x.shape,y.shape print x[0],y[0] label1 = np.where(y.ravel() == -1) plt.scatter(x[label1,0],x[label1,1],marker='x',color = 'r',label = '-1') label2 = np.where(y.ravel() == 1) plt.scatter(x[label2,0],x[label2,1],marker='o',color = 'b',label = '1') plt.xlabel('x1') plt.ylabel('x2') plt.legend(loc = 'upper left') plt.show() (100L, 2L) (100L, 1L) [ 3.542485 1.977398] [-1] def selectJrand(i,m): j=i #we want to select any J not equal to i...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/svm/smo.html",
        "teaser":null},{
        "title": "xgboost",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/xgboost/index.html",
        "teaser":null},{
        "title": "xgboost",
        
        "excerpt":
            "   title: algorithms-xgboost1   author: niult   date: 2019-01-16   category: algorithms   tags: python,numpy,algorithms   xgBoost   https://www.cnblogs.com/mfryf/p/6238185.html   https://www.zhihu.com/question/41354392   https://blog.csdn.net/github_38414650/article/details/76061893   https://www.cnblogs.com/csyuan/p/6537255.html   https://www.cnblogs.com/infaraway/p/7890558.html   Gradient Boosting Tree 算法原理   Friedman于论文”GreedyFunctionApproximation…”中最早提出GBDT      其模型F定义为加法模型:     其中，$x$为输入样本，$h$为分类回归树，$w$是分类回归树的参数， $\\alpha$是每棵树的权重。      通过最小化损失函数求解最优模型:     NP难问题 -&gt; 通过贪心法，迭代求局部最优解   输入：$\\big(x_i,y_i\\big),T,L$   1 初始化$f_0$   2 for t = 1 to T do   2.1 计算响应：$\\tilde {y_i}= -{\\Bigg[\\frac{\\partial L\\big(y_i,F(x_i) \\big)}{\\partial F(x_i)} \\Bigg]}{F(x)=F{t-1}(x)},i=1,2,…,N$    2.2 学习第$t$棵树：$w^=\\arg_w{\\min\\sum\\limits^N_{i=1}\\Big(\\tilde{y_i}-h_i\\big(x_i;w \\big) \\Big)}$    2.3 line search找步长：   $\\rho^=\\arg_{\\rho}\\min\\sum\\limits^N_{i=1}L\\Big(y_i,F_{t-1}(x_i)+\\rho h_i\\big(x_i;w^* \\big) \\Big)$      2.4 令$f_t=\\rho^h_i(x;w^)$，更新模型  $F_t=F_{t-1}+f_t$   3 输出$F_T$   目标函数  参数空间中的目标函数:     其中$L(\\Theta)$是误差函数，有许多拟合函数，$\\omega(\\Theta)$是正则化项，为了惩罚复杂模型   误差函数可以是square loss，logloss等，正则项可以是L1正则， L2正则等。   Ridge Regression(岭回归):$\\sum^n_{i=1}\\big(y_i-\\theta^Tx_i \\big)^2+\\lambda|\\theta|^2$   LASSO:    $\\sum^n_{i=1}\\big(y_i-\\theta^Tx_i \\big)^2+\\lambda|\\theta|$   误差函数   正则项  正则项的作用，可以从几个角度去解释：     通过偏差方差分解去解释  PAC-learning泛化界解释  Bayes先验解释，把正则当先验  从Bayes角度来看，正则相当于对模型参数引入先验分布:    L1正则，模型参数服从拉普拉斯分布，对参数加了分布约束，大部分取值为0  L2正则，模型参数服从高斯分布 $\\theta~N(0,\\sigma^2)$ 对参数加了分布约束，大部分绝对值很小   XGBoost的目标函数(函数空间)  正则项对每棵回归树的复杂度进行了惩罚  相比原始的GBDT，XGBoost的目标函数多了正则项，使得学习出来的 模型更加不容易过拟合。  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/xgboost/xgboost.html",
        "teaser":null},{
        "title": "十分钟上手xgboost",
        
        "excerpt":
            "title: algorithms-十分钟上手xgboost author: niult date: 2019-01-16 category: algorithms tags: python,numpy,algorithms xgboost 参考网址 1 XGBoost参数调优完全指南（附Python代码） https://www.cnblogs.com/mfryf/p/6293814.html XGBoost的参数 XGBoost的作者把所有的参数分成了三类： 1、通用参数：宏观函数控制。 2、Booster参数：控制每一步的booster(tree/regression)。 3、学习目标参数：控制训练目标的表现。 在这里我会类比GBM来讲解，所以作为一种基础知识。 通用参数 这些参数用来控制XGBoost的宏观功能。 booster[默认gbtree] 选择每次迭代的模型，有两种选择： gbtree：基于树的模型 gbliner：线性模型 silent[默认0] 当这个参数值为1时，静默模式开启，不会输出任何信息。 一般这个参数就保持默认的0，因为这样能帮我们更好地理解模型。 nthread[默认值为最大可能的线程数] 这个参数用来进行多线程控制，应当输入系统的核数。 如果你希望使用CPU全部的核，那就不要输入这个参数，算法会自动检测它。 还有两个参数，XGBoost会自动设置，目前你不用管它。接下来咱们一起看booster参数。 booster参数 尽管有两种booster可供选择，我这里只介绍tree booster，因为它的表现远远胜过linear booster，所以linear booster很少用到。 eta[默认0.3] 和GBM中的 learning rate 参数类似。 通过减少每一步的权重，可以提高模型的鲁棒性。 典型值为0.01-0.2。 min_child_weight[默认1] 决定最小叶子节点样本权重和。 和GBM的 min_child_leaf 参数类似，但不完全一样。XGBoost的这个参数是最小样本权重的和，而GBM参数是最小样本总数。 这个参数用于避免过拟合。当它的值较大时，可以避免模型学习到局部的特殊样本。 但是如果这个值过高，会导致欠拟合。这个参数需要使用CV来调整。 max_depth[默认6] 和GBM中的参数相同，这个值为树的最大深度。 这个值也是用来避免过拟合的。max_depth越大，模型会学到更具体更局部的样本。 需要使用CV函数来进行调优。 典型值：3-10 max_leaf_nodes 树上最大的节点或叶子的数量。 可以替代max_depth的作用。因为如果生成的是二叉树，一个深度为n的树最多生成n2个叶子。 如果定义了这个参数，GBM会忽略max_depth参数。 gamma[默认0] 在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值。 这个参数的值越大，算法越保守。这个参数的值和损失函数息息相关，所以是需要调整的。 max_delta_step[默认0] 这参数限制每棵树权重改变的最大步长。如果这个参数的值为0，那就意味着没有约束。如果它被赋予了某个正值，那么它会让这个算法更加保守。 通常，这个参数不需要设置。但是当各类别的样本十分不平衡时，它对逻辑回归是很有帮助的。 这个参数一般用不到，但是你可以挖掘出来它更多的用处。 subsample[默认1] 和GBM中的subsample参数一模一样。这个参数控制对于每棵树，随机采样的比例。 减小这个参数的值，算法会更加保守，避免过拟合。但是，如果这个值设置得过小，它可能会导致欠拟合。 典型值：0.5-1 colsample_bytree[默认1] 和GBM里面的max_features参数类似。用来控制每棵随机采样的列数的占比(每一列是一个特征)。 典型值：0.5-1 colsample_bylevel[默认1] 用来控制树的每一级的每一次分裂，对列数的采样的占比。 我个人一般不太用这个参数，因为subsample参数和colsample_bytree参数可以起到相同的作用。但是如果感兴趣，可以挖掘这个参数更多的用处。 lambda[默认1] 权重的L2正则化项。(和Ridge regression类似)。 这个参数是用来控制XGBoost的正则化部分的。虽然大部分数据科学家很少用到这个参数，但是这个参数在减少过拟合上还是可以挖掘出更多用处的。 alpha[默认1] 权重的L1正则化项。(和Lasso regression类似)。 可以应用在很高维度的情况下，使得算法的速度更快。 scale_pos_weight[默认1] 在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。 学习目标参数 这个参数用来控制理想的优化目标和每一步结果的度量方法。 objective[默认reg:linear] 这个参数定义需要被最小化的损失函数。最常用的值有： binary:logistic 二分类的逻辑回归，返回预测的概率(不是类别)。 multi:softmax 使用softmax的多分类器，返回预测的类别(不是概率)。 在这种情况下，你还需要多设一个参数：num_class(类别数目)。 multi:softprob 和multi:softmax参数一样，但是返回的是每个数据属于各个类别的概率。...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/xgboost/%E5%8D%81%E5%88%86%E9%92%9F%E4%B8%8A%E6%89%8Bxgboost.html",
        "teaser":null},{
        "title": "decisionTree",
        
        "excerpt":
            "title: algorithms-决策树实例 author: niult date: 2019-01-16 category: algorithms tags: python,numpy,algorithms #决策树算法 import math #小的数据集 def createDataSet(): dataSet = [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']] featureNames = ['no surfacing','flippers'] #不浮出水面是否存活 ，有无脚蹼 #change to discrete values return dataSet, featureNames #计算信息熵,因为我们会利用最大信息增益的方法划分数据集-----看哪个特征划分使得，信息熵(数据无序度)减小的最多 def Entropy(dataSet): num = len(dataSet) labelCounts = {} for featVec in dataSet: # 统计每个类别的数量 currentLabel = featVec[-1] #最后1列为键 if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0 #初始值=0 labelCounts[currentLabel] += 1 #统计+1 entropy = 0.0 for key in labelCounts: # prob = float(labelCounts[key])/num entropy -= prob * math.log(prob,2) #log base 2 return entropy #测试一下...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E5%86%B3%E7%AD%96%E6%A0%91(DecisionTree)/decisionTree.html",
        "teaser":null},{
        "title": "决策树(DecisionTree)",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E5%86%B3%E7%AD%96%E6%A0%91(DecisionTree)/index.html",
        "teaser":null},{
        "title": "决策树简介",
        
        "excerpt":
            "algorithms-决策树 决策树 决策树 概述 决策树（Decision Tree）算法是一种基本的分类与回归方法，是最经常使用的数据挖掘算法之一。我们这章节只讨论用于分类的决策树。 决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是 if-then 规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。 决策树学习通常包括 3 个步骤：特征选择、决策树的生成和决策树的修剪。 决策树 场景 一个叫做 “二十个问题” 的游戏，游戏的规则很简单：参与游戏的一方在脑海中想某个事物，其他参与者向他提问，只允许提 20 个问题，问题的答案也只能用对或错回答。问问题的人通过推断分解，逐步缩小待猜测事物的范围，最后得到游戏的答案。 一个邮件分类系统，大致工作流程如下： 首先检测发送邮件域名地址。如果地址为 myEmployer.com, 则将其放在分类 \"无聊时需要阅读的邮件\"中。 如果邮件不是来自这个域名，则检测邮件内容里是否包含单词 \"曲棍球\" , 如果包含则将邮件归类到 \"需要及时处理的朋友邮件\", 如果不包含则将邮件归类到 \"无需阅读的垃圾邮件\" 。 决策树的定义： 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点（node）和有向边（directed edge）组成。结点有两种类型：内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或属性(features)，叶结点表示一个类(labels)。 用决策树对需要测试的实例进行分类：从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至达到叶结点。最后将实例分配到叶结点的类中。 决策树 原理 决策树 须知概念 信息熵 &amp; 信息增益 熵（entropy）： 熵指的是体系的混乱的程度，在不同的学科中也有引申出的更为具体的定义，是各领域十分重要的参量。 信息论（information theory）中的熵（香农熵）： 是一种信息的度量方式，表示信息的混乱程度，也就是说：信息越有序，信息熵越低。例如：火柴有序放在火柴盒里，熵值很低，相反，熵值很高。 信息增益（information gain）： 在划分数据集前后信息发生的变化称为信息增益。 决策树 工作原理 如何构造一个决策树? 我们使用 createBranch() 方法，如下所示： def createBranch(): ''' 此处运用了迭代的思想。 感兴趣可以搜索 迭代 recursion， 甚至是 dynamic programing。 ''' 检测数据集中的所有数据的分类标签是否相同: If so return 类标签 Else: 寻找划分数据集的最好特征（划分之后信息熵最小，也就是信息增益最大的特征） 划分数据集 创建分支节点 for 每个划分的子集 调用函数 createBranch （创建分支的函数）并增加返回结果到分支节点中 return 分支节点 决策树 开发流程 收集数据：可以使用任何方法。 准备数据：树构造算法 (这里使用的是ID3算法，只适用于标称型数据，这就是为什么数值型数据必须离散化。 还有其他的树构造算法，比如CART) 分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。 训练算法：构造树的数据结构。 测试算法：使用训练好的树计算错误率。 使用算法：此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义。 决策树 算法特点 优点：计算复杂度不高，输出结果易于理解，数据有缺失也能跑，可以处理不相关特征。 缺点：容易过拟合。...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E5%86%B3%E7%AD%96%E6%A0%91(DecisionTree)/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%80%E4%BB%8B.html",
        "teaser":null},{
        "title": "建模过程",
        
        "excerpt":
            "import pandas as pd 数据收集 主要收集和业务相关的数据，通常会有专门的同事在app位置进行埋点，拿到业务数据 数据清洗 对埋点拿到的业务数据进行去脏去重 特征工程 对原始数据进行基本的特征处理，包括去除相关性大的特征，离散变量one-hot，连续特征离散化等等; 构造数据集 经过预处理的业务数据，构造数据集，在切分训练、测试、验证集时应该合理根据业务逻辑来进行切分； 模型选择 选择合理的机器学习模型来完成相应工作，原则是先从简入深，先找到baseline，然后逐步优化； 超参选择 利用gridsearch、randomsearch或者hyperopt来进行超参选择，选择在离线数据集中性能最好的超参组合； 在线A/B Test 选择优化过后的模型和原先模型（如baseline）进行A/B Test，若性能有提升则替换原先模型 特征选择 可用的特征 获取难度 准确度 覆盖度 特征清洗 异常样本 正负样本不均衡 特征选择 不同的特征对模型的影响程度不同，我们要自动地选择出对问题重要的一些特征，移除与问题相关性不是很大的特征，这个过程就叫做特征选择。特征的选择在特征工程中十分重要，往往可以直接决定最后模型训练效果的好坏。常用的特征选择方法有：过滤式（filter）、包裹式（wrapper）、嵌入式（embedding)。 过滤式 过滤式特征选择是通过评估每个特征和结果的相关性，来对特征进行筛选，留下相关性最强的几个特征。核心思想是：先对数据集进行特征选择，然后再进行模型的训练。过滤式特征选择的优点是思路简单，往往通过 Pearson 相关系数法、方差选择法、互信息法等方法计算相关性，然后保留相关性最强的N个特征，就可以交给模型训练；缺点是没有考虑到特征与特征之间的相关性，从而导致模型最后的训练效果没那么好。 包裹式 包裹式特征选择是把最终要使用的机器学习模型、评测性能的指标作为特征选择的重要依据，每次去选择若干特征，或是排除若干特征。通常包裹式特征选择要比过滤式的效果更好，但由于训练过程时间久，系统的开销也更大。最典型的包裹型算法为递归特征删除算法，其原理是使用一个基模型（如：随机森林、逻辑回归等）进行多轮训练，每轮训练结束后，消除若干权值系数较低的特征，再基于新的特征集进行新的一轮训练。 嵌入式 嵌入式特征选择法是根据机器学习的算法、模型来分析特征的重要性，从而选择最重要的 N 个特征。与包裹式特征选择法最大的不同是，嵌入式方法是将特征选择过程与模型的训练过程结合为一体，这样就可以快速地找到最佳的特征集合，更加高效、快捷。常用的嵌入式特征选择方法有基于正则化项（如：L1正则化）的特征选择法和基于树模型的特征选择法（如：GBDT）。 降维 如果拿特征选择后的数据直接进行模型的训练，由于数据的特征矩阵维度大，可能会存在数据难以理解、计算量增大、训练时间过长等问题，因此我们要对数据进行降维。降维是指把原始高维空间的特征投影到低维度的空间，进行特征的重组，以减少数据的维度。降维与特征最大的不同在于，特征选择是进行特征的剔除、删减，而降维是做特征的重组构成新的特征，原始特征全部“消失”了，性质发生了根本的变化。常见的降维方法有：主成分分析法（PCA）和线性判别分析法（LDA）。 数据预处理 数据预处理是特征工程中最为重要的一个环节，良好的数据预处理可以使模型的训练达到事半功倍的效果。数据预处理旨在通过归一化、标准化、正则化等方式改进不完整、不一致、无法直接使用的数据。具体方法有： 连续特征 归一化 归一化是对数据集进行区间缩放，缩放到 [0,1] 的区间内，把有单位的数据转化为没有单位的数据，即统一数据的衡量标准，消除单位的影响。这样方便了数据的处理，使数据处理更加快速、敏捷。Skearn 中最常用的归一化的方法是：MinMaxScaler。此外还有对数函数转换（log），反余切转换等。 标准化 标准化是在不改变原数据分布的前提下，将数据按比例缩放，使之落入一个限定的区间，使数据之间具有可比性。但当个体特征太过或明显不遵从高斯正态分布时，标准化表现的效果会比较差。标准化的目的是为了方便数据的下一步处理，比如：进行的数据缩放等变换。常用的标准化方法有 z-score 标准化、StandardScaler 标准化等。 离散化/分段/分箱 离散化是把连续型的数值型特征分段，每一段内的数据都可以当做成一个新的特征。具体又可分为等步长方式离散化和等频率的方式离散化，等步长的方式比较简单，等频率的方式更加精准，会跟数据分布有很大的关系。 代码层面，可以用 pandas 中的 cut 方法进行切分。总之，离散化的特征能够提高模型的运行速度以及准确率。 二值化 特征的二值化处理是将数值型数据输出为布尔类型。其核心在于设定一个阈值，当样本书籍大于该阈值时，输出为 1 ，小于等于该阈值时输出为 0 。我们通常使用 preproccessing 库的 Binarizer 类对数据进行二值化处理。 离散特征 one hot 我们针对类别型的特征，通常采用哑编码（One_Hot Encodin）的方式。所谓的哑编码，直观的讲就是用N个维度来对N个类别进行编码，并且对于每个类别，只有一个维度有效，记作数字 1 ；其它维度均记作数字 0 。但有时使用哑编码的方式，可能会造成维度的灾难，所以通常我们在做哑编码之前，会先对特征进行 Hash 处理，把每个维度的特征编码成词向量。 以上为大家介绍了几种较为常见、通用的数据预处理方式，但只是浩大特征工程中的冰山一角。往往很多特征工程的方法需要我们在项目中不断去总结积累比如：针对缺失值的处理，在不同的数据集中，用均值填充、中位数填充、前后值填充的效果是不一样的；对于类别型的变量，有时我们不需要对全部的数据都进行哑编码处理；对于时间型的变量有时我们有时会把它当作是离散值，有时会当成连续值处理等。所以很多情况下，我们要根据实际问题，进行不同的数据预处理。 embdding 缺失值的处理 现实世界中的数据往往非常杂乱，未经处理的原始数据中某些属性数据缺失是经常出现的情况。另外，在做特征工程时经常会有些样本的某些特征无法求出。下面是几种处理数据中缺失值的主要方法。 删除 最简单的方法是删除，删除属性或者删除样本。如果大部分样本该属性都缺失，这个属性能提供的信息有限，可以选择放弃使用该维属性；如果一个样本大部分属性缺失，可以选择放弃该样本。虽然这种方法简单，但只适用于数据集中缺失较少的情况。 统计填充 对于缺失值的属性，尤其是数值类型的属性，根据所有样本关于这维属性的统计值对其进行填充，如使用平均数、中位数、众数、最大值、最小值等，具体选择哪种统计值需要具体问题具体分析。另外，如果有可用类别信息，还可以进行类内统计，比如身高，男性和女性的统计填充应该是不同的。 统一填充 对于含缺失值的属性，把所有缺失值统一填充为自定义值，如何选择自定义值也需要具体问题具体分析。当然，如果有可用类别信息，也可以为不同类别分别进行统一填充。常用的统一填充值有：“空”、“0”、“正无穷”、“负无穷”等。 预测填充 我们可以通过预测模型利用不存在缺失值的属性来预测缺失值，也就是先用预测模型把数据填充后再做进一步的工作，如统计、学习等。虽然这种方法比较复杂，但是最后得到的结果比较好。 具体分析 上面两次提到具体问题具体分析，为什么要具体问题具体分析呢？因为属性缺失有时并不意味着数据缺失，缺失本身是包含信息的，所以需要根据不同应用场景下缺失值可能包含的信息进行合理填充。下面通过一些例子来说明如何具体问题具体分析，仁者见仁智者见智： 性别：1男，2女，0未知，通过名称预测，大于置信度的取预测值 年龄： 价格：...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E5%BB%BA%E6%A8%A1%E8%BF%87%E7%A8%8B.html",
        "teaser":null},{
        "title": "推荐算法",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/index.html",
        "teaser":null},{
        "title": "协同过滤推荐算法总结",
        
        "excerpt":
            "参考文献   参考文献   　　　　推荐算法具有非常多的应用场景和商业价值，因此对推荐算法值得好好研究。推荐算法种类很多，但是目前应用最广泛的应该是协同过滤类别的推荐算法，本文就对协同过滤类别的推荐算法做一个概括总结，后续也会对一些典型的协同过滤推荐算法做原理总结。   推荐算法概述   　　　　推荐算法是非常古老的，在机器学习还没有兴起的时候就有需求和应用了。概括来说，可以分为以下5种：   　　　　1）基于内容的推荐：这一类一般依赖于自然语言处理NLP的一些知识，通过挖掘文本的TF-IDF特征向量，来得到用户的偏好，进而做推荐。这类推荐算法可以找到用户独特的小众喜好，而且还有较好的解释性。这一类由于需要NLP的基础，本文就不多讲，在后面专门讲NLP的时候再讨论。   　　　　2）协调过滤推荐：本文后面要专门讲的内容。协调过滤是推荐算法中目前最主流的种类，花样繁多，在工业界已经有了很多广泛的应用。它的优点是不需要太多特定领域的知识，可以通过基于统计的机器学习算法来得到较好的推荐效果。最大的优点是工程上容易实现，可以方便应用到产品中。目前绝大多数实际应用的推荐算法都是协同过滤推荐算法。   　　　　3）混合推荐：这个类似我们机器学习中的集成学习，博才众长，通过多个推荐算法的结合，得到一个更好的推荐算法，起到三个臭皮匠顶一个诸葛亮的作用。比如通过建立多个推荐算法的模型，最后用投票法决定最终的推荐结果。混合推荐理论上不会比单一任何一种推荐算法差，但是使用混合推荐，算法复杂度就提高了，在实际应用中有使用，但是并没有单一的协调过滤推荐算法，比如逻辑回归之类的二分类推荐算法广泛。   　　　　4）基于规则的推荐：这类算法常见的比如基于最多用户点击，最多用户浏览等，属于大众型的推荐方法，在目前的大数据时代并不主流。   　　　　5）基于人口统计信息的推荐：这一类是最简单的推荐算法了，它只是简单的根据系统用户的基本信息发现用户的相关程度，然后进行推荐，目前在大型系统中已经较少使用。      协同过滤推荐概述   　　　　协同过滤(Collaborative Filtering)作为推荐算法中最经典的类型，包括在线的协同和离线的过滤两部分。所谓在线协同，就是通过在线数据找到用户可能喜欢的物品，而离线过滤，则是过滤掉一些不值得推荐的数据，比比如推荐值评分低的数据，或者虽然推荐值高但是用户已经购买的数据。   　　　　协同过滤的模型一般为m个物品，m个用户的数据，只有部分用户和部分数据之间是有评分数据的，其它部分评分是空白，此时我们要用已有的部分稀疏数据来预测那些空白的物品和数据之间的评分关系，找到最高评分的物品推荐给用户。   　　　　一般来说，协同过滤推荐分为三种类型。第一种是基于用户(user-based)的协同过滤，第二种是基于项目(item-based)的协同过滤，第三种是基于模型(model based)的协同过滤。   　　　　基于用户(user-based)的协同过滤主要考虑的是用户和用户之间的相似度，只要找出相似用户喜欢的物品，并预测目标用户对对应物品的评分，就可以找到评分最高的若干个物品推荐给用户。而基于项目(item-based)的协同过滤和基于用户的协同过滤类似，只不过这时我们转向找到物品和物品之间的相似度，只有找到了目标用户对某些物品的评分，那么我们就可以对相似度高的类似物品进行预测，将评分最高的若干个相似物品推荐给用户。比如你在网上买了一本机器学习相关的书，网站马上会推荐一堆机器学习，大数据相关的书给你，这里就明显用到了基于项目的协同过滤思想。   　　　　我们可以简单比较下基于用户的协同过滤和基于项目的协同过滤：基于用户的协同过滤需要在线找用户和用户之间的相似度关系，计算复杂度肯定会比基于基于项目的协同过滤高。但是可以帮助用户找到新类别的有惊喜的物品。而基于项目的协同过滤，由于考虑的物品的相似性一段时间不会改变，因此可以很容易的离线计算，准确度一般也可以接受，但是推荐的多样性来说，就很难带给用户惊喜了。一般对于小型的推荐系统来说，基于项目的协同过滤肯定是主流。但是如果是大型的推荐系统来说，则可以考虑基于用户的协同过滤，当然更加可以考虑我们的第三种类型，基于模型的协同过滤。   　　　　基于模型(model based)的协同过滤是目前最主流的协同过滤类型了，我们的一大堆机器学习算法也可以在这里找到用武之地。下面我们就重点介绍基于模型的协同过滤。   基于模型的协同过滤   　　　　基于模型的协同过滤作为目前最主流的协同过滤类型，其相关算法可以写一本书了，当然我们这里主要是对其思想做有一个归类概括。我们的问题是这样的m个物品，m个用户的数据，只有部分用户和部分数据之间是有评分数据的，其它部分评分是空白，此时我们要用已有的部分稀疏数据来预测那些空白的物品和数据之间的评分关系，找到最高评分的物品推荐给用户。   　　　　对于这个问题，用机器学习的思想来建模解决，主流的方法可以分为：用关联算法，聚类算法，分类算法，回归算法，矩阵分解，神经网络,图模型以及隐语义模型来解决。下面我们分别加以介绍。   用关联算法做协同过滤   　　　　一般我们可以找出用户购买的所有物品数据里频繁出现的项集活序列，来做频繁集挖掘，找到满足支持度阈值的关联物品的频繁N项集或者序列。如果用户购买了频繁N项集或者序列里的部分物品，那么我们可以将频繁项集或序列里的其他物品按一定的评分准则推荐给用户，这个评分准则可以包括支持度，置信度和提升度等。   　　　　常用的关联推荐算法有Apriori，FP Tree和PrefixSpan。如果大家不熟悉这些算法，可以参考我的另外几篇文章：   　　　　Apriori算法原理总结   　　　　FP Tree算法原理总结   　　　　PrefixSpan算法原理总结　   用聚类算法做协同过滤   　　　　用聚类算法做协同过滤就和前面的基于用户或者项目的协同过滤有些类似了。我们可以按照用户或者按照物品基于一定的距离度量来进行聚类。如果基于用户聚类，则可以将用户按照一定距离度量方式分成不同的目标人群，将同样目标人群评分高的物品推荐给目标用户。基于物品聚类的话，则是将用户评分高物品的相似同类物品推荐给用户。   　　　　常用的聚类推荐算法有K-Means, BIRCH, DBSCAN和谱聚类，如果大家不熟悉这些算法，可以参考我的另外几篇文章：   　　　　K-Means聚类算法原理   　　　　BIRCH聚类算法原理   　　　　DBSCAN密度聚类算法   　　　　谱聚类（spectral clustering）原理总结   用分类算法做协同过滤   　　　　如果我们根据用户评分的高低，将分数分成几段的话，则这个问题变成分类问题。比如最直接的，设置一份评分阈值，评分高于阈值的就是推荐，评分低于阈值就是不推荐，我们将问题变成了一个二分类问题。虽然分类问题的算法多如牛毛，但是目前使用最广泛的是逻辑回归。为啥是逻辑回归而不是看起来更加高大上的比如支持向量机呢？因为逻辑回归的解释性比较强，每个物品是否推荐我们都有一个明确的概率放在这，同时可以对数据的特征做工程化，得到调优的目的。目前逻辑回归做协同过滤在BAT等大厂已经非常成熟了。   　　　　常见的分类推荐算法有逻辑回归和朴素贝叶斯，两者的特点是解释性很强。如果大家不熟悉这些算法，可以参考我的另外几篇文章：   　　　　逻辑回归原理小结   　　　　朴素贝叶斯算法原理小结   用回归算法做协同过滤   　　　　用回归算法做协同过滤比分类算法看起来更加的自然。我们的评分可以是一个连续的值而不是离散的值，通过回归模型我们可以得到目标用户对某商品的预测打分。   　　　　常用的回归推荐算法有Ridge回归，回归树和支持向量回归。如果大家不熟悉这些算法，可以参考我的另外几篇文章：   　　　　线性回归原理小结   　　　　决策树算法原理(下)   　　　　支持向量机原理(五)线性支持回归   3.5 用矩阵分解做协同过滤   　　　　用矩阵分解做协同过滤是目前使用也很广泛的一种方法。由于传统的奇异值分解SVD要求矩阵不能有缺失数据，必须是稠密的，而我们的用户物品评分矩阵是一个很典型的稀疏矩阵，直接使用传统的SVD到协同过滤是比较复杂的。   　　　　目前主流的矩阵分解推荐算法主要是SVD的一些变种，比如FunkSVD，BiasSVD和SVD++。这些算法和传统SVD的最大区别是不再要求将矩阵分解为UΣVTUΣVTU\\Sigma V^T的形式，而变是两个低秩矩阵PTQPTQP^TQ的乘积形式。对于矩阵分解的推荐算法，后续我会专门开篇来讲。   3.6 用神经网络做协同过滤   　　　　用神经网络乃至深度学习做协同过滤应该是以后的一个趋势。目前比较主流的用两层神经网络来做推荐算法的是限制玻尔兹曼机(RBM)。在目前的Netflix算法比赛中， RBM算法的表现很牛。当然如果用深层的神经网络来做协同过滤应该会更好，大厂商用深度学习的方法来做协同过滤应该是将来的一个趋势。后续我会专门开篇来讲讲RBM。   3.7  用图模型做协同过滤   　　　　用图模型做协同过滤，则将用户之间的相似度放到了一个图模型里面去考虑，常用的算法是SimRank系列算法和马尔科夫模型算法。对于SimRank系列算法，它的基本思想是被相似对象引用的两个对象也具有相似性。算法思想有点类似于大名鼎鼎的PageRank。而马尔科夫模型算法当然是基于马尔科夫链了，它的基本思想是基于传导性来找出普通距离度量算法难以找出的相似性。后续我会专门开篇来讲讲SimRank系列算法。　   3.8 用隐语义模型做协同过滤   　　　　隐语义模型主要是基于NLP的，涉及到对用户行为的语义分析来做评分推荐，主要方法有隐性语义分析LSA和隐含狄利克雷分布LDA，这些等讲NLP的再专门讲。   4. 协同过滤的一些新方向   　　　　当然推荐算法的变革也在进行中，就算是最火爆的基于逻辑回归推荐算法也在面临被取代。哪些算法可能取代逻辑回归之类的传统协同过滤呢？下面是我的理解：   　　　　a) 基于集成学习的方法和混合推荐:这个和混合推荐也靠在一起了。由于集成学习的成熟，在推荐算法上也有较好的表现。一个可能取代逻辑回归的算法是GBDT。目前GBDT在很多算法比赛都有好的表现，而有工业级的并行化实现类库。   　　　　b)基于矩阵分解的方法：矩阵分解，由于方法简单，一直受到青睐。目前开始渐渐流行的矩阵分解方法有分解机(Factorization Machine)和张量分解(Tensor Factorization)。   　　　　c) 基于深度学习的方法：目前两层的神经网络RBM都已经有非常好的推荐算法效果，而随着深度学习和多层神经网络的兴起，以后可能推荐算法就是深度学习的天下了？目前看最火爆的是基于CNN和RNN的推荐算法。   5. 协同过滤总结　   　　　　协同过滤作为一种经典的推荐算法种类，在工业界应用广泛，它的优点很多，模型通用性强，不需要太多对应数据领域的专业知识，工程实现简单，效果也不错。这些都是它流行的原因。   　　　　当然，协同过滤也有些难以避免的难题，比如令人头疼的“冷启动”问题，我们没有新用户任何数据的时候，无法较好的为新用户推荐物品。同时也没有考虑情景的差异，比如根据用户所在的场景和用户当前的情绪。当然，也无法得到一些小众的独特喜好，这块是基于内容的推荐比较擅长的。　　　   　　　　以上就是协同过滤推荐算法的一个总结，希望可以帮大家对推荐算法有一个更深的认识，并预祝大家新年快乐！   ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93.html",
        "teaser":null},{
        "title": "矩阵分解",
        
        "excerpt":
            "在协同过滤推荐算法总结中，我们讲到了用矩阵分解做协同过滤是广泛使用的方法，这里就对矩阵分解在协同过滤推荐算法中的应用做一个总结。(过年前最后一篇！祝大家新年快乐！明年的目标是写120篇机器学习，深度学习和NLP相关的文章) 1. 矩阵分解用于推荐算法要解决的问题 在推荐系统中，我们常常遇到的问题是这样的，我们有很多用户和物品，也有少部分用户对少部分物品的评分，我们希望预测目标用户对其他未评分物品的评分，进而将评分高的物品推荐给目标用户。比如下面的用户物品评分表; 对于每个用户，我们希望较准确的预测出用户对未评分物品的评分。 对于这个问题我们有很多解决方法，本文我们关注于用矩阵分解的方法来做。如果将m个用户和n个物品对应的评分看做一个矩阵$M$，我们希望通过矩阵分解来解决这个问题。 2. 传统的奇异值分解SVD用于推荐 说道矩阵分解，我们首先想到的就是奇异值分解SVD。在奇异值分解(SVD)原理与在降维中的应用中，我们对SVD原理做了总结。如果大家对SVD不熟悉的话，可以翻看该文。 此时可以将这个用户物品对应的$m \\times n$矩阵$M$进行SVD分解，并通过选择部分较大的一些奇异值来同时进行降维，也就是说矩阵$M$此时分解为： 其中k是矩阵$M$中较大的部分奇异值的个数，一般会远远的小于用户数和物品树。如果我们要预测第i个用户对第j个物品的评分$m_{ij}$,则只需要计算$u_i^T\\Sigma v_j$即可。通过这种方法，我们可以将评分表里面所有没有评分的位置得到一个预测评分。通过找到最高的若干个评分对应的物品推荐给用户。 可以看出这种方法简单直接，似乎很有吸引力。但是有一个很大的问题我们忽略了，就是SVD分解要求矩阵是稠密的，也就是说矩阵的所有位置不能有空白。有空白时我们的$M$是没法直接去SVD分解的。大家会说，如果这个矩阵是稠密的，那不就是说我们都已经找到所有用户物品的评分了嘛，那还要SVD干嘛! 的确，这是一个问题，传统SVD采用的方法是对评分矩阵中的缺失值进行简单的补全，比如用全局平均值或者用用户物品平均值补全，得到补全后的矩阵。接着可以用SVD分解并降维。 虽然有了上面的补全策略，我们的传统SVD在推荐算法上还是较难使用。因为我们的用户数和物品一般都是超级大，随便就成千上万了。这么大一个矩阵做SVD分解是非常耗时的。那么有没有简化版的矩阵分解可以用呢？我们下面来看看实际可以用于推荐系统的矩阵分解。 3. FunkSVD算法用于推荐 FunkSVD是在传统SVD面临计算效率问题时提出来的，既然将一个矩阵做SVD分解成3个矩阵很耗时，同时还面临稀疏的问题，那么我们能不能避开稀疏问题，同时只分解成两个矩阵呢？也就是说，现在期望我们的矩阵$M$这样进行分解： 我们知道SVD分解已经很成熟了，但是FunkSVD如何将矩阵$M$分解为$P$和$Q$呢？这里采用了线性回归的思想。我们的目标是让用户的评分和用矩阵乘积得到的评分残差尽可能的小，也就是说，可以用均方差作为损失函数，来寻找最终的$P$和$Q$。 对于某一个用户评分$m_{ij}$，如果用FunkSVD进行矩阵分解，则对应的表示为$q_j^Tp_i$，采用均方差做为损失函数，则我们期望$(m_{ij}-q_j^Tp_i)^2$尽可能的小，如果考虑所有的物品和样本的组合，则我们期望最小化下式： 只要我们能够最小化上面的式子，并求出极值所对应的$p_i, q_j$，则我们最终可以得到矩阵$P$和$Q$，那么对于任意矩阵$M$任意一个空白评分的位置，我们可以通过$q_j^Tp_i$计算预测评分。很漂亮的方法！ 当然，在实际应用中，我们为了防止过拟合，会加入一个L2的正则化项，因此正式的FunkSVD的优化目标函数$J(p,q)$是这样的：$$\\underbrace{arg\\;min}{p_i,q_j}\\;\\sum\\limits{i,j}(m_{ij}-q_j^Tp_i)^2 + \\lambda(   p_i   _2^2 +   q_j   _2^2 ) $$ 其中$\\lambda$为正则化系数，需要调参。对于这个优化问题，我们一般通过梯度下降法来进行优化得到结果。 将上式分别对$p_i, q_j$求导我们得到: 则在梯度下降法迭代时，$p_i, q_j$的迭代公式为： 通过迭代我们最终可以得到$P$和$Q$，进而用于推荐。FunkSVD算法虽然思想很简单，但是在实际应用中效果非常好，这真是验证了大道至简。 4. BiasSVD算法用于推荐 在FunkSVD算法火爆之后，出现了很多FunkSVD的改进版算法。其中BiasSVD算是改进的比较成功的一种算法。BiasSVD假设评分系统包括三部分的偏置因素：一些和用户物品无关的评分因素，用户有一些和物品无关的评分因素，称为用户偏置项。而物品也有一些和用户无关的评分因素，称为物品偏置项。这其实很好理解。比如一个垃圾山寨货评分不可能高，自带这种烂属性的物品由于这个因素会直接导致用户评分低，与用户无关。 假设评分系统平均分为$\\mu$,第i个用户的用户偏置项为$b_i$,而第j个物品的物品偏置项为$b_j$，则加入了偏置项以后的优化目标函数$J(p,q)$是这样的$$\\underbrace{arg\\;min}{p_i,q_j}\\;\\sum\\limits{i,j}(m_{ij}-\\mu-b_i-b_j-q_j^Tp_i)^2 + \\lambda(   p_i   _2^2 +   q_j   _2^2 +   b_i   _2^2 +   b_j   _2^2) $$ 这个优化目标也可以采用梯度下降法求解。和FunkSVD不同的是，此时我们多了两个偏执项$b_i,b_j$,，$p_i, q_j$的迭代公式和FunkSVD类似，只是每一步的梯度导数稍有不同而已，这里就不给出了。而$b_i,b_j$一般可以初始设置为0，然后参与迭代。这里给出$b_i,b_j$的迭代方法 通过迭代我们最终可以得到$P$和$Q$，进而用于推荐。BiasSVD增加了一些额外因素的考虑，因此在某些场景会比FunkSVD表现好。 5. SVD++算法用于推荐 SVD++算法在BiasSVD算法上进一步做了增强，这里它增加考虑用户的隐式反馈。好吧，一个简单漂亮的FunkSVD硬是被越改越复杂。 对于某一个用户i，它提供了隐式反馈的物品集合定义为$N(i)$, 这个用户对某个物品j对应的隐式反馈修正的评分值为$c_{ij}$, 那么该用户所有的评分修正值为$\\sum\\limits_{s \\in N(i)}c_{sj}$。一般我们将它表示为用$q_j^Ty_s$形式，则加入了隐式反馈项以后的优化目标函数$J(p,q)$是这样的:$$\\underbrace{arg\\;min}{p_i,q_j}\\;\\sum\\limits{i,j}(m_{ij}-\\mu-b_i-b_j-q_j^Tp_i - q_j^T N(i) ^{-1/2}\\sum\\limits_{s \\in N(i)}y_{s})^2+ \\lambda(   p_i   _2^2 +   q_j   _2^2 +   b_i...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3.html",
        "teaser":null},{
        "title": "LDA模型",
        
        "excerpt":
            "LDA基础 隐含狄利克雷分布(Latent Dirichlet Allocation，以下简称LDA)。注意机器学习还有一个LDA，即线性判别分析，主要是用于降维和分类的。文本关注于隐含狄利克雷分布对应的LDA。 LDA贝叶斯模型 LDA是基于贝叶斯模型的，涉及到贝叶斯模型离不开“先验分布”，“数据（似然）”和”后验分布”三块。 在贝叶斯学派这里： 先验分布 + 数据（似然）= 后验分布 这点其实很好理解，因为这符合我们人的思维方式，比如你对好人和坏人的认知，先验分布为：100个好人和100个的坏人，即你认为好人坏人各占一半，现在你被2个好人（数据）帮助了和1个坏人骗了，于是你得到了新的后验分布为：102个好人和101个的坏人。现在你的后验分布里面认为好人比坏人多了。这个后验分布接着又变成你的新的先验分布，当你被1个好人（数据）帮助了和3个坏人（数据）骗了后，你又更新了你的后验分布为：103个好人和104个的坏人。依次继续更新下去。 二项分布与Beta分布 对于上一节的贝叶斯模型和认知过程，假如用数学和概率的方式该如何表达呢？ 对于我们的数据（似然），这个好办，用一个二项分布就可以搞定，即对于二项分布：$$Binom(k n,p) = {n \\choose k}p^k(1-p)^{n-k}$$ 其中p我们可以理解为好人的概率，k为好人的个数，n为好人坏人的总数。 虽然数据(似然)很好理解，但是对于先验分布，我们就要费一番脑筋了，为什么呢？因为我们希望这个先验分布和数据（似然）对应的二项分布集合后，得到的后验分布在后面还可以作为先验分布！就像上面例子里的“102个好人和101个的坏人”，它是前面一次贝叶斯推荐的后验分布，又是后一次贝叶斯推荐的先验分布。也即是说，我们希望先验分布和后验分布的形式应该是一样的，这样的分布我们一般叫共轭分布。在我们的例子里，我们希望找到和二项分布共轭的分布。 和二项分布共轭的分布其实就是Beta分布。Beta分布的表达式为：$$Beta(p \\alpha,\\beta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}p^{\\alpha-1}(1-p)^$$ 其中$\\Gamma$是Gamma函数，满足$\\Gamma(x) = (x-1)!$ 仔细观察Beta分布和二项分布，可以发现两者的密度函数很相似，区别仅仅在前面的归一化的阶乘项。那么它如何做到先验分布和后验分布的形式一样呢？后验分布$P(p n,k,\\alpha,\\beta)$推导如下： \\begin{align} P(p|n,k,\\alpha,\\beta) &amp; \\propto P(k|n,p)P(p|\\alpha,\\beta) \\ &amp; = P(k|n,p)P(p|\\alpha,\\beta) \\&amp; = Binom(k|n,p) Beta(p|\\alpha,\\beta) \\ &amp;= {n \\choose k}p^k(1-p)^{n-k} \\times \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}p^{\\alpha-1}(1-p)^ \\&amp; \\propto p^{k+\\alpha-1}(1-p)^{n-k + \\beta -1} \\end{align} 将上面最后的式子归一化以后，得到我们的后验概率为：$$P(p n,k,\\alpha,\\beta) = \\frac{\\Gamma(\\alpha + \\beta + n)}{\\Gamma(\\alpha + k)\\Gamma(\\beta + n - k)}p^{k+\\alpha-1}(1-p)^{n-k + \\beta -1} $$ 可见我们的后验分布的确是Beta分布，而且我们发现：$$ Beta(p \\alpha,\\beta) + BinomCount(k,n-k) = Beta(p \\alpha + k,\\beta +n-k)$$ 这个式子完全符合我们在上一节好人坏人例子里的情况，我们的认知会把数据里的好人坏人数分别加到我们的先验分布上，得到后验分布。 我们在来看看Beta分布$Beta(p|\\alpha,\\beta)$的期望: \\begin{align} E(Beta(p|\\alpha,\\beta)) &amp; = \\int_{0}^{1}tBeta(p|\\alpha,\\beta)dt \\&amp; =...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98/LDA%E6%A8%A1%E5%9E%8B.html",
        "teaser":null},{
        "title": "Word2Vec",
        
        "excerpt":
            "简介 word2vec是google在2013年推出的一个NLP工具，它的特点是将所有的词向量化，这样词与词之间就可以定量的去度量他们之间的关系，挖掘词之间的联系。虽然源码是开源的，但是谷歌的代码库国内无法访问，因此本文的讲解word2vec原理以Github上的word2vec代码为准。本文关注于word2vec的基础知识。 词向量基础 用词向量来表示词并不是word2vec的首创，在很久之前就出现了。最早的词向量是很冗长的，它使用是词向量维度大小为整个词汇表的大小，对于每个具体的词汇表中的词，将对应的位置置为1。比如我们有下面的5个词组成的词汇表，词”Queen”的序号为2， 那么它的词向量就是$(0,1,0,0,0)$。同样的道理，词”Woman”的词向量就是$(0,0,0,1,0)$。这种词向量的编码方式我们一般叫做1-of-N representation或者one hot representation. One hot representation用来表示词向量非常简单，但是却有很多问题。最大的问题是我们的词汇表一般都非常大，比如达到百万级别，这样每个词都用百万维的向量来表示简直是内存的灾难。这样的向量其实除了一个位置是1，其余的位置全部都是0，表达的效率不高，能不能把词向量的维度变小呢？ Distributed representation可以解决One hot representation的问题，它的思路是通过训练，将每个词都映射到一个较短的词向量上来。所有的这些词向量就构成了向量空间，进而可以用普通的统计学的方法来研究词与词之间的关系。这个较短的词向量维度是多大呢？这个一般需要我们在训练时自己来指定。 比如下图我们将词汇表里的词用”Royalty”,”Masculinity”, “Femininity”和”Age”4个维度来表示，King这个词对应的词向量可能是$(0.99, 0.99,0.05, 0.7)$。当然在实际情况中，我们并不能对词向量的每个维度做一个很好的解释。 有了用Distributed Representation表示的较短的词向量，我们就可以较容易的分析词之间的关系了，比如我们将词的维度降维到2维，有一个有趣的研究表明，用下图的词向量表示我们的词时，我们可以发现： 可见我们只要得到了词汇表里所有词对应的词向量，那么我们就可以做很多有趣的事情了。不过，怎么训练得到合适的词向量呢？一个很常见的方法是使用神经网络语言模型。 CBOW与Skip-Gram用于神经网络语言模型 在word2vec出现之前，已经有用神经网络DNN来用训练词向量进而处理词与词之间的关系了。采用的方法一般是一个三层的神经网络结构（当然也可以多层），分为输入层，隐藏层和输出层(softmax层)。 这个模型是如何定义数据的输入和输出呢？一般分为CBOW(Continuous Bag-of-Words 与Skip-Gram两种模型。 CBOW模型的训练输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。比如下面这段话，我们的上下文大小取值为4，特定的这个词是”Learning”，也就是我们需要的输出词向量,上下文对应的词有8个，前后各4个，这8个词是我们模型的输入。由于CBOW使用的是词袋模型，因此这8个词都是平等的，也就是不考虑他们和我们关注的词之间的距离大小，只要在我们上下文之内即可。 这样我们这个CBOW的例子里，我们的输入是8个词向量，输出是所有词的softmax概率（训练的目标是期望训练样本特定词对应的softmax概率最大），对应的CBOW神经网络模型输入层有8个神经元，输出层有词汇表大小个神经元。隐藏层的神经元个数我们可以自己指定。通过DNN的反向传播算法，我们可以求出DNN模型的参数，同时得到所有的词对应的词向量。这样当我们有新的需求，要求出某8个词对应的最可能的输出中心词时，我们可以通过一次DNN前向传播算法并通过softmax激活函数找到概率最大的词对应的神经元即可。 Skip-Gram模型和CBOW的思路是反着来的，即输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量。还是上面的例子，我们的上下文大小取值为4， 特定的这个词”Learning”是我们的输入，而这8个上下文词是我们的输出。 这样我们这个Skip-Gram的例子里，我们的输入是特定词， 输出是softmax概率排前8的8个词，对应的Skip-Gram神经网络模型输入层有1个神经元，输出层有词汇表大小个神经元。隐藏层的神经元个数我们可以自己指定。通过DNN的反向传播算法，我们可以求出DNN模型的参数，同时得到所有的词对应的词向量。这样当我们有新的需求，要求出某1个词对应的最可能的8个上下文词时，我们可以通过一次DNN前向传播算法得到概率大小排前8的softmax概率对应的神经元所对应的词即可。 以上就是神经网络语言模型中如何用CBOW与Skip-Gram来训练模型与得到词向量的大概过程。但是这和word2vec中用CBOW与Skip-Gram来训练模型与得到词向量的过程有很多的不同。 word2vec为什么 不用现成的DNN模型，要继续优化出新方法呢？最主要的问题是DNN模型的这个处理过程非常耗时。我们的词汇表一般在百万级别以上，这意味着我们DNN的输出层需要进行softmax计算各个词的输出概率的的计算量很大。有没有简化一点点的方法呢？ word2vec基础之霍夫曼树 word2vec也使用了CBOW与Skip-Gram来训练模型与得到词向量，但是并没有使用传统的DNN模型。最先优化使用的数据结构是用霍夫曼树来代替隐藏层和输出层的神经元，霍夫曼树的叶子节点起到输出层神经元的作用，叶子节点的个数即为词汇表的小大。 而内部节点则起到隐藏层神经元的作用。 具体如何用霍夫曼树来进行CBOW和Skip-Gram的训练我们在下一节讲，这里我们先复习下霍夫曼树。 霍夫曼树的建立其实并不难，过程如下： 输入：权值为$(w_1,w_2,…w_n)$的$n$个节点 输出：对应的霍夫曼树 1）将$(w_1,w_2,…w_n)$看做是有$n$棵树的森林，每个树仅有一个节点。 2）在森林中选择根节点权值最小的两棵树进行合并，得到一个新的树，这两颗树分布作为新树的左右子树。新树的根节点权重为左右子树的根节点权重之和。 3） 将之前的根节点权值最小的两棵树从森林删除，并把新树加入森林。 4）重复步骤2）和3）直到森林里只有一棵树为止。 下面我们用一个具体的例子来说明霍夫曼树建立的过程，我们有(a,b,c,d,e,f)共6个节点，节点的权值分布是(20,4,8,6,16,3)。 首先是最小的b和f合并，得到的新树根节点权重是7.此时森林里5棵树，根节点权重分别是20,8,6,16,7。此时根节点权重最小的6,7合并，得到新子树，依次类推，最终得到下面的霍夫曼树。 那么霍夫曼树有什么好处呢？一般得到霍夫曼树后我们会对叶子节点进行霍夫曼编码，由于权重高的叶子节点越靠近根节点，而权重低的叶子节点会远离根节点，这样我们的高权重节点编码值较短，而低权重值编码值较长。这保证的树的带权路径最短，也符合我们的信息论，即我们希望越常用的词拥有更短的编码。如何编码呢？一般对于一个霍夫曼树的节点（根节点除外），可以约定左子树编码为0，右子树编码为1.如上图，则可以得到c的编码是00。 在word2vec中，约定编码方式和上面的例子相反，即约定左子树编码为1，右子树编码为0，同时约定左子树的权重不小于右子树的权重。 word2vec有两种改进方法，一种是基于Hierarchical Softmax的，另一种是基于Negative Sampling的。 基于Hierarchical Softmax的模型 模型概述 我们先回顾下传统的神经网络词向量语言模型，里面一般有三层，输入层（词向量），隐藏层和输出层（softmax层）。里面最大的问题在于从隐藏层到输出的softmax层的计算量很大，因为要计算所有词的softmax概率，再去找概率最大的值。这个模型如下图所示。其中$V$是词汇表的大小， word2vec对这个模型做了改进，首先，对于从输入层到隐藏层的映射，没有采取神经网络的线性变换加激活函数的方法，而是采用简单的对所有输入词向量求和并取平均的方法。比如输入的是三个4维词向量：$(1,2,3,4), (9,6,11,8),(5,10,7,12)$,那么我们word2vec映射后的词向量就是$(5,6,7,8)$。由于这里是从多个词向量变成了一个词向量。 第二个改进就是从隐藏层到输出的softmax层这里的计算量个改进。为了避免要计算所有词的softmax概率，word2vec采样了霍夫曼树来代替从隐藏层到输出softmax层的映射。我们在上一节已经介绍了霍夫曼树的原理。如何映射呢？这里就是理解word2vec的关键所在了。 由于我们把之前所有都要计算的从输出softmax层的概率计算变成了一颗二叉霍夫曼树，那么我们的softmax概率计算只需要沿着树形结构进行就可以了。如下图所示，我们可以沿着霍夫曼树从根节点一直走到我们的叶子节点的词$w_2$。 和之前的神经网络语言模型相比，我们的霍夫曼树的所有内部节点就类似之前神经网络隐藏层的神经元,其中，根节点的词向量对应我们的投影后的词向量，而所有叶子节点就类似于之前神经网络softmax输出层的神经元，叶子节点的个数就是词汇表的大小。在霍夫曼树中，隐藏层到输出层的softmax映射不是一下子完成的，而是沿着霍夫曼树一步步完成的，因此这种softmax取名为”Hierarchical Softmax”。 如何“沿着霍夫曼树一步步完成”呢？在word2vec中，我们采用了二元逻辑回归的方法，即规定沿着左子树走，那么就是负类(霍夫曼树编码1)，沿着右子树走，那么就是正类(霍夫曼树编码0)。判别正类和负类的方法是使用sigmoid函数，即： 其中$x_w$是当前内部节点的词向量，而$\\theta$则是我们需要从训练样本求出的逻辑回归的模型参数。 使用霍夫曼树有什么好处呢？首先，由于是二叉树，之前计算量为$V$,现在变成了$log_2V$。第二，由于使用霍夫曼树是高频的词靠近树根，这样高频词需要更少的时间会被找到，这符合我们的贪心优化思想。 容易理解，被划分为左子树而成为负类的概率为$P(-) = 1-P(+)$。在某一个内部节点，要判断是沿左子树还是右子树走的标准就是看$P(-),P(+)$谁的概率值大。而控制$P(-),P(+)$谁的概率值大的因素一个是当前节点的词向量，另一个是当前节点的模型参数$\\theta$。 对于上图中的$w_2$，如果它是一个训练样本的输出，那么我们期望对于里面的隐藏节点$n(w_2,1)$的$P(-)$概率大，$n(w_2,2)$的$P(-)$概率大，$n(w_2,3)$的$P(+)$概率大。 回到基于Hierarchical Softmax的word2vec本身，我们的目标就是找到合适的所有节点的词向量和所有内部节点$\\theta$, 使训练样本达到最大似然。那么如何达到最大似然呢？ 模型梯度计算 我们使用最大似然法来寻找所有节点的词向量和所有内部节点$\\theta$。先拿上面的$w_2$例子来看，我们期望最大化下面的似然函数： 对于所有的训练样本，我们期望最大化所有样本的似然函数乘积。 为了便于我们后面一般化的描述，我们定义输入的词为$w$,其从输入层词向量求和平均后的霍夫曼树根节点词向量为$x_w$, 从根节点到$w$所在的叶子节点，包含的节点总数为$l_w$, $w$在霍夫曼树中从根节点开始，经过的第$i$个节点表示为$p_i^w$,对应的霍夫曼编码为$d_i^w \\in {0,1}$,其中$i =2,3,…l_w$。而该节点对应的模型参数表示为$\\theta_i^w$, 其中$i =1,2,…l_w-1$，没有$i =l_w$是因为模型参数仅仅针对于霍夫曼树的内部节点。 定义$w$经过的霍夫曼树某一个节点j的逻辑回归概率为$P(d_j^w x_w, \\theta_{j-1}^w)$，其表达式为： 那么对于某一个目标输出词$w$,其最大似然为：$$\\prod_{j=2}^{l_w}P(d_j^w x_w, \\theta_{j-1}^w) = \\prod_{j=2}^{l_w} [\\sigma(x_w^T\\theta_{j-1}^w)] ^{1-d_j^w}[1-\\sigma(x_w^T\\theta_{j-1}^w)]^{d_j^w}$$ 在word2vec中，由于使用的是随机梯度上升法，所以并没有把所有样本的似然乘起来得到真正的训练集最大似然，仅仅每次只用一个样本更新梯度，这样做的目的是减少梯度计算量。这样我们可以得到$w$的对数似然函数$L$如下： 要得到模型中$w$词向量和内部节点的模型参数$\\theta$, 我们使用梯度上升法即可。首先我们求模型参数$\\theta_{j-1}^w$的梯度：...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98/Word2Vec.html",
        "teaser":null},{
        "title": "文本挖掘",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98/index.html",
        "teaser":null},{
        "title": "文本主题模型之潜在语义索引(LSI)",
        
        "excerpt":
            "   title: algorithms-文本主题模型之潜在语义索引(LSI)   author: niult   date: 2019-01-16   category: 文本挖掘   tags: python,numpy,文本挖掘   简介  在文本挖掘中，主题模型是比较特殊的一块，它的思想不同于我们常用的机器学习算法，因此这里我们需要专门来总结文本主题模型的算法。本文关注于潜在语义索引算法(LSI)的原理。   文本主题模型的问题特点   在数据分析中，我们经常会进行非监督学习的聚类算法，它可以对我们的特征数据进行非监督的聚类。而主题模型也是非监督的算法，目的是得到文本按照主题的概率分布。从这个方面来说，主题模型和普通的聚类算法非常的类似。但是两者其实还是有区别的。   聚类算法关注于从样本特征的相似度方面将数据聚类。比如通过数据样本之间的欧式距离，曼哈顿距离的大小聚类等。而主题模型，顾名思义，就是对文字中隐含主题的一种建模方法。比如从“人民的名义”和“达康书记”这两个词我们很容易发现对应的文本有很大的主题相关度，但是如果通过词特征来聚类的话则很难找出，因为聚类方法不能考虑到到隐含的主题这一块。   那么如何找到隐含的主题呢？这个一个大问题。常用的方法一般都是基于统计学的生成方法。即假设以一定的概率选择了一个主题，然后以一定的概率选择当前主题的词。最后这些词组成了我们当前的文本。所有词的统计概率分布可以从语料库获得，具体如何以“一定的概率选择”，这就是各种具体的主题模型算法的任务了。   当然还有一些不是基于统计的方法，比如我们下面讲到的LSI。   潜在语义索引(LSI)概述   潜在语义索引(Latent Semantic Indexing,以下简称LSI)，有的文章也叫Latent Semantic Analysis（LSA）。其实是一个东西，后面我们统称LSI，它是一种简单实用的主题模型。LSI是基于奇异值分解（SVD）的方法来得到文本的主题的。而SVD及其应用我们在前面的文章也多次讲到，比如：奇异值分解(SVD)原理与在降维中的应用和矩阵分解在协同过滤推荐算法中的应用。如果大家对SVD还不熟悉，建议复习奇异值分解(SVD)原理与在降维中的应用后再读下面的内容。   这里我们简要回顾下SVD：对于一个$m \\times n$的矩阵$A$，可以分解为下面三个矩阵：     有时为了降低矩阵的维度到k，SVD的分解可以近似的写为：     如果把上式用到我们的主题模型，则SVD可以这样解释：我们输入的有m个文本，每个文本有n个词。而$A_{ij}$则对应第i个文本的第j个词的特征值，这里最常用的是基于预处理后的标准化TF-IDF值。k是我们假设的主题数，一般要比文本数少。SVD分解后，$U_{il}$对应第i个文本和第l个主题的相关度。$V_{jm}$对应第j个词和第m个词义的相关度。$\\Sigma_{lm}$对应第l个主题和第m个词义的相关度。   也可以反过来解释：我们输入的有m个词，对应n个文本。而$A_{ij}$则对应第i个词档的第j个文本的特征值，这里最常用的是基于预处理后的标准化TF-IDF值。k是我们假设的主题数，一般要比文本数少。SVD分解后，$U_{il}$对应第i个词和第l个词义的相关度。$V_{jm}$对应第j个文本和第m个主题的相关度。$\\Sigma_{lm}$对应第l个词义和第m个主题的相关度。   这样我们通过一次SVD，就可以得到文档和主题的相关度，词和词义的相关度以及词义和主题的相关度。   LSI简单实例   这里举一个简单的LSI实例，假设我们有下面这个有11个词三个文本的词频TF对应矩阵如下：      这里我们没有使用预处理，也没有使用TF-IDF，在实际应用中最好使用预处理后的TF-IDF值矩阵作为输入。   我们假定对应的主题数为2，则通过SVD降维后得到的三矩阵为：      从矩阵$U_k$我们可以看到词和词义之间的相关性。而从$V_k$可以看到3个文本和两个主题的相关性。大家可以看到里面有负数，所以这样得到的相关度比较难解释。   LSI用于文本相似度计算   在上面我们通过LSI得到的文本主题矩阵可以用于文本相似度计算。而计算方法一般是通过余弦相似度。比如对于上面的三文档两主题的例子。我们可以计算第一个文本和第二个文本的余弦相似度如下 ：   LSI主题模型总结   LSI是最早出现的主题模型了，它的算法原理很简单，一次奇异值分解就可以得到主题模型，同时解决词义的问题，非常漂亮。但是LSI有很多不足，导致它在当前实际的主题模型中已基本不再使用。   主要的问题有：   1） SVD计算非常的耗时，尤其是我们的文本处理，词和文本数都是非常大的，对于这样的高维度矩阵做奇异值分解是非常难的。   2） 主题值的选取对结果的影响非常大，很难选择合适的k值。   3） LSI得到的不是一个概率模型，缺乏统计基础，结果难以直观的解释。   对于问题1），主题模型非负矩阵分解（NMF）可以解决矩阵分解的速度问题。对于问题2），这是老大难了，大部分主题模型的主题的个数选取一般都是凭经验的，较新的层次狄利克雷过程（HDP）可以自动选择主题个数。对于问题3），牛人们整出了pLSI(也叫pLSA)和隐含狄利克雷分布(LDA)这类基于概率分布的主题模型来替代基于矩阵分解的主题模型。   回到LSI本身，对于一些规模较小的问题，如果想快速粗粒度的找出一些主题分布的关系，则LSI是比较好的一个选择，其他时候，如果你需要使用主题模型，推荐使用LDA和HDP。   参考文献   文本主题模型之潜在语义索引(LSI)  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%E4%B9%8B%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E7%B4%A2%E5%BC%95(LSI).html",
        "teaser":null},{
        "title": "文本挖掘预处理之TF-IDF",
        
        "excerpt":
            "title: algorithms-文本挖掘预处理之TF-IDF author: niult date: 2019-01-16 category: 文本挖掘 tags: python,numpy,文本挖掘 在文本挖掘预处理之向量化与Hash Trick中我们讲到在文本挖掘的预处理中，向量化之后一般都伴随着TF-IDF的处理，那么什么是TF-IDF，为什么一般我们要加这一步预处理呢？这里就对TF-IDF的原理做一个总结。 文本向量化特征的不足 在将文本分词并向量化后，我们可以得到词汇表中每个词在各个文本中形成的词向量，比如在文本挖掘预处理之向量化与Hash Trick这篇文章中，我们将下面4个短文本做了词频统计： corpus=[\"I come to China to travel\", \"This is a car polupar in China\", \"I love tea and Apple \", \"The work is to write some papers in science\"] 不考虑停用词，处理后得到的词向量如下： [[0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 1 0 0] [0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0] [1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E9%A2%84%E5%A4%84%E7%90%86%E4%B9%8BTF-IDF.html",
        "teaser":null},{
        "title": "隐马尔科夫模型HMM",
        
        "excerpt":
            "title: algorithms-隐马尔科夫模型HMM author: niult date: 2019-01-16 category: 文本挖掘 tags: python,numpy,文本挖掘 HMM模型基础 隐马尔科夫模型（Hidden Markov Model，以下简称HMM）是比较经典的机器学习模型了，它在语言识别，自然语言处理，模式识别等领域得到广泛的应用。当然，随着目前深度学习的崛起，尤其是RNN，LSTM等神经网络序列模型的火热，HMM的地位有所下降。但是作为一个经典的模型，学习HMM的模型和对应算法，对我们解决问题建模的能力提高以及算法思路的拓展还是很好的。本文是HMM系列的第一篇，关注于HMM模型的基础。 什么样的问题需要HMM模型 首先我们来看看什么样的问题解决可以用HMM模型。使用HMM模型时我们的问题一般有这两个特征：１）我们的问题是基于序列的，比如时间序列，或者状态序列。２）我们的问题中有两类数据，一类序列数据是可以观测到的，即观测序列；而另一类数据是不能观察到的，即隐藏状态序列，简称状态序列。 有了这两个特征，那么这个问题一般可以用HMM模型来尝试解决。这样的问题在实际生活中是很多的。比如：我现在在打字写博客，我在键盘上敲出来的一系列字符就是观测序列，而我实际想写的一段话就是隐藏序列，输入法的任务就是从敲入的一系列字符尽可能的猜测我要写的一段话，并把最可能的词语放在最前面让我选择，这就可以看做一个HMM模型了。再举一个，我在和你说话，我发出的一串连续的声音就是观测序列，而我实际要表达的一段话就是状态序列，你大脑的任务，就是从这一串连续的声音中判断出我最可能要表达的话的内容。 从这些例子中，我们可以发现，HMM模型可以无处不在。但是上面的描述还不精确，下面我们用精确的数学符号来表述我们的HMM模型。 HMM模型的定义 对于HMM模型，首先我们假设$Q$是所有可能的隐藏状态的集合，$V$是所有可能的观测状态的集合，即： 其中，$N$是可能的隐藏状态数，$M$是所有的可能的观察状态数。 对于一个长度为$T$的序列，$I$对应的状态序列, $O$是对应的观察序列，即： 其中，任意一个隐藏状态$i_t \\in Q$,任意一个观察状态$o_t \\in V$ HMM模型做了两个很重要的假设如下： 1） 齐次马尔科夫链假设。即任意时刻的隐藏状态只依赖于它前一个隐藏状态，这个我们在MCMC(二)马尔科夫链中有详细讲述。当然这样假设有点极端，因为很多时候我们的某一个隐藏状态不仅仅只依赖于前一个隐藏状态，可能是前两个或者是前三个。但是这样假设的好处就是模型简单，便于求解。如果在时刻$t$的隐藏状态是$i_t= q_i$,在时刻$t+1$的隐藏状态是$i_{t+1} = q_j$, 则从时刻$t$到时刻$t+1$的HMM状态转移概率$a_{ij}$可以表示为：$$a_{ij} = P(i_{t+1} = q_j i_t= q_i)$$ 这样$a_{ij}$可以组成马尔科夫链的状态转移矩阵$A$: 2） 观测独立性假设。即任意时刻的观察状态只仅仅依赖于当前时刻的隐藏状态，这也是一个为了简化模型的假设。如果在时刻$t$的隐藏状态是$i_t= q_j$, 而对应的观察状态为$o_t = v_k$, 则该时刻观察状态$v_k$在隐藏状态$q_j$下生成的概率为$b_j(k)$,满足：$$b_j(k) = P(o_t = v_k i_t= q_j)$$ 这样$b_j(k) $可以组成观测状态生成的概率矩阵$B$: 除此之外，我们需要一组在时刻$t=1$的隐藏状态概率分布$\\Pi$: 一个HMM模型，可以由隐藏状态初始概率分布$\\Pi$, 状态转移概率矩阵$A$和观测状态概率矩阵$B$决定。$\\Pi,A$决定状态序列，$B$决定观测序列。因此，HMM模型可以由一个三元组$\\lambda$表示如下： 一个HMM模型实例 下面我们用一个简单的实例来描述上面抽象出的HMM模型。这是一个盒子与球的模型，例子来源于李航的《统计学习方法》。 假设我们有3个盒子，每个盒子里都有红色和白色两种球，这三个盒子里球的数量分别是： 盒子123红球数547白球数563 按照下面的方法从盒子里抽球，开始的时候，从第一个盒子抽球的概率是0.2，从第二个盒子抽球的概率是0.4，从第三个盒子抽球的概率是0.4。以这个概率抽一次球后，将球放回。然后从当前盒子转移到下一个盒子进行抽球。规则是：如果当前抽球的盒子是第一个盒子，则以0.5的概率仍然留在第一个盒子继续抽球，以0.2的概率去第二个盒子抽球，以0.3的概率去第三个盒子抽球。如果当前抽球的盒子是第二个盒子，则以0.5的概率仍然留在第二个盒子继续抽球，以0.3的概率去第一个盒子抽球，以0.2的概率去第三个盒子抽球。如果当前抽球的盒子是第三个盒子，则以0.5的概率仍然留在第三个盒子继续抽球，以0.2的概率去第一个盒子抽球，以0.3的概率去第二个盒子抽球。如此下去，直到重复三次，得到一个球的颜色的观测序列: 注意在这个过程中，观察者只能看到球的颜色序列，却不能看到球是从哪个盒子里取出的。 那么按照我们上一节HMM模型的定义，我们的观察集合是: 我们的状态集合是： 而观察序列和状态序列的长度为3. 初始状态分布为： 状态转移概率分布矩阵为： 观测状态概率矩阵为： HMM观测序列的生成 从上一节的例子，我们也可以抽象出HMM观测序列生成的过程。 输入的是HMM的模型$\\lambda = (A, B, \\Pi)$,观测序列的长度$T$ 输出是观测序列$O ={o_1,o_2,…o_T}$ 生成的过程如下： 1）根据初始状态概率分布$\\Pi$生成隐藏状态$i_1$ 2) for t from 1 to T a. 按照隐藏状态$i_t$的观测状态分布$b_{i_t}(k)$生成观察状态$o_t$ b. 按照隐藏状态$i_t$的状态转移概率分布$a_{i_t\\;\\;i_{t+1}}$产生隐藏状态$i_{t+1}$ 所有的$o_t$一起形成观测序列$O ={o_1,o_2,…o_T}$ HMM模型的三个基本问题 HMM模型一共有三个经典的问题需要解决： 1） 评估观察序列概率。即给定模型$\\lambda...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8BHMM.html",
        "teaser":null},{
        "title": "LSTM",
        
        "excerpt":
            "前言   前文提到，由于梯度消失/梯度爆炸的问题传统RNN在实际中很难处理长期依赖，而LSTM（Long Short Term Memory）则绕开了这些问题依然可以从语料中学习到长期依赖关系。比如“I grew up in France… I speak fluent (French)”要预测()中应该填哪个词时，跟很久之前的”France”有密切关系。   LSTM   传统RNN每一步的隐藏单元只是执行一个简单的tanh或ReLU操作。      LSTM每个循环的模块内又有4层结构:3个sigmoid层，1个tanh层      LSTM每个模块的4层结构后文会详细说明，先来解释一下基本的图标。      粉色的圆圈表示一个二目运算。  两个箭头汇合成一个箭头表示2个向量首尾相连拼接在一起。  一个箭头分叉成2个箭头表示一个数据被复制成2份，分发到不同的地方去。   LSTM内部结构详解   LSTM的关键是细胞状态$C$，一条水平线贯穿于图形的上方，这条线上只有些少量的线性操作，信息在上面流传很容易保持。      第一层是个忘记层，决定细胞状态中丢弃什么信息。把$h_{t-1}$和$x_t$拼接起来，传给一个sigmoid函数，该函数输出0到1之间的值，这个值乘到细胞状态$C_{t-1}$上去。sigmoid函数的输出值直接决定了状态信息保留多少。比如当我们要预测下一个词是什么时，细胞状态可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语。      上一步的细胞状态$C_{t-1}$已经被忘记了一部分，接下来本步应该把哪些信息新加到细胞状态中呢？这里又包含2层：一个tanh层用来产生更新值的候选项$\\tilde{C}_t$，tanh的输出在[-1,1]上，说明细胞状态在某些维度上需要加强，在某些维度上需要减弱；还有一个sigmoid层（输入门层），它的输出值要乘到tanh层的输出上，起到一个缩放的作用，极端情况下sigmoid输出0说明相应维度上的细胞状态不需要更新。在那个预测下一个词的例子中，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语。      现在可以让旧的细胞状态$C_{t-1}$与$f_t$（f是forget忘记门的意思）相乘来丢弃一部分信息，然后再加个需要更新的部分$i_t * \\tilde{C}_t$（i是input输入门的意思），这就生成了新的细胞状态$C_t$。      最后该决定输出什么了。输出值跟细胞状态有关，把$C_t$输给一个tanh函数得到输出值的候选项。候选项中的哪些部分最终会被输出由一个sigmoid层来决定。在那个预测下一个词的例子中，如果细胞状态告诉我们当前代词是第三人称，那我们就可以预测下一词可能是一个第三人称的动词。      GRU  GRU（Gated Recurrent Unit）是LSTM最流行的一个变体，比LSTM模型要简单。      LSTM的变体  https://www.cnblogs.com/baiting/p/7560316.html   前向计算   参考文献   前面描述的开关是怎样在算法中实现的呢？这就用到了门（gate）的概念。门实际上就是一层全连接层，它的输入是一个向量，输出是一个0到1之间的实数向量。假设W是门的权重向量，是偏置项，那么门可以表示为：     门的使用，就是用门的输出向量按元素乘以我们需要控制的那个向量。因为门的输出是0到1之间的实数向量，那么，当门输出为0时，任何向量与之相乘都会得到0向量，这就相当于啥都不能通过；输出为1时，任何向量与之相乘都不会有任何改变，这就相当于啥都可以通过。因为$\\sigma$（也就是sigmoid函数）的值域是(0,1)，所以门的状态都是半开半闭的。   LSTM用两个门来控制单元状态c的内容，一个是遗忘门（forget gate），它决定了上一时刻的单元状态有多少保留到当前时刻；另一个是输入门（input gate），它决定了当前时刻网络的输入有多少保存到单元状态。LSTM用输出门（output gate）来控制单元状态有多少输出到LSTM的当前输出值。   我们先来看一下遗忘门：   式 上式中，是遗忘门的权重矩阵，表示把两个向量连接成一个更长的向量，是遗忘门的偏置项，是sigmoid函数。如果输入的维度是，隐藏层的维度是，单元状态的维度是（通常），则遗忘门的权重矩阵维度是。事实上，权重矩阵都是两个矩阵拼接而成的：一个是，它对应着输入项，其维度为；一个是，它对应着输入项，其维度为。可以写为：   下图显示了遗忘门的计算：                     遗忘门 $\\mathbf{f_t}$   输入门 $\\mathbf{i_t}$   单元状态 $\\mathbf{\\tilde{c}}_t$   单元状态 $\\mathbf{c}_t$   $\\sigma$ 产生权重，$\\tanh$ 输出处理   单元状态$\\mathbf{c}t$，由前一时刻状态$\\mathbf{c}{c-1}$和现在时刻状态$\\mathbf{\\tilde{c}}_t$加权得到，因此需要产出当前状态$\\mathbf{\\tilde{c}}_t$和两个权重系数 $\\mathbf{f_t}$ 和 $\\mathbf{i_t}$  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/LSTM.html",
        "teaser":null},{
        "title": "RNN",
        
        "excerpt":
            "前言   主要参考   本文部分参考和摘录了以下文章，在此由衷感谢以下作者的分享！   参考文献  参考文献  参考文献   参考文献   参考文献   参考文献   参考文献   参考文献   简介  RNN（Recurrent Neural Network）是一类用于处理序列数据的神经网络。首先我们要明确什么是序列数据，摘取百度百科词条：时间序列数据是指在不同时间点上收集到的数据，这类数据反映了某一事物、现象等随时间的变化状态或程度。这是时间序列数据的定义，当然这里也可以不是时间，比如文字序列，但总归序列数据有一个特点——后面的数据跟前面的数据有关系。   DNN和CNN中，训练样本的输入和输出是比较的确定的。但是有一类问题DNN和CNN不好解决，就是训练样本输入是连续的序列,且序列的长短不一，比如基于时间的序列：一段段连续的语音，一段段连续的手写文字。这些序列比较长，且长度不一，比较难直接的拆分成一个个独立的样本来通过DNN/CNN进行训练。   而对于这类问题，RNN则比较的擅长。那么RNN是怎么做到的呢？RNN假设我们的样本是基于序列的。比如是从序列索引1到序列索引$\\tau$的。对于这其中的任意序列索引号$t$,它对应的输入是对应的样本序列中的$x^{(t)}$。而模型在序列索引号$t$位置的隐藏状态$h^{(t)}$，则由$x^{(t)}$和在$t-1$位置的隐藏状态$h^{(t-1)}$共同决定。在任意序列索引号$t$，我们也有对应的模型预测输出$o^{(t)}$。通过预测输出$o^{(t)}$和训练序列真实输出$y^{(t)}$,以及损失函数$L^{(t)}$，我们就可以用DNN类似的方法来训练模型，接着用来预测测试序列中的一些位置的输出。   RNN模型   RNN模型有比较多的变种，这里介绍最主流的RNN模型结构如下：      上图中左边是RNN模型没有按时间展开的图，如果按时间序列展开，则是上图中的右边部分。我们重点观察右边部分的图。   这幅图描述了在序列索引号$t$附近RNN的模型。其中：   1）$x^{(t)}$代表在序列索引号$t$时训练样本的输入。同样的，$x^{(t-1)}$和$x^{(t+1)}$代表在序列索引号$t-1$和$t+1$时训练样本的输入。   2）$h^{(t)}$代表在序列索引号$t$时模型的隐藏状态。$h^{(t)}$由$x^{(t)}$和$h^{(t-1)}$共同决定。   3）$o^{(t)}$代表在序列索引号$t$时模型的输出。$o^{(t)}$只由模型当前的隐藏状态$h^{(t)}$决定。   4）$L^{(t)}$代表在序列索引号$t$时模型的损失函数。   5）$y^{(t)}$代表在序列索引号$t$时训练样本序列的真实输出。   6）$U,W,V$这三个矩阵是我们的模型的线性关系参数，它在整个RNN网络中是共享的，这点和DNN很不相同。 也正因为是共享了，它体现了RNN的模型的“循环反馈”的思想。　　   RNN前向传播算法   有了上面的模型，RNN的前向传播算法就很容易得到了。   对于任意一个序列索引号$t$，我们隐藏状态$h^{(t)}$由$x^{(t)}$和$h^{(t-1)}$得到：   其中$\\sigma$为RNN的激活函数，一般为$tanh$, $b$为线性关系的偏倚。   序列索引号$t$时模型的输出$o^{(t)}$的表达式比较简单：   在最终在序列索引号$t$时我们的预测输出为:   通常由于RNN是识别类的分类模型，所以上面这个激活函数一般是softmax。   通过损失函数$L^{(t)}$，比如对数似然损失函数，我们可以量化模型在当前位置的损失，即$\\hat{y}^{(t)}$和$y^{(t)}$的差距。     RNN反向传播算法推导   有了RNN前向传播算法的基础，就容易推导出RNN反向传播算法的流程了。RNN反向传播算法的思路和DNN是一样的，即通过梯度下降法一轮轮的迭代，得到合适的RNN模型参数$U,W,V,b,c$。由于我们是基于时间反向传播，所以RNN的反向传播有时也叫做BPTT(back-propagation through time)。当然这里的BPTT和DNN也有很大的不同点，即这里所有的$U,W,V,b,c$在序列的各个位置是共享的，反向传播时我们更新的是相同的参数。   为了简化描述，这里的损失函数我们为对数损失函数，输出的激活函数为softmax函数，隐藏层的激活函数为tanh函数。   对于RNN，由于我们在序列的每个位置都有损失函数，因此最终的损失$L$为：   其中$V,c,$的梯度计算是比较简单的：      但是$W,U,b$的梯度计算就比较的复杂了。从RNN的模型可以看出，在反向传播时，在在某一序列位置t的梯度损失由当前位置的输出对应的梯度损失和序列索引位置$t+1$时的梯度损失两部分共同决定。对于$W$在某一序列位置t的梯度损失需要反向传播一步步的计算。我们定义序列索引$t$位置的隐藏状态的梯度为：   这样我们可以像DNN一样从$\\delta^{(t+1)} $递推$\\delta^{(t)}$ 。     对于$\\delta^{(\\tau)} $，由于它的后面没有其他的序列索引了，因此有：     有了$\\delta^{(t)} $,计算$W,U,b$就容易了，这里给出$W,U,b$的梯度计算表达式：      除了梯度表达式不同，RNN的反向传播算法和DNN区别不大，因此这里就不再重复总结了。   RNN的变体1  以上是RNN的标准结构，然而在实际中这一种结构并不能解决所有问题，例如我们输入为一串文字，输出为分类类别，那么输出就不需要一个序列，只需要单个输出。如图。      同样的，我们有时候还需要单输入但是输出为序列的情况。那么就可以使用如下结构：      还有一种结构是输入虽是序列，但不随着序列变化，就可以使用如下结构：      原始的N vs N RNN要求序列等长，然而我们遇到的大部分问题序列都是不等长的，如机器翻译中，源语言和目标语言的句子往往并没有相同的长度。  下面我们来介绍RNN最重要的一个变种：N vs M。这种结构又叫Encoder-Decoder模型，也可以称之为Seq2Seq模型。      从名字就能看出，这个结构的原理是先编码后解码。左侧的RNN用来编码得到c，拿到c后再用右侧的RNN进行解码。得到c有多种方式，最简单的方法就是把Encoder的最后一个隐状态赋值给c，还可以对最后的隐状态做一个变换得到c，也可以对所有的隐状态做变换。      参考文献   RNN的变体2   双向RNN（Bidirectional RNNs网络结构）   双向RNN认为$o_t$不仅依赖于序列之前的元素，也跟$t$之后的元素有关，这在序列挖掘中也是很常见的事实。      深层双向RNN  在双向RNN的基础上，每一步由原来的一个隐藏层变成了多个隐藏层。     ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/RNN.html",
        "teaser":null},{
        "title": "Seq2Seq",
        
        "excerpt":
            "编解码（Encoder-Decoder）   前言   参考漫谈四种神经网络序列解码模型【附示例代码】   编码 （Encoder）      解码（Decoder）               Seq2Seq   参考文献  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Seq2Seq.html",
        "teaser":null},{
        "title": "TensorFlow-字符串处理",
        
        "excerpt":
            "#先导入TensorFlow from __future__ import absolute_import from __future__ import division from __future__ import nested_scopes from __future__ import print_function import logging import math import tensorflow as tf batch_size = 100 max_step = 10000 max_window_size = 1000 embedding_size = 128 # 商品的embedding size user_feature_size = 5 # 用户的特征size # Parameters learning_rate = 0.01 training_epochs = 1 display_step = 1 # Network Parameters n_hidden_0 = embedding_size + user_feature_size # 输入层单元个数 n_hidden_1 = 128 # 第一层隐层单元个数 n_hidden_2 = 128 # 第二层隐层单元个数 item_dict = {} # 商品的映射 item_list = [] # 商品的映射 user_dict = {} # 用户的映射 user_list = [] # 用户的映射 item_size = 0 #...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/TensorFlow-%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%A4%84%E7%90%86.html",
        "teaser":null},{
        "title": "TensorFlow常见代码块",
        
        "excerpt":
            "Batch Normalization 正则化 tf.nn.moments，矩 mean, variance = tf.nn.moments(input) mean: $\\mu$ 一阶矩，也就是均值 variance: $\\sigma^2$ 二阶矩， 也就是方差 tf.nn.batch_normalization，正则化 norm_x = tf.nn.batch_normalization(x, batch_mean, batch_var) 正则化方法和实例 import tensorflow as tf def batch_norm1(x): epsilon = 1e-8 batch_mean, batch_var = tf.nn.moments(x, [0]) return tf.nn.batch_normalization(x, batch_mean, batch_var, offset=None, scale=None,variance_epsilon=epsilon) def batch_norm2(inputs, epsilon=1e-8, scope=\"ln\", reuse=None): with tf.variable_scope(scope, reuse=reuse): inputs_shape = inputs.get_shape() params_shape = inputs_shape[-1:] mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True) beta = tf.Variable(tf.zeros(params_shape)) gamma = tf.Variable(tf.ones(params_shape)) normalized = (inputs - mean) / ((variance + epsilon) ** (.5)) outputs = gamma * normalized + beta return outputs shape = [128, 32, 32, 64] data = tf.Variable(tf.random_normal(shape)) norm1 = batch_norm1(data)...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/TensorFlow%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E5%9D%97.html",
        "teaser":null},{
        "title": "深度学习",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.html",
        "teaser":null},{
        "title": "tensorflow学习",
        
        "excerpt":
            "        #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time    : 2018/03/23 17:55 # @Author  : niuliangtao  import tensorflow as tf import numpy as np              ##           #定义‘符号’变量，也称为占位符  a = tf.placeholder(\"float\") b = tf.placeholder(\"float\")  y = tf.add(a, b) #构造一个op节点  sess = tf.Session()#建立会话 #运行会话，输入数据，并计算节点，同时打印结果 print sess.run(y, feed_dict={a: 3, b: 3}) # 任务完成, 关闭会话. sess.close()                         6.0                 ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/tensorflow%E5%AD%A6%E4%B9%A0.html",
        "teaser":null},{
        "title": "tf优化方法",
        
        "excerpt":
            "title: tf优化方法 author: niult date: 2019-01-15 category: tools tags: python,numpy,handbook 在很多机器学习和深度学习的应用中，我们发现用的最多的优化器是 Adam，为什么呢？ 下面是 TensorFlow 中的优化器， https://www.tensorflow.org/api_guides/python/train 在 keras 中也有 SGD，RMSprop，Adagrad，Adadelta，Adam 等： https://keras.io/optimizers/ 我们可以发现除了常见的梯度下降，还有 Adadelta，Adagrad，RMSProp 等几种优化器，都是什么呢，又该怎么选择呢？ 在 Sebastian Ruder 的这篇论文中给出了常用优化器的比较，今天来学习一下： https://arxiv.org/pdf/1609.04747.pdf 本文将梳理： 每个算法的梯度更新规则和缺点 为了应对这个不足而提出的下一个算法 超参数的一般设定值 几种算法的效果比较 选择哪种算法 1. 优化器算法简述? 首先来看一下梯度下降最常见的三种变形 BGD，SGD，MBGD，这三种形式的区别就是取决于我们用多少数据来计算目标函数的梯度，这样的话自然就涉及到一个 trade－off，即参数更新的准确率和运行时间。 1. Batch gradient descent 梯度更新规则: BGD 采用整个训练集的数据来计算 cost function 对参数的梯度： 缺点: 由于这种方法是在一次更新中，就对整个数据集计算梯度，所以计算起来非常慢，遇到很大量的数据集也会非常棘手，而且不能投入新数据实时更新模型 for i in range(nb_epochs): params_grad = evaluate_gradient(loss_function, data, params) params = params - learning_rate * params_grad 我们会事先定义一个迭代次数 epoch，首先计算梯度向量 params_grad，然后沿着梯度的方向更新参数 params，learning rate 决定了我们每一步迈多大。 Batch gradient descent 对于凸函数可以收敛到全局极小值，对于非凸函数可以收敛到局部极小值。 2. Stochastic gradient descent 梯度更新规则: 和 BGD 的一次用所有数据计算梯度相比，SGD 每次更新时对每个样本进行梯度更新，对于很大的数据集来说，可能会有相似的样本，这样 BGD 在计算梯度时会出现冗余，而 SGD 一次只进行一次更新，就没有冗余，而且比较快，并且可以新增样本。 for i in range(nb_epochs): np.random.shuffle(data) for example in...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/tf%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95.html",
        "teaser":null},{
        "title": "tf函数",
        
        "excerpt":
            "tf.gather  类似于数组的索引，可以把向量中某些索引值提取出来，得到新的向量，适用于要提取的索引为不连续的情况。这个函数似乎只适合在一维的情况下使用。           import tensorflow as tf   a = tf.Variable([[1,2,3,4,5], [6,7,8,9,10], [11,12,13,14,15]]) index_a = tf.Variable([0,2])  b = tf.Variable([1,2,3,4,5,6,7,8,9,10]) index_b = tf.Variable([2,4,6,8])  with tf.Session() as sess:     sess.run(tf.global_variables_initializer())     print(sess.run(tf.gather(a, index_a)))     print(sess.run(tf.gather(b, index_b)))                         [[ 1  2  3  4  5]  [11 12 13 14 15]] [3 5 7 9]                 ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/tf%E5%87%BD%E6%95%B0.html",
        "teaser":null},{
        "title": "人工神经网络",
        
        "excerpt":
            "   ANN(人工神经网络)   #   ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html",
        "teaser":null},{
        "title": "AutoEncoder-自编码实践",
        
        "excerpt":
            "title: AutoEncoder-自编码实践 author: niult date: 2019-02-20 category: 神经网络 tags: python,numpy,algorithms Variational Auto-Encoder Example VAE Overview References: Auto-Encoding Variational Bayes The International Conference on Learning Representations (ICLR), Banff, 2014. D.P. Kingma, M. Welling Understanding the difficulty of training deep feedforward neural networks. X Glorot, Y Bengio. Aistats 9, 249-256 Other tutorials: Variational Auto Encoder Explained. Kevin Frans. MNIST Dataset Overview This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/AutoEncoder-%E8%87%AA%E7%BC%96%E7%A0%81%E5%AE%9E%E8%B7%B5.html",
        "teaser":null},{
        "title": "AutoEncoder-自编码理论",
        
        "excerpt":
            "   title: AutoEncoder-自编码理论   author: niult   date: 2019-02-20   category: 神经网络   tags: python,numpy,algorithms  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/AutoEncoder-%E8%87%AA%E7%BC%96%E7%A0%81%E7%90%86%E8%AE%BA.html",
        "teaser":null},{
        "title": "CNN-卷积神经网络理论",
        
        "excerpt":
            "title: CNN-卷积神经网络理论 author: niult date: 2019-01-16 category: 神经网络 tags: python,numpy,神经网络 CNN的基本结构 首先我们来看看CNN的基本结构。一个常见的CNN例子如下图： 图中是一个图形识别的CNN模型。可以看出最左边的船的图像就是我们的输入层，计算机理解为输入若干个矩阵，这点和DNN基本相同。 接着是卷积层（Convolution Layer）,这个是CNN特有的，我们后面专门来讲。卷积层的激活函数使用的是ReLU。我们在DNN中介绍过ReLU的激活函数，它其实很简单，就是$ReLU(x) = max(0,x)$。在卷积层后面是池化层(Pooling layer)，这个也是CNN特有的，我们后面也会专门来讲。需要注意的是，池化层没有激活函数。 卷积层+池化层的组合可以在隐藏层出现很多次，上图中出现两次。而实际上这个次数是根据模型的需要而来的。当然我们也可以灵活使用使用卷积层+卷积层，或者卷积层+卷积层+池化层的组合，这些在构建模型的时候没有限制。但是最常见的CNN都是若干卷积层+池化层的组合，如上图中的CNN结构。 在若干卷积层+池化层后面是全连接层（Fully Connected Layer, 简称FC），全连接层其实就是我们前面讲的DNN结构，只是输出层使用了Softmax激活函数来做图像识别的分类，这点我们在DNN中也有讲述。 从上面CNN的模型描述可以看出，CNN相对于DNN，比较特殊的是卷积层和池化层，如果我们熟悉DNN，只要把卷积层和池化层的原理搞清楚了，那么搞清楚CNN就容易很多了。 初识卷积 首先，我们去学习卷积层的模型原理，在学习卷积层的模型原理前，我们需要了解什么是卷积，以及CNN中的卷积是什么样子的。 大家学习数学时都有学过卷积的知识，微积分中卷积的表达式为： 离散形式是： 这个式子如果用矩阵表示可以为： 其中星号表示卷积。 如果是二维的卷积，则表示式为： 在CNN中，虽然我们也是说卷积，但是我们的卷积公式和严格意义数学中的定义稍有不同,比如对于二维的卷积，定义为： 这个式子虽然从数学上讲不是严格意义上的卷积，但是大牛们都这么叫了，那么我们也跟着这么叫了。后面讲的CNN的卷积都是指的上面的最后一个式子。 其中，我们叫W为我们的卷积核，而X则为我们的输入。如果X是一个二维输入的矩阵，而W也是一个二维的矩阵。但是如果X是多维张量，那么W也是一个多维的张量。 卷积层 有了卷积的基本知识，我们现在来看看CNN中的卷积，假如是对图像卷积，回想我们的上一节的卷积公式，其实就是对输入的图像的不同局部的矩阵和卷积核矩阵各个位置的元素相乘，然后相加得到。 举个例子如下，图中的输入是一个二维的3x4的矩阵，而卷积核是一个2x2的矩阵。这里我们假设卷积是一次移动一个像素来卷积的，那么首先我们对输入的左上角2x2局部和卷积核卷积，即各个位置的元素相乘再相加，得到的输出矩阵S的$S_{00}$的元素，值为$aw+bx+ey+fz$。接着我们将输入的局部向右平移一个像素，现在是(b,c,f,g)四个元素构成的矩阵和卷积核来卷积，这样我们得到了输出矩阵S的$S_{01}$的元素，同样的方法，我们可以得到输出矩阵S的$S_{02}，S_{10}，S_{11}， S_{12}$的元素。 最终我们得到卷积输出的矩阵为一个2x3的矩阵S。 再举一个动态的卷积过程的例子如下： 我们有下面这个绿色的5x5输入矩阵，卷积核是一个下面这个黄色的3x3的矩阵。卷积的步幅是一个像素。则卷积的过程如下面的动图。卷积的结果是一个3x3的矩阵。 上面举的例子都是二维的输入，卷积的过程比较简单，那么如果输入是多维的呢？比如在前面一组卷积层+池化层的输出是3个矩阵，这3个矩阵作为输入呢，那么我们怎么去卷积呢？又比如输入的是对应RGB的彩色图像，即是三个分布对应R，G和B的矩阵呢？ 在斯坦福大学的cs231n的课程上，有一个动态的例子，链接在这。建议大家对照着例子中的动图看下面的讲解。 大家打开这个例子可以看到，这里面输入是3个7x7的矩阵。实际上原输入是3个5x5的矩阵。只是在原来的输入周围加上了1的padding，即将周围都填充一圈的0，变成了3个7x7的矩阵。 例子里面使用了两个卷积核，我们先关注于卷积核W0。和上面的例子相比，由于输入是3个7x7的矩阵，或者说是7x7x3的张量，则我们对应的卷积核W0也必须最后一维是3的张量，这里卷积核W0的单个子矩阵维度为3x3。那么卷积核W0实际上是一个3x3x3的张量。同时和上面的例子比，这里的步幅为2，也就是每次卷积后会移动2个像素的位置。 最终的卷积过程和上面的2维矩阵类似，上面是矩阵的卷积，即两个矩阵对应位置的元素相乘后相加。这里是张量的卷积，即两个张量的3个子矩阵卷积后，再把卷积的结果相加后再加上偏倚b。 7x7x3的张量和3x3x3的卷积核张量W0卷积的结果是一个3x3的矩阵。由于我们有两个卷积核W0和W1，因此最后卷积的结果是两个3x3的矩阵。或者说卷积的结果是一个3x3x2的张量。 仔细回味下卷积的过程，输入是7x7x3的张量，卷积核是两个3x3x3的张量。卷积步幅为2，最后得到了输出是3x3x2的张量。如果把上面的卷积过程用数学公式表达出来就是： 其中，$n_in$为输入矩阵的个数，或者是张量的最后一维的维数。$X_k$代表第k个输入矩阵。$W_k$代表卷积核的第k个子卷积核矩阵。$s(i,j)$即卷积核$W$对应的输出矩阵的对应位置元素的值。 通过上面的例子，相信大家对CNN的卷积层的卷积过程有了一定的了解。 对于卷积后的输出，一般会通过ReLU激活函数，将输出的张量中的小于0的位置对应的元素值都变为0。 池化层 相比卷积层的复杂，池化层则要简单的多，所谓的池化，个人理解就是对输入张量的各个子矩阵进行压缩。假如是2x2的池化，那么就将子矩阵的每2x2个元素变成一个元素，如果是3x3的池化，那么就将子矩阵的每3x3个元素变成一个元素，这样输入矩阵的维度就变小了。 要想将输入子矩阵的每nxn个元素变成一个元素，那么需要一个池化标准。常见的池化标准有2个，MAX或者是Average。即取对应区域的最大值或者平均值作为池化后的元素值。 下面这个例子采用取最大值的池化方法。同时采用的是2x2的池化。步幅为2。 首先对红色2x2区域进行池化，由于此2x2区域的最大值为6.那么对应的池化输出位置的值为6，由于步幅为2，此时移动到绿色的位置去进行池化，输出的最大值为8.同样的方法，可以得到黄色区域和蓝色区域的输出值。最终，我们的输入4x4的矩阵在池化后变成了2x2的矩阵。进行了压缩。 模型结构小结 理解了CNN模型中的卷积层和池化层，就基本理解了CNN的基本原理，后面再去理解CNN模型的前向传播算法和反向传播算法就容易了。下一篇我们就来讨论CNN模型的前向传播算法。 前向传播算法 回顾CNN的结构 这里我们用一个彩色的汽车样本的图像识别再从感官上回顾下CNN的结构。图中的CONV即为卷积层，POOL即为池化层，而FC即为DNN全连接层，包括了我们上面最后的用Softmax激活函数的输出层。 从上图可以看出，要理顺CNN的前向传播算法，重点是输入层的前向传播，卷积层的前向传播以及池化层的前向传播。而DNN全连接层和用Softmax激活函数的输出层的前向传播算法我们在讲DNN时已经讲到了。 输入层前向传播到卷积层 输入层的前向传播是CNN前向传播算法的第一步。一般输入层对应的都是卷积层，因此我们标题是输入层前向传播到卷积层。 我们这里还是以图像识别为例。 先考虑最简单的，样本都是二维的黑白图片。这样输入层$X$就是一个矩阵，矩阵的值等于图片的各个像素位置的值。这时和卷积层相连的卷积核$W$就也是矩阵。 如果样本都是有RGB的彩色图片，这样输入$X$就是3个矩阵，即分别对应R，G和B的矩阵，或者说是一个张量。这时和卷积层相连的卷积核$W$就也是张量，对应的最后一维的维度为3.即每个卷积核都是3个子矩阵组成。 同样的方法，对于3D的彩色图片之类的样本，我们的输入$X$可以是4维，5维的张量，那么对应的卷积核$W$也是个高维的张量。 不管维度多高，对于我们的输入，前向传播的过程可以表示为： 其中，上标代表层数，星号代表卷积，而b代表我们的偏倚, $\\sigma$为激活函数，这里一般都是ReLU。 和DNN的前向传播比较一下，其实形式非常的像，只是我们这儿是张量的卷积，而不是矩阵的乘法。同时由于$W$是张量，那么同样的位置，$W$参数的个数就比DNN多很多了。 为了简化我们的描述，本文后面如果没有特殊说明，我们都默认输入是3维的张量，即用RBG可以表示的彩色图片。 这里需要我们自己定义的CNN模型参数是： 1） 一般我们的卷积核不止一个，比如有K个，那么我们输入层的输出，或者说第二层卷积层的对应的输入就K个。 2） 卷积核中每个子矩阵的的大小，一般我们都用子矩阵为方阵的卷积核，比如FxF的子矩阵。 3） 填充padding（以下简称P），我们卷积的时候，为了可以更好的识别边缘，一般都会在输入矩阵在周围加上若干圈的0再进行卷积，加多少圈则P为多少。 4） 步幅stride（以下简称S），即在卷积过程中每次移动的像素距离大小。 这些参数我们在上一篇都有讲述。 隐藏层前向传播到卷积层 现在我们再来看普通隐藏层前向传播到卷积层时的前向传播算法。 假设隐藏层的输出是M个矩阵对应的三维张量，则输出到卷积层的卷积核也是M个子矩阵对应的三维张量。这时表达式和输入层的很像，也是 其中，上标代表层数，星号代表卷积，而b代表我们的偏倚, $\\sigma$为激活函数，这里一般都是ReLU。 也可以写成M个子矩阵子矩阵卷积后对应位置相加的形式，即： 和上一节唯一的区别仅仅在于，这里的输入是隐藏层来的，而不是我们输入的原始图片样本形成的矩阵。 需要我们定义的CNN模型参数也和上一节一样，这里我们需要定义卷积核的个数K，卷积核子矩阵的维度F，填充大小P以及步幅S。 隐藏层前向传播到池化层 池化层的处理逻辑是比较简单的，我们的目的就是对输入的矩阵进行缩小概括。比如输入的若干矩阵是NxN维的，而我们的池化大小是kxk的区域，则输出的矩阵都是$\\frac{N}{k} \\times \\frac{N}{k}$维的。 这里需要需要我们定义的CNN模型参数是： 1）池化区域的大小k 2）池化的标准，一般是MAX或者Average。 隐藏层前向传播到全连接层 由于全连接层就是普通的DNN模型结构，因此我们可以直接使用DNN的前向传播算法逻辑，即：...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%90%86%E8%AE%BA.html",
        "teaser":null},{
        "title": "DNN-深度神经网络",
        
        "excerpt":
            "title: DNN-深度神经网络 author: niult date: 2019-01-16 category: 神经网络 tags: python,numpy,神经网络 简介 深度神经网络（Deep Neural Networks， 以下简称DNN）是深度学习的基础，而要理解DNN，首先我们要理解DNN模型，下面我们就对DNN的模型与前向传播算法做一个总结。 从感知机到神经网络 在感知机原理小结中，我们介绍过感知机的模型，它是一个有若干输入和一个输出的模型，如下图: 输出和输入之间学习到一个线性关系，得到中间输出结果： 接着是一个神经元激活函数: 从而得到我们想要的输出结果1或者-1。 这个模型只能用于二元分类，且无法学习比较复杂的非线性模型，因此在工业界无法使用。 而神经网络则在感知机的模型上做了扩展，总结下主要有三点： 1）加入了隐藏层，隐藏层可以有多层，增强模型的表达能力，如下图实例，当然增加了这么多隐藏层模型的复杂度也增加了好多。 2）输出层的神经元也可以不止一个输出，可以有多个输出，这样模型可以灵活的应用于分类回归，以及其他的机器学习领域比如降维和聚类等。多个神经元输出的输出层对应的一个实例如下图，输出层现在有4个神经元了。 3） 对激活函数做扩展，感知机的激活函数是$sign(z)$,虽然简单但是处理能力有限，因此神经网络中一般使用的其他的激活函数，比如我们在逻辑回归里面使用过的Sigmoid函数，即： 还有后来出现的tanx, softmax,和ReLU等。通过使用不同的激活函数，神经网络的表达能力进一步增强。对于各种常用的激活函数，我们在后面再专门讲。 DNN的基本结构 上一节我们了解了神经网络基于感知机的扩展，而DNN可以理解为有很多隐藏层的神经网络。这个很多其实也没有什么度量标准, 多层神经网络和深度神经网络DNN其实也是指的一个东西，当然，DNN有时也叫做多层感知机（Multi-Layer perceptron,MLP）, 名字实在是多。后面我们讲到的神经网络都默认为DNN。 从DNN按不同层的位置划分，DNN内部的神经网络层可以分为三类，输入层，隐藏层和输出层,如下图示例，一般来说第一层是输入层，最后一层是输出层，而中间的层数都是隐藏层。 层与层之间是全连接的，也就是说，第i层的任意一个神经元一定与第i+1层的任意一个神经元相连。虽然DNN看起来很复杂，但是从小的局部模型来说，还是和感知机一样，即一个线性关系$z=\\sum\\limits w_ix_i + b$加上一个激活函数$\\sigma(z)$。 由于DNN层数多，则我们的线性关系系数$w$和偏倚$b$的数量也就是很多了。具体的参数在DNN是如何定义的呢？ 首先我们来看看线性关系系数$w$的定义。以下图一个三层的DNN为例，第二层的第4个神经元到第三层的第2个神经元的线性系数定义为$w_{24}^3$。上标3代表线性系数$w$所在的层数，而下标对应的是输出的第三层索引2和输入的第二层索引4。你也许会问，为什么不是$w_{42}^3$, 而是$w_{24}^3$呢？这主要是为了便于模型用于矩阵表示运算，如果是$w_{42}^3$而每次进行矩阵运算是$w^Tx+b$，需要进行转置。将输出的索引放在前面的话，则线性运算不用转置,即直接为$wx+b$。总结下，第$l-1$层的第k个神经元到第$l$层的第j个神经元的线性系数定义为$w_{jk}^l$。注意，输入层是没有$w$参数的。 再来看看偏倚$b$的定义。还是以这个三层的DNN为例，第二层的第三个神经元对应的偏倚定义为$b_3^{2}$。其中，上标2代表所在的层数，下标3代表偏倚所在的神经元的索引。同样的道理，第三个的第一个神经元的偏倚应该表示为$b_1^{3}$。同样的，输入层是没有偏倚参数$b$的。 前向传播算法 数学原理 在上一节，我们已经介绍了DNN各层线性关系系数$w$,偏倚$b$的定义。假设我们选择的激活函数是$\\sigma(z)$，隐藏层和输出层的输出值为$a$，则对于下图的三层DNN,利用和感知机一样的思路，我们可以利用上一层的输出计算下一层的输出，也就是所谓的DNN前向传播算法。 对于第二层的的输出$a_1^2,a_2^2,a_3^2$，我们有： 对于第三层的的输出$a_1^3$，我们有： 将上面的例子一般化，假设第$l-1$层共有m个神经元，则对于第$l$层的第j个神经元的输出$a_j^l$，我们有： 其中，如果$l=2$,则对于的$a_k^1$即为输入层的$x_k$。 从上面可以看出，使用代数法一个个的表示输出比较复杂，而如果使用矩阵法则比较的简洁。假设第$l-1$层共有m个神经元，而第$l$层共有n个神经元，则第$l$层的线性系数$w$组成了一个$n \\times m$的矩阵$W^l$, 第$l$层的偏倚$b$组成了一个$n \\times 1$的向量$b^l$ , 第$l-1$层的的输出$a$组成了一个$m \\times 1$的向量$a^{l-1}$，第$l$层的的未激活前线性输出$z$组成了一个$n \\times 1$的向量$z^{l}$, 第$l$层的的输出$a$组成了一个$n \\times 1$的向量$a^{l}$。则用矩阵法表示，第l层的输出为： 这个表示方法简洁漂亮，后面我们的讨论都会基于上面的这个矩阵法表示来。 前向传播算法 有了上一节的数学推导，DNN的前向传播算法也就不难了。所谓的DNN的前向传播算法也就是利用我们的若干个权重系数矩阵$W$,偏倚向量$b$来和输入值向量$x$进行一系列线性运算和激活运算，从输入层开始，一层层的向后计算，一直到运算到输出层，得到输出结果为值。 输入: 总层数L，所有隐藏层和输出层对应的矩阵$W$,偏倚向量$b$，输入值向量$x$ 输出：输出层的输出$a^L$ 1） 初始化$a^1 = x $ 2) for $l = 2$ to $L$, 计算： 最后的结果即为输出$a^L$。 前向传播算法小结 单独看DNN前向传播算法，似乎没有什么大用处，而且这一大堆的矩阵$W$,偏倚向量$b$对应的参数怎么获得呢？怎么得到最优的矩阵$W$,偏倚向量$b$呢？这个我们在讲DNN的反向传播算法时再讲。而理解反向传播算法的前提就是理解DNN的模型与前向传播算法。这也是我们这一篇先讲的原因。 在深度神经网络（DNN）模型与前向传播算法中，我们对DNN的模型和前向传播算法做了总结，这里我们更进一步，对DNN的反向传播算法（Back Propagation，BP）做一个总结。 反向传播算法 在了解DNN的反向传播算法前，我们先要知道DNN反向传播算法要解决的问题，也就是说，什么时候我们需要这个反向传播算法？ 回到我们监督学习的一般问题，假设我们有m个训练样本：${(x_1,y_1), (x_2,y_2), …, (x_m,y_m)}$,其中$x$为输入向量，特征维度为$n_in$,而$y$为输出向量，特征维度为$n_out$。我们需要利用这m个样本训练出一个模型，当有一个新的测试样本$(x_{test},?)$来到时, 我们可以预测$y_{test}$向量的输出。 如果我们采用DNN的模型，即我们使输入层有$n_in$个神经元，而输出层有$n_out$个神经元。再加上一些含有若干神经元的隐藏层。此时我们需要找到合适的所有隐藏层和输出层对应的线性系数矩阵$W$,偏倚向量$b$,让所有的训练样本输入计算出的输出尽可能的等于或很接近样本输出。怎么找到合适的参数呢？ 如果大家对传统的机器学习的算法优化过程熟悉的话，这里就很容易联想到我们可以用一个合适的损失函数来度量训练样本的输出损失，接着对这个损失函数进行优化求最小化的极值，对应的一系列线性系数矩阵$W$,偏倚向量$b$即为我们的最终结果。在DNN中，损失函数优化极值求解的过程最常见的一般是通过梯度下降法来一步步迭代完成的，当然也可以是其他的迭代方法比如牛顿法与拟牛顿法。如果大家对梯度下降法不熟悉，建议先阅读我之前写的梯度下降（Gradient Descent）小结。 对DNN的损失函数用梯度下降法进行迭代优化求极小值的过程即为我们的反向传播算法。 反向传播算法的基本思路...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/DNN-%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html",
        "teaser":null},{
        "title": "LSTM-长短期记忆网络理论",
        
        "excerpt":
            "title: LSTM-长短期记忆网络理论 author: niult date: 2019-01-16 category: 神经网络 tags: python,numpy,algorithms 简介 由于RNN也有梯度消失的问题，因此很难处理长序列的数据，大牛们对RNN做了改进，得到了RNN的特例LSTM（Long Short-Term Memory），它可以避免常规RNN的梯度消失，因此在工业界得到了广泛的应用。下面我们就对LSTM模型做一个总结。 从RNN到LSTM 在RNN模型里，我们讲到了RNN具有如下的结构，每个序列索引位置t都有一个隐藏状态$h^{(t)}$。 如果我们略去每层都有的$o^{(t)}, L^{(t)}, y^{(t)}$，则RNN的模型可以简化成如下图的形式： 图中可以很清晰看出在隐藏状态$h^{(t)}$由$x^{(t)}$和$h^{(t-1)}$得到。得到$h^{(t)}$后一方面用于当前层的模型损失计算，另一方面用于计算下一层的$h^{(t+1)}$。 由于RNN梯度消失的问题，大牛们对于序列索引位置t的隐藏结构做了改进，可以说通过一些技巧让隐藏结构复杂了起来，来避免梯度消失的问题，这样的特殊RNN就是我们的LSTM。由于LSTM有很多的变种，这里我们以最常见的LSTM为例讲述。LSTM的结构如下图： 可以看到LSTM的结构要比RNN的复杂的多，真佩服牛人们怎么想出来这样的结构，然后这样居然就可以解决RNN梯度消失的问题？由于LSTM怎么可以解决梯度消失是一个比较难讲的问题，我也不是很熟悉，这里就不多说，重点回到LSTM的模型本身。 LSTM模型结构剖析 上面我们给出了LSTM的模型结构，下面我们就一点点的剖析LSTM模型在每个序列索引位置t时刻的内部结构。 从上图中可以看出，在每个序列索引位置t时刻向前传播的除了和RNN一样的隐藏状态$h^{(t)}$，还多了另一个隐藏状态，如图中上面的长横线。这个隐藏状态我们一般称为细胞状态(Cell State)，记为$C^{(t)}$。如下图所示： 除了细胞状态，LSTM图中还有了很多奇怪的结构，这些结构一般称之为门控结构(Gate)。LSTM在在每个序列索引位置t的门一般包括遗忘门，输入门和输出门三种。下面我们就来研究上图中LSTM的遗忘门，输入门和输出门以及细胞状态。 LSTM之遗忘门 遗忘门（forget gate）顾名思义，是控制是否遗忘的，在LSTM中即以一定的概率控制是否遗忘上一层的隐藏细胞状态。遗忘门子结构如下图所示： 图中输入的有上一序列的隐藏状态$h^{(t-1)}$和本序列数据$x^{(t)}$，通过一个激活函数，一般是sigmoid，得到遗忘门的输出$f^{(t)}$。由于sigmoid的输出$f^{(t)}$在[0,1]之间，因此这里的输出f^{(t)}代表了遗忘上一层隐藏细胞状态的概率。用数学表达式即为： 其中$W_f, U_f, b_f$为线性关系的系数和偏倚，和RNN中的类似。$\\sigma$为sigmoid激活函数。 LSTM之输入门 输入门（input gate）负责处理当前序列位置的输入，它的子结构如下图： 从图中可以看到输入门由两部分组成，第一部分使用了sigmoid激活函数，输出为$i^{(t)}$,第二部分使用了tanh激活函数，输出为$a^{(t)}$, 两者的结果后面会相乘再去更新细胞状态。用数学表达式即为： 其中$W_i, U_i, b_i, W_a, U_a, b_a,$为线性关系的系数和偏倚，和RNN中的类似。$\\sigma$为sigmoid激活函数。 LSTM之细胞状态更新 在研究LSTM输出门之前，我们要先看看LSTM之细胞状态。前面的遗忘门和输入门的结果都会作用于细胞状态$C^{(t)}$。我们来看看从细胞状态$C^{(t-1)}$如何得到$C^{(t)}$。如下图所示： 细胞状态$C^{(t)}$由两部分组成，第一部分是$C^{(t-1)}$和遗忘门输出$f^{(t)}$的乘积，第二部分是输入门的$i^{(t)}$和$a^{(t)}$的乘积，即： 其中，$\\odot$为Hadamard积，在DNN中也用到过。 LSTM之输出门 有了新的隐藏细胞状态$C^{(t)}$，我们就可以来看输出门了，子结构如下： 从图中可以看出，隐藏状态$h^{(t)}$的更新由两部分组成，第一部分是$o^{(t)}$, 它由上一序列的隐藏状态$h^{(t-1)}$和本序列数据$x^{(t)}$，以及激活函数sigmoid得到，第二部分由隐藏状态$C^{(t)}$和tanh激活函数组成, 即： 通过本节的剖析，相信大家对于LSTM的模型结构已经有了解了。当然，有些LSTM的结构和上面的LSTM图稍有不同，但是原理是完全一样的。 LSTM前向传播算法 现在我们来总结下LSTM前向传播算法。LSTM模型有两个隐藏状态$h^{(t)}, C^{(t)}$，模型参数几乎是RNN的4倍，因为现在多了$W_f, U_f, b_f, W_a, U_a, b_a, W_i, U_i, b_i, W_o, U_o, b_o$这些参数。 前向传播过程在每个序列索引位置的过程为： 1）更新遗忘门输出： 2）更新输入门两部分输出： 3）更新细胞状态： 4）更新输出门输出： 5）更新当前序列索引预测输出： LSTM反向传播算法推导关键点 有了LSTM前向传播算法，推导反向传播算法就很容易了， 思路和RNN的反向传播算法思路一致，也是通过梯度下降法迭代更新我们所有的参数，关键点在于计算所有参数基于损失函数的偏导数。 在RNN中，为了反向传播误差，我们通过隐藏状态$h^{(t)}$的梯度$\\delta^{(t)}$一步步向前传播。在LSTM这里也类似。只不过我们这里有两个隐藏状态$h^{(t)}$和$C^{(t)}$。这里我们定义两个$\\delta$，即： 为了便于推导，我们将损失函数$L(t)$分成两块，一块是时刻$t$位置的损失$l(t)$，另一块是时刻$t$之后损失$L(t+1)$，即： 而在最后的序列索引位置$\\tau$的$\\delta_h^{(\\tau)}$和 $\\delta_C^{(\\tau)} $为： 接着我们由$\\delta_C^{(t+1)},\\delta_h^{(t+1)}$反向推导$\\delta_h^{(t)}, \\delta_C^{(t)}$。 $\\delta_h^{(t)}$的梯度由本层t时刻的输出梯度误差和大于t时刻的误差两部分决定，即： 整个LSTM反向传播的难点就在于$ \\frac{\\partial h^{(t+1)}}{\\partial h^{(t)}}$这部分的计算。仔细观察，由于$h^{(t)} = o^{(t)} \\odot tanh(C^{(t)})$, 在第一项$o^{(t)}$中，包含一个$h$的递推关系，第二项$tanh(C^{(t)})$就复杂了，$tanh$函数里面又可以表示成： $tanh$函数的第一项中，$f^{(t)} $包含一个$h$的递推关系，在$tanh$函数的第二项中，$i^{(t)}$和$a^{(t)}$都包含$h$的递推关系，因此，最终$ \\frac{\\partial h^{(t+1)}}{\\partial h^{(t)}}$这部分的计算结果由四部分组成。即：...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/LSTM-%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C%E7%90%86%E8%AE%BA.html",
        "teaser":null},{
        "title": "PCA-主成分分析",
        
        "excerpt":
            "This is the notebook for the “Reducing Dimensionality from Dimensionality Reduction Techniques” Medium post: https://medium.com/@eliorcohen/reducing-dimensionality-from-dimensionality-reduction-techniques-f658aec24dfe CODE First lets import all the necessary libraries %matplotlib notebook import tensorflow as tf import math from sklearn import datasets from sklearn.manifold import TSNE import numpy as np import matplotlib.pyplot as plt from matplotlib import cm from mpl_toolkits.mplot3d import Axes3D import seaborn as sns Loading the iris dataset iris_dataset = datasets.load_iris() PCA %matplotlib notebook import tensorflow as tf import math from sklearn import datasets from sklearn.manifold import TSNE import numpy as np import matplotlib.pyplot as plt from matplotlib import cm from mpl_toolkits.mplot3d import Axes3D import...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/PCA-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90.html",
        "teaser":null},{
        "title": "RBM-受限玻尔兹曼机实践",
        
        "excerpt":
            "title: RBM-受限玻尔兹曼机实践 author: niult date: 2019-01-16 category: 神经网络 tags: python,numpy,algorithms 实例一 很多人讲RBM都要从能量函数讲起，由能量最低导出极小化目标函数（你听说过最常见的建立目标函数的方法可能是最小化平方误差或者最大化似然函数），然后用梯度下降法求解，得到网络参数。Introduction to Restricted Boltzmann Machines这篇博客没有遵循这种套路来讲RBM，它直接给RBM网络权重的训练方法，讲得浅显易懂，清新脱俗。本文只是对英文版的翻译。 在基于LFM(Latent Factor Model)的推荐算法一文中我们介绍了用因子分析法来做推荐。比如用户购买了《推荐系统实践》、《用Python做数据分析》，背后的隐藏因子是数据挖掘；用户购买了《一课经济学》、《郎咸平说》，背后的隐藏因子是经济学。因子分析法就是找到用户对各个隐藏因子的喜好程度$U=(uf_1,uf_2,…,uf_n)$，以及商品在各个隐藏因子上的概率分布$I=(if_1,if_2,…,if_n)$，然后两个向量做乘法即得到用户对商品的喜好程度。RBM可以理解为一种二值化的因子分析法（当然RBM还有其他的理解方式）。先来一张图看看RBM的网络结构。 它只有两层：可视层和隐藏层，两层之间是全连接。另有一个偏置单元，跟所有的可视单元和隐藏单元都有连接。可视层之间、隐藏层之间无连接。 所有连接都是双向的，并且是带权重的。 所有神经元的状态只有0和1两种。 RBM是一个随机网络，即所有神经元以一个概率值选择状态为0还是1。偏置单元是个例外，它总为1。偏置单元用来反应商品固有的受欢迎程度，所谓“固有”就是跟外界无关，反应到RBM网络里面就是隐藏单元的状态并不是完全由可视层决定的，也由隐藏层神经元自身固有的一些因素决定，这些固有的因素就由偏置单元来承载。反过来对于可视层也一样，可视层神经元固有的受欢迎程度由偏置单元来承载。我们在带偏置的LFM中也说明了偏置的作用。 RBM的运作方式 可视层的神经元用$x_i$表示，隐藏层神经元用$x_j$表示，它们之间的权重用$w_{ij}$表示，可视层神经元个数为$m$，隐藏层神经元个数为$n$。当给定可视层状态后，用下式更新隐藏层的状态。 \\begin{equation}net_j=\\sum_{i=0}^m{x_i{w_{ij}}}\\end{equation}$x_0$是偏置单元，总为1 \\begin{equation}prob(j)=sigmoid(net_j)=\\frac{1}{1+e^{-net_j}}\\end{equation} $x_j$以概率$prob(j)$取1，以概率$1-prob(j)$取0。 $sigmoid$函数关于(0,0.5)这一点中心对称，$x$为正时$sigmoid(x)&gt;0.5$，$x\\to\\infty$时$sigmoid(x)\\to{1}$。 根据隐藏层求可视层方式雷同，就不写公式了。 对于推荐系统来说，我们知道用户购买了哪些商品，将对应的可视层神经元置为1，其他置为0，求出隐藏层状态，由隐藏层再返回来求可视层状态，这个时候可视层哪些神经元为1我们就把相应有商品推荐给用户。 权重学习方法 训练RBM网络就是训练权重$w_{ij}$。首先随机初始化$w_{ij}$，然后每一次拿一个样本（即可视层是已知的）经历下面的步骤。 由可视层的$x_i$算出隐藏层的$x_j$，令$w_{ij}$的正向梯度为\\begin{equation}positive(w_{ij})=x_i{x_j}\\end{equation} 由隐藏层$x_j$再来反向计算$x’_i$，注意此时算出的$x’_i$跟原先的$x_i$已经不一样了，令$w_{ij}$的负向梯度为\\begin{equation}negative(w_{ij})=x’_i{x_j}\\end{equation} 更新权重\\begin{equation}w_{ij}=w_{ij}+\\alpha*(positive(w_{ij})-negative(w_{ij}))\\end{equation} 我们不去深究为什么正向梯度和负向梯度是这样一个公式。上述学习方式叫对比散度(contrastive divergence)法。 循环拿样本去训练网络，不停迭代，直到收敛（即$x_i’$和$x_i$很接近）。 实践中的优化 上面讲述中我们拿$x_i$去RBM网络中返回一次得到$x_i’$，然后就开始计算$negative(w_{ij})$，改进方法是多往返几次后再计算$negative(w_{ij})$。 计算$positive(w_{ij})$时用$prob(i)prob(j)$，而非$x_i{x_j}$。$negative(w_{ij})$同样。 加正则项，对较大的权重$w_{ij}$进行惩罚。 更新$w_{ij}$时加动量向，即本次前进的方向是本次的梯度与上次迭代中梯度的线性加权。 每次调整权重时使用一批样本，而非不一个样本。虽然计算结果是一样的，但由于numpy对矩阵乘法做了加速优化，比逐个计算向量乘法要快。 from __future__ import print_function import numpy as np class RBM: def __init__(self, num_visible, num_hidden): self.num_hidden = num_hidden self.num_visible = num_visible self.debug_print = True # Initialize a weight matrix, of dimensions (num_visible x num_hidden), using # a uniform distribution between -sqrt(6. / (num_hidden + num_visible)) # and sqrt(6. / (num_hidden + num_visible)). One could vary the...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/RBM-%E5%8F%97%E9%99%90%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E6%9C%BA%E5%AE%9E%E8%B7%B5.html",
        "teaser":null},{
        "title": "RBM-受限玻尔兹曼机理论",
        
        "excerpt":
            "title: RBM-受限玻尔兹曼机理论 author: niult date: 2019-01-16 category: 神经网络 tags: python,numpy,algorithms RBM模型 模型结构 玻尔兹曼机是一大类的神经网络模型，但是在实际应用中使用最多的则是RBM。RBM本身模型很简单，只是一个两层的神经网络，因此严格意义上不能算深度学习的范畴。不过深度玻尔兹曼机（Deep Boltzmann Machine，以下简称DBM）可以看做是RBM的推广。理解了RBM再去研究DBM就不难了，因此本文主要关注于RBM。 回到RBM的结构，它是一个个两层的神经网络，如下图所示： 上面一层神经元组成隐藏层(hidden layer), 用$h$向量隐藏层神经元的值。下面一层的神经元组成可见层(visible layer),用$v$向量表示可见层神经元的值。隐藏层和可见层之间是全连接的，这点和DNN类似, 隐藏层神经元之间是独立的，可见层神经元之间也是独立的。连接权重可以用矩阵$W$表示。和DNN的区别是，RBM不区分前向和反向，可见层的状态可以作用于隐藏层，而隐藏层的状态也可以作用于可见层。隐藏层的偏倚系数是向量$b$,而可见层的偏倚系数是向量$a$。 常用的RBM一般是二值的，即不管是隐藏层还是可见层，它们的神经元的取值只为0或者1。本文只讨论二值RBM。 总结下RBM模型结构的结构：主要是权重矩阵$W$, 偏倚系数向量$a$和$b$，隐藏层神经元状态向量$h$和可见层神经元状态向量$v$。 概率分布 RBM是基于基于能量的概率分布模型。怎么理解呢？分两部分理解，第一部分是能量函数，第二部分是基于能量函数的概率分布函数。 对于给定的状态向量$h$和$v$，则RBM当前的能量函数可以表示为： 有了能量函数，则我们可以定义RBM的状态为给定$v,h$的概率分布为： 其中$Z$为归一化因子，类似于softmax中的归一化因子，表达式为： 有了概率分布，我们现在来看条件分布$P(h|v)$: \\begin{align} P(h|v) &amp; = \\frac{P(h,v)}{P(v)} \\&amp; = \\frac{1}{P(v)}\\frac{1}{Z}exp{a^Tv + b^Th + h^TWv} \\&amp; = \\frac{1}{Z’}exp{b^Th + h^TWv} \\&amp; = \\frac{1}{Z’}exp{\\sum\\limits_{j=1}^{n_h}(b_j^Th_j + h_j^TW_{j,:}v)} \\&amp; = \\frac{1}{Z’} \\prod\\limits_{j=1}^{n_h}exp{b_j^Th_j + h_j^TW_{j,:}v} \\end{align} 其中$Z’$为新的归一化系数，表达式为： 同样的方式，我们也可以求出$P(v h)$,这里就不再列出了。 有了条件概率分布，现在我们来看看RBM的激活函数，提到神经网络，我们都绕不开激活函数，但是上面我们并没有提到。由于使用的是能量概率模型，RBM的基于条件分布的激活函数是很容易推导出来的。我们以$P(h_j=1 v)$为例推导如下。 \\begin{align} P(h_j =1|v) &amp; = \\frac{P(h_j =1|v)}{P(h_j =1|v) + P(h_j =0|v) } \\&amp; = \\frac{exp{b_j + W_{j,:}v}}{exp{0} + exp{b_j + W_{j,:}v}} \\&amp; = \\frac{1}{1+ exp{-(b_j + W_{j,:}v)}}\\&amp; = sigmoid(b_j + W_{j,:}v) \\end{align} 从上面可以看出， RBM里从可见层到隐藏层用的其实就是sigmoid激活函数。同样的方法，我们也可以得到隐藏层到可见层用的也是sigmoid激活函数。即：$$ P(v_j =1...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/RBM-%E5%8F%97%E9%99%90%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E6%9C%BA%E7%90%86%E8%AE%BA.html",
        "teaser":null},{
        "title": "RNN-递归神经网络理论",
        
        "excerpt":
            "   title: RNN-递归神经网络   author: niult   date: 2019-01-16   category: 神经网络   tags: python,numpy,algorithms   RNN概述   在前面讲到的DNN和CNN中，训练样本的输入和输出是比较的确定的。但是有一类问题DNN和CNN不好解决，就是训练样本输入是连续的序列,且序列的长短不一，比如基于时间的序列：一段段连续的语音，一段段连续的手写文字。这些序列比较长，且长度不一，比较难直接的拆分成一个个独立的样本来通过DNN/CNN进行训练。   而对于这类问题，RNN则比较的擅长。那么RNN是怎么做到的呢？RNN假设我们的样本是基于序列的。比如是从序列索引1到序列索引$\\tau$的。对于这其中的任意序列索引号$t$,它对应的输入是对应的样本序列中的$x^{(t)}$。而模型在序列索引号$t$位置的隐藏状态$h^{(t)}$，则由$x^{(t)}$和在$t-1$位置的隐藏状态$h^{(t-1)}$共同决定。在任意序列索引号$t$，我们也有对应的模型预测输出$o^{(t)}$。通过预测输出$o^{(t)}$和训练序列真实输出$y^{(t)}$,以及损失函数$L^{(t)}$，我们就可以用DNN类似的方法来训练模型，接着用来预测测试序列中的一些位置的输出。   下面我们来看看RNN的模型。   RNN模型   RNN模型有比较多的变种，这里介绍最主流的RNN模型结构如下：      上图中左边是RNN模型没有按时间展开的图，如果按时间序列展开，则是上图中的右边部分。我们重点观察右边部分的图。   这幅图描述了在序列索引号$t$附近RNN的模型。其中：   1）$x^{(t)}$代表在序列索引号$t$时训练样本的输入。同样的，$x^{(t-1)}$和$x^{(t+1)}$代表在序列索引号$t-1$和$t+1$时训练样本的输入。   2）$h^{(t)}$代表在序列索引号$t$时模型的隐藏状态。$h^{(t)}$由$x^{(t)}$和$h^{(t-1)}$共同决定。   3）$o^{(t)}$代表在序列索引号$t$时模型的输出。$o^{(t)}$只由模型当前的隐藏状态$h^{(t)}$决定。   4）$L^{(t)}$代表在序列索引号$t$时模型的损失函数。   5）$y^{(t)}$代表在序列索引号$t$时训练样本序列的真实输出。   6）$U,W,V$这三个矩阵是我们的模型的线性关系参数，它在整个RNN网络中是共享的，这点和DNN很不相同。 也正因为是共享了，它体现了RNN的模型的“循环反馈”的思想。   前向传播算法   有了上面的模型，RNN的前向传播算法就很容易得到了。   对于任意一个序列索引号$t$，我们隐藏状态$h^{(t)}$由$x^{(t)}$和$h^{(t-1)}$得到：   其中$\\sigma$为RNN的激活函数，一般为$tanh$, $b$为线性关系的偏倚。   序列索引号$t$时模型的输出$o^{(t)}$的表达式比较简单：   在最终在序列索引号$t$时我们的预测输出为:   通常由于RNN是识别类的分类模型，所以上面这个激活函数一般是softmax。   通过损失函数$L^{(t)}$，比如对数似然损失函数，我们可以量化模型在当前位置的损失，即$\\hat{y}^{(t)}$和$y^{(t)}$的差距。   反向传播算法推导   有了RNN前向传播算法的基础，就容易推导出RNN反向传播算法的流程了。RNN反向传播算法的思路和DNN是一样的，即通过梯度下降法一轮轮的迭代，得到合适的RNN模型参数$U,W,V,b,c$。由于我们是基于时间反向传播，所以RNN的反向传播有时也叫做BPTT(back-propagation through time)。当然这里的BPTT和DNN也有很大的不同点，即这里所有的$U,W,V,b,c$在序列的各个位置是共享的，反向传播时我们更新的是相同的参数。   为了简化描述，这里的损失函数我们为对数损失函数，输出的激活函数为softmax函数，隐藏层的激活函数为tanh函数。   对于RNN，由于我们在序列的每个位置都有损失函数，因此最终的损失$L$为：   其中$V,c,$的梯度计算是比较简单的：   但是$W,U,b$的梯度计算就比较的复杂了。从RNN的模型可以看出，在反向传播时，在在某一序列位置t的梯度损失由当前位置的输出对应的梯度损失和序列索引位置$t+1$时的梯度损失两部分共同决定。对于$W$在某一序列位置t的梯度损失需要反向传播一步步的计算。我们定义序列索引$t$位置的隐藏状态的梯度为：   这样我们可以像DNN一样从$\\delta^{(t+1)} $递推$\\delta^{(t)}$ 。   对于$\\delta^{(\\tau)} $，由于它的后面没有其他的序列索引了，因此有：   有了$\\delta^{(t)} $,计算$W,U,b$就容易了，这里给出$W,U,b$的梯度计算表达式：   除了梯度表达式不同，RNN的反向传播算法和DNN区别不大，因此这里就不再重复总结了。   RNN小结   上面总结了通用的RNN模型和前向反向传播算法。当然，有些RNN模型会有些不同，自然前向反向传播的公式会有些不一样，但是原理基本类似。   RNN虽然理论上可以很漂亮的解决序列数据的训练，但是它也像DNN一样有梯度消失时的问题，当序列很长的时候问题尤其严重。因此，上面的RNN模型一般不能直接用于应用领域。在语音识别，手写书别以及机器翻译等NLP领域实际应用比较广泛的是基于RNN模型的一个特例LSTM，下一篇我们就来讨论LSTM模型。   # 参考文献   循环神经网络(RNN)模型与前向反向传播算法   零基础入门深度学习(5) - 循环神经网络   ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/RNN-%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%90%86%E8%AE%BA.html",
        "teaser":null},{
        "title": "神经网络",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html",
        "teaser":null},{
        "title": "向量降维",
        
        "excerpt":
            "```%matplotlib notebook import tensorflow as tf import math from sklearn import datasets from sklearn.manifold import TSNE import numpy as np import matplotlib.pyplot as plt from matplotlib import cm from mpl_toolkits.mplot3d import Axes3D import seaborn as sns &lt;/div&gt; &lt;/div&gt; # 词向量降维 ## 词向量数据 &lt;div markdown=\"1\" class=\"cell code_cell\"&gt; &lt;div class=\"input_area\" markdown=\"1\"&gt; ```import word2vec model = word2vec.load('/Users/weidian/workspace/data/hanlp/data1/data/vector/hanlp-wiki-vec-zh.txt') input_vec=model.vectors.astype('float32') vocabs=model.vocab 评价标准 ```import numpy import faiss def verify(vec,word=’菠萝’): verify_index = np.argwhere(vocabs == word)[0][0] index = faiss.IndexFlatL2(vec.shape[1]) index.add(vec) print(“向量数：” + str(index.ntotal)+”\\t维数：” + str(index.d)) xq = np.array([vec[verify_index]]).astype('float32') k = 10 D, I = index.search(xq, k) print(model.vocab[I[:15]]) #print(I) print(\"*********************************************\") verify(input_vec) &lt;/div&gt; &lt;div class=\"output_wrapper\" markdown=\"1\"&gt; &lt;div class=\"output_subarea\" markdown=\"1\"&gt; {:.output_stream}...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%90%91%E9%87%8F%E9%99%8D%E7%BB%B4.html",
        "teaser":null},{
        "title": "常见损失函数汇总",
        
        "excerpt":
            "损失函数   mean_squared_error (MSE) 平方差函数   计算公式     其中y是我们期望的输出，$a$为神经元的实际输出$a=\\sigma(Wx+b)$。也就是说，当神经元的实际输出与我们的期望输出差距越大，代价就越高。想法非常的好，然而在实际应用中，我们知道参数的修正是与$\\frac{\\partial{L}}{\\partial{W}}$和$\\frac{\\partial{L}}{\\partial{b}}$成正比的，而根据     我们发现其中都有$\\sigma’(y_i)$这一项。因为$sigmoid$函数的性质，导致$\\sigma’(z)$在$z$取大部分值时会造成饱和现象，从而使得参数的更新速度非常慢，甚至会造成离期望值越远，更新越慢的现象。那么怎么克服这个问题呢？我们想到了交叉熵函数。   binary_crossentropy(logloss) 交叉熵函数   我们知道，熵的计算公式是     而在实际操作中，我们并不知道y的分布，只能对y的分布做一个估计，也就是算得的a值, 这样我们就能够得到用a来表示y的交叉熵     如果有多个样本，则整个样本的平均交叉熵为     其中$n$表示样本编号,$i$表示类别编。 如果用于logistic分类，则上式可以简化成     与平方损失函数相比，交叉熵函数有个非常好的特质，     可以看到其中没有了$\\sigma’$这一项，这样一来也就不会受到饱和性的影响了。当误差大的时候，权重更新就快，当误差小的时候，权重的更新就慢。这是一个很好的性质。   Keras损失函数   mean_squared_error (MSE) 平方差函数     mean_absolute_error (mae) 绝对值均差     mean_absolute_percentage_error (mape)                  公式为：(       (y_true - y_pred) / clip((       y_true       ),epsilon, infinite)       ).mean(axis=-1) * 100，和mae的区别就是，累加的是（预测值与实际值的差）除以（剔除不介于epsilon和infinite之间的实际值)，然后求均值。           mean_squared_logarithmic_error (msle)   hinge   logcosh   squared_hinge   categorical_crossentropy   sparse_categorical_crossentropy   kullback_leibler_divergence   poisson   cosine_proximity   参考文献  深度神经网络（DNN）损失函数和激活函数的选择   ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%B8%B8%E8%A7%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%B1%87%E6%80%BB.html",
        "teaser":null},{
        "title": "常见激励函数汇总",
        
        "excerpt":
            "title: 常见激活函数汇总 author: niult date: 2019-01-15 category: 神经网络 tags: python,numpy,tf,激活函数 激活函数 关于激活函数，首先要搞清楚的问题是，激活函数是什么，有什么用？不用激活函数可不可以？答案是不可以。激活函数的主要作用是提供网络的非线性建模能力。如果没有激活函数，那么该网络仅能够表达线性映射，此时即便有再多的隐藏层，其整个网络跟单层神经网络也是等价的。因此也可以认为，只有加入了激活函数之后，深度神经网络才具备了分层的非线性映射学习能力。 那么激活函数应该具有什么样的性质呢？ 可微性： 当优化方法是基于梯度的时候，这个性质是必须的。 单调性： 当激活函数是单调的时候，单层网络能够保证是凸函数。 输出值的范围： 当激活函数输出值是 有限 的时候，基于梯度的优化方法会更加 稳定，因为特征的表示受有限权值的影响更显著;当激活函数的输出是 无限 的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的learning rate 从目前来看，常见的激活函数多是分段线性和具有指数形状的非线性函数 %matplotlib inline import numpy as np import matplotlib.pyplot as plt sigmoid sigmoid 是使用范围最广的一类激活函数，具有指数函数形状，它在物理意义上最为接近生物神经元。此外，(0, 1) 的输出还可以被表示作概率，或用于输入的归一化，代表性的如Sigmoid交叉熵损失函数。 然而，sigmoid也有其自身的缺陷，最明显的就是饱和性。从上图可以看到，其两侧导数逐渐趋近于0 具有这种性质的称为软饱和激活函数。具体的，饱和又可分为左饱和与右饱和。与软饱和对应的是硬饱和, 即 $sigmoid$ 的软饱和性，使得深度神经网络在二三十年里一直难以有效的训练，是阻碍神经网络发展的重要原因。具体来说，由于在后向传递过程中，sigmoid向下传导的梯度包含了一个 $f′(x)$ 因子（sigmoid关于输入的导数），因此一旦输入落入饱和区，$f′(x)$ 就会变得接近于0，导致了向底层传递的梯度也变得非常小。此时，网络参数很难得到有效训练。这种现象被称为梯度消失。一般来说， $sigmoid$ 网络在 5 层之内就会产生梯度消失现象 此外，sigmoid函数的输出均大于0，使得输出不是0均值，这称为偏移现象，这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。 def sigmoid(x): y = 1.0 / (1.0 + np.exp(-x)) return y x = np.linspace(start=-5, stop=5, num=100) y = sigmoid(x) plt.plot(x,y) [&lt;matplotlib.lines.Line2D at 0x11ab227f0&gt;] ELU 融合了sigmoid和ReLU，左侧具有软饱和性，右侧无饱和性。右侧线性部分使得ELU能够缓解梯度消失，而左侧软饱能够让ELU对输入变化或噪声更鲁棒。ELU的输出均值接近于零，所以收敛速度更快。在 ImageNet上，不加 Batch Normalization 30 层以上的 ReLU 网络会无法收敛，PReLU网络在MSRA的Fan-in （caffe ）初始化下会发散，而 ELU 网络在Fan-in/Fan-out下都能收敛 def elu(x, a): y = x.copy() for i in range(y.shape[0]):...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%B8%B8%E8%A7%81%E6%BF%80%E5%8A%B1%E5%87%BD%E6%95%B0%E6%B1%87%E6%80%BB.html",
        "teaser":null},{
        "title": "15tools",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/15tools/index.html",
        "teaser":null},{
        "title": "LaTeX公式实例",
        
        "excerpt":
            "定理引用   \\begin{theorem} \\label{theo:dotp}  $u$ 和 $v$ 是两个向量 $\\mathbb{R}^n$. 点成可以表示成 \\begin{equation} \\label{eq:dotp} u^Tv = |u||v| \\cos \\theta, \\end{equation} where $\\theta$ is the angle between $u$ and $v$ … \\end{theorem}   定理 \\ref{theo:dotp} 举了一个例子   公式引用   $u$ 和 $v$ 是两个向量 $\\mathbb{R}^n$. 点成可以表示成   \\begin{equation} \\label{eq:dotp2} u^Tv = |u||v| \\cos \\theta, \\end{equation} where $\\theta$ is the angle between $u$ and $v$ …   公式 \\ref{eq:dotp2} 和\\ref{eq:dotp} 举了一个例子   公式 \\ref{eq:dotp2} 和\\ref{eq:dotp} 举了一个例子  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/15tools/latex/LaTeX%E5%85%AC%E5%BC%8F%E5%AE%9E%E4%BE%8B.html",
        "teaser":null},{
        "title": "latex",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/15tools/latex/index.html",
        "teaser":null},{
        "title": "latex公式编号",
        
        "excerpt":
            "(some) LaTeX environments for Jupyter notebook IPython.utils.load_extensions('calico-document-tools'); &lt;IPython.core.display.Javascript object&gt; %%html &lt;style&gt; .prompt{ display: none; &lt;/style&gt; Presentation and main features This extension for IPython 3.x or Jupyter enables to use some LaTeX commands and environments in the notebook’s markdown cells. \\begin{enumerate} \\item LaTeX commands and environments \\begin{itemize} \\item support for some LaTeX commands within markdown cells, e.g. \\textit, \\textbf, \\underline \\item support for theorems-like environments \\item support for lists: enumerate, itemize, \\item limited support for a figure environment, \\item support for an environment listing, \\item additional textboxa environment \\end{itemize} \\item Citations and bibliography \\begin{itemize} \\item support for \\cite with creation of...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/15tools/latex/latex%E5%85%AC%E5%BC%8F%E7%BC%96%E5%8F%B7.html",
        "teaser":null},{
        "title": "十分钟上手LaTeX",
        
        "excerpt":
            "简介 http://zh.wikipedia.org/wiki/Help:MATH 函数、符号及特殊字符 常用 声调 语法 效果 语法 效果 \\bar{x} $\\bar{x}$ \\acute{\\eta} $\\acute{\\eta}$ \\check{\\alpha} $\\check{\\alpha}$ \\breve{a} $ \\breve{a}$ \\grave{\\eta} $\\grave{\\eta}$ \\ddot{y} $\\ddot{y}$ \\dot{x} $\\dot{x} $ \\hat{\\alpha} $\\hat{\\alpha}$ \\tilde{\\iota} $\\tilde{\\iota}$     函数 语法 效果 语法 效果     \\sin\\theta $\\sin\\theta$ \\cos\\theta $\\cos\\theta$     \\tan\\theta $\\tan\\theta$ \\arctan\\frac{L}{T} $\\arctan\\frac{L}{T}$     \\arcsin\\frac{L}{r} $\\arcsin\\frac{L}{r}$ \\arccos\\frac{T}{r} $\\arccos\\frac{T}{r}$     \\sinh g $\\sinh g$ \\cosh h $\\cosh h$ \\tanh i $\\tanh i$ \\operatorname{sh}j $\\operatorname{sh}j$ \\operatorname{argsh}k $\\operatorname{argsh}k$     \\operatorname{ch}h $\\operatorname{ch}h$ \\operatorname{argch}l $\\operatorname{argch}l$     \\operatorname{th}i $\\operatorname{th}i$ \\operatorname{argth}m $\\operatorname{argth}m$     \\limsup S $\\limsup S$ \\liminf I $\\liminf I$ \\lim_{t\\to n}T $\\lim_{t\\to n}T$ \\max H...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/15tools/latex/%E5%8D%81%E5%88%86%E9%92%9F%E4%B8%8A%E6%89%8BLaTeX.html",
        "teaser":null},{
        "title": "numpy",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/15tools/numpy/index.html",
        "teaser":null},{
        "title": "numpy教程00",
        
        "excerpt":
            "NumPy Credits: Forked from Parallel Machine Learning with scikit-learn and IPython by Olivier Grisel NumPy Arrays, dtype, and shape Common Array Operations Reshape and Update In-Place Combine Arrays Create Sample Data import numpy as np NumPy Arrays, dtypes, and shapes a = np.array([1, 2, 3]) print(a) print(a.shape) print(a.dtype) [1 2 3] (3,) int64 b = np.array([[0, 2, 4], [1, 3, 5]]) print(b) print(b.shape) print(b.dtype) [[0 2 4] [1 3 5]] (2, 3) int64 np.zeros(5) array([ 0., 0., 0., 0., 0.]) np.ones(shape=(3, 4), dtype=np.int32) array([[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]], dtype=int32) Common Array Operations c...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/15tools/numpy/numpy%E6%95%99%E7%A8%8B00.html",
        "teaser":null},{
        "title": "numpy教程01",
        
        "excerpt":
            "Introduction to NumPy This chapter, along with chapter 3, outlines techniques for effectively loading, storing, and manipulating in-memory data in Python. The topic is very broad: datasets can come from a wide range of sources and a wide range of formats, including be collections of documents, collections of images, collections of sound clips, collections of numerical measurements, or nearly anything else. Despite this apparent heterogeneity, it will help us to think of all data fundamentally as arrays of numbers. For example, images–particularly digital images–can be thought of as simply two-dimensional arrays of numbers representing pixel brightness across the area. Sound...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/15tools/numpy/numpy%E6%95%99%E7%A8%8B01.html",
        "teaser":null},{
        "title": "numpy教程02",
        
        "excerpt":
            "Aggregations: Min, Max, and Everything In Between Often when faced with a large amount of data, a first step is to compute summary statistics for the data in question. Perhaps the most common summary statistics are the mean and standard deviation, which allow you to summarize the “typical” values in a dataset, but other aggregates are useful as well (the sum, product, median, minimum and maximum, quantiles, etc.). NumPy has fast built-in aggregation functions for working on arrays; we’ll discuss and demonstrate some of them here. Summing the Values in an Array As a quick example, consider computing the sum...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/15tools/numpy/numpy%E6%95%99%E7%A8%8B02.html",
        "teaser":null},{
        "title": "numpy教程03",
        
        "excerpt":
            "Fancy Indexing In the previous sections, we saw how to access and modify portions of arrays using simple indices (e.g., arr[0]), slices (e.g., arr[:5]), and Boolean masks (e.g., arr[arr &gt; 0]). In this section, we’ll look at another style of array indexing, known as fancy indexing. Fancy indexing is like the simple indexing we’ve already seen, but we pass arrays of indices in place of single scalars. This allows us to very quickly access and modify complicated subsets of an array’s values. Exploring Fancy Indexing Fancy indexing is conceptually simple: it means passing an array of indices to access multiple...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/15tools/numpy/numpy%E6%95%99%E7%A8%8B03.html",
        "teaser":null},{
        "title": "pandas",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/15tools/pandas/index.html",
        "teaser":null},{
        "title": "pandas",
        
        "excerpt":
            "This notebook was prepared by Donne Martin. Source and license info is on GitHub. Pandas Credits: The following are notes taken while working through Python for Data Analysis by Wes McKinney Series DataFrame Reindexing Dropping Entries Indexing, Selecting, Filtering Arithmetic and Data Alignment Function Application and Mapping Sorting and Ranking Axis Indices with Duplicate Values Summarizing and Computing Descriptive Statistics Cleaning Data (Under Construction) Input and Output (Under Construction) from pandas import Series, DataFrame import pandas as pd import numpy as np Series A Series is a one-dimensional array-like object containing an array of data and an associated array of...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/15tools/pandas/pandas.html",
        "teaser":null},{
        "title": "pandas函数深入",
        
        "excerpt":
            "replace 替换函数   Series对象值替换            # s = df.iloc[2]#获取行索引为2数据 # #单值替换 # s.replace('?',np.nan)#用np.nan替换？ # s.replace({'?':'NA'})#用NA替换？ # #多值替换 # s.replace(['?',r'$'],[np.nan,'NA'])#列表值替换 # s.replace({'?':np.nan,'$':'NA'})#字典映射 # #同缺失值填充方法类似 # s.replace(['?','$'],method='pad')#向前填充 # s.replace(['?','$'],method='ffill')#向前填充 # s.replace(['?','$'],method='bfill')#向后填充 # #limit参数控制填充次数 # s.replace(['?','$'],method='bfill',limit=1) # #DataFrame对象值替换 # #单值替换 # df.replace('?',np.nan)#用np.nan替换？ # df.replace({'?':'NA'})#用NA替换？ # #按列指定单值替换 # df.replace({'EMPNO':'?'},np.nan)#用np.nan替换EMPNO列中? # df.replace({'EMPNO':'?','ENAME':'.'},np.nan)#用np.nan替换EMPNO列中?和ENAME中. # #多值替换 # df.replace(['?','.','$'],[np.nan,'NA','None'])##用np.nan替换？用NA替换. 用None替换$ # df.replace({'?':'NA','$':None})#用NA替换？ 用None替换$ # df.replace({'?','$'},{'NA',None})#用NA替换？ 用None替换$ # #正则替换 # df.replace(r'\\?|\\.|\\$',np.nan,regex=True)#用np.nan替换？或.或$原字符 # df.replace([r'\\?',r'\\$'],np.nan,regex=True)#用np.nan替换？和$ # df.replace([r'\\?',r'\\$'],[np.nan,'NA'],regex=True)#用np.nan替换？用NA替换$符号 # df.replace(regex={r'\\?':None}) # #value参数显示传递 # df.replace(regex=[r'\\?|\\.|\\$'],value=np.nan)#用np.nan替换？或.或$原字符            ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/15tools/pandas/pandas%E5%87%BD%E6%95%B0%E6%B7%B1%E5%85%A5.html",
        "teaser":null},{
        "title": "pandas教程01",
        
        "excerpt":
            "Data Manipulation with Pandas In the previous chapter, we dove into detail on NumPy and its ndarray object, which provides efficient storage and manipulation of dense typed arrays in Python. Here we’ll build on this knowledge by looking in detail at the data structures provided by the Pandas library. Pandas is a newer package built on top of NumPy, and provides an efficient implementation of a DataFrame. DataFrames are essentially multidimensional arrays with attached row and column labels, and often with heterogeneous types and/or missing data. As well as offering a convenient storage interface for labeled data, Pandas implements a...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/15tools/pandas/pandas%E6%95%99%E7%A8%8B01.html",
        "teaser":null},{
        "title": "pandas教程02",
        
        "excerpt":
            "Hierarchical Indexing Up to this point we’ve been focused primarily on one-dimensional and two-dimensional data, stored in Pandas Series and DataFrame objects, respectively. Often it is useful to go beyond this and store higher-dimensional data–that is, data indexed by more than one or two keys. While Pandas does provide Panel and Panel4D objects that natively handle three-dimensional and four-dimensional data (see Aside: Panel Data), a far more common pattern in practice is to make use of hierarchical indexing (also known as multi-indexing) to incorporate multiple index levels within a single index. In this way, higher-dimensional data can be compactly represented...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/15tools/pandas/pandas%E6%95%99%E7%A8%8B02.html",
        "teaser":null},{
        "title": "pandas教程03",
        
        "excerpt":
            "Vectorized String Operations One strength of Python is its relative ease in handling and manipulating string data. Pandas builds on this and provides a comprehensive set of vectorized string operations that become an essential piece of the type of munging required when working with (read: cleaning up) real-world data. In this section, we’ll walk through some of the Pandas string operations, and then take a look at using them to partially clean up a very messy dataset of recipes collected from the Internet. Introducing Pandas String Operations We saw in previous sections how tools like NumPy and Pandas generalize arithmetic...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/15tools/pandas/pandas%E6%95%99%E7%A8%8B03.html",
        "teaser":null},{
        "title": "十分钟上手Pandas",
        
        "excerpt":
            "title: 十分钟上手Pandas_jupyter author: niult date: 2018-01-01 category: 01常用工具 tags: jupyter,python Pandas 简介 简介 来自官网十分钟教学 Pandas的主要数据结构： Dimensions Name Description 1 Series 1D labeled homogeneously-typed array 2 DataFrame General 2D labeled, size-mutable tabular structure with potentially heterogeneously-typed columns 3 PanelGeneral 3D labeled, also size-mutable array matplotlib图库具有大量代码案例，可直接使用 pandas 官网教程 import pandas as pd #数据分析，代码基于numpy import numpy as np #处理数据，代码基于ndarray import matplotlib.pyplot as plt #画图 引入 Series字典对象 s = pd.Series([1,3,5,np.nan,6,8]) #默认以数字从0开始作为键值,使用np.nan表示不参与计算 s 0 1.0 1 3.0 2 5.0 3 NaN 4 6.0 5 8.0 dtype: float64 s = pd.Series(data=[1,2,3,4],index = ['a','b','c','d']) #传入键和值方式 s a 1 b 2 c 3 d 4 dtype: int64 s.index...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/15tools/pandas/%E5%8D%81%E5%88%86%E9%92%9F%E4%B8%8A%E6%89%8BPandas.html",
        "teaser":null},{
        "title": "十分钟上手Pandas函数",
        
        "excerpt":
            "title: 十分钟上后Pands函数 author: niult date: 2018-01-01 category: 01常用工具 tags: jupyter,pyhton Pandas库 https://blog.csdn.net/Junming_Lu/article/details/79336878 Pandas是Python第三方库，提供高性能易用数据类型和分析工具 import pandas as pd Pandas基于NumPy实现，常于NumPy和Matplotlib一同使用 两个数据类型：Series, DataFrame，基于上述数据类型的各类操作，基本操作、运算操作、特征类操作、关联类操作 :NumPy: Pandas: 基础数据类型 扩展数据类型 关注数据的结构表达 关注数据的应用表达 维度：数据间的关系 数据与索引间关系 import pandas as pd import numpy as np Pandas的Series类型 Series类型由一组数据及与之相关的数据索引组成 a = pd.Series([9, 8, 7, 6]) a 0 9 1 8 2 7 3 6 dtype: int64 b = pd.Series([9, 8, 7, 6], index = ['a','b','c','d']) b a 9 b 8 c 7 d 6 dtype: int64 Series类型可以由如下类型创建： Python列表，index与列表元素个数一致 标量值，index表达Series类型的尺寸 Python字典，键值对中的“键”是索引，index从字典中进行选择操作 ndarray，索引和数据都可以通过ndarray类型创建 其他函数，range()函数等 从标量值创建 s = pd.Series(25, index = ['a','b','c']) #不能省略index= ### 从字典类型创建 d = pd.Series({'a':9, 'b':8, 'c':7}) ### 从ndarray类型创建 DataFrame类型 数据类型操作...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/15tools/pandas/%E5%8D%81%E5%88%86%E9%92%9F%E4%B8%8A%E6%89%8BPandas%E5%87%BD%E6%95%B0.html",
        "teaser":null},{
        "title": "Untitled",
        
        "excerpt":
            "        !pip install PyUserInput                        Collecting PyUserInput   Downloading https://files.pythonhosted.org/packages/d0/09/17fe0b16c7eeb52d6c14e904596ddde82503aeee268330120b595bf22d7b/PyUserInput-0.1.11.tar.gz Collecting pyobjc-framework-Quartz (from PyUserInput) \u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/3e/8d4fe867b8af8826d7c319eb45e9126667fac8ef676698e5ebcd24d7f9a5/pyobjc_framework_Quartz-5.2-cp37-cp37m-macosx_10_9_x86_64.whl (100kB) \u001b[K     |████████████████████████████████| 102kB 1.4MB/s  \u001b[?25hCollecting pyobjc-core&gt;=5.2 (from pyobjc-framework-Quartz-&gt;PyUserInput) \u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/ba/cb2f5b8a6c78de08b89f7f90cddbde3b10a8392b8350c0a0cf92120c5f43/pyobjc_core-5.2-cp37-cp37m-macosx_10_9_x86_64.whl (294kB) \u001b[K     |████████████████████████████████| 296kB 1.8MB/s  \u001b[?25hCollecting pyobjc-framework-Cocoa&gt;=5.2 (from pyobjc-framework-Quartz-&gt;PyUserInput) \u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/67/697f392aadf661488ef8590ffdd2a7734fa1db999a2830c136276929c58c/pyobjc_framework_Cocoa-5.2-cp37-cp37m-macosx_10_9_x86_64.whl (235kB) \u001b[K     |████████████████████████████████| 235kB 6.4MB/s  \u001b[?25hBuilding wheels for collected packages: PyUserInput   Building wheel for PyUserInput (setup.py) ... \u001b[?25ldone \u001b[?25h  Stored in directory: /Users/weidian/Library/Caches/pip/wheels/69/0e/a6/020f0a3bb3a1fc4307c5a2aab327e5671ec3826884116010f8 Successfully built PyUserInput Installing collected packages: pyobjc-core, pyobjc-framework-Cocoa, pyobjc-framework-Quartz, PyUserInput Successfully installed PyUserInput-0.1.11 pyobjc-core-5.2 pyobjc-framework-Cocoa-5.2 pyobjc-framework-Quartz-5.2                          from pykeyboard import PyKeyboard k = PyKeyboard() k.press_keys(['Command','s'])  #k.press_keys(['a'])               ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/15tools/%E7%94%BB%E5%9B%BE/Untitled.html",
        "teaser":null},{
        "title": "画图",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/15tools/%E7%94%BB%E5%9B%BE/index.html",
        "teaser":null},{
        "title": "python画图之folium地图",
        
        "excerpt":
            "简介 folium是js上著名的地理信息可视化库leaflet.js为Python提供的接口，通过它，我们可以通过在Python端编写代码操纵数据，来调用leaflet的相关功能，基于内建的osm或自行获取的osm资源和地图原件进行地理信息内容的可视化，以及制作优美的可交互地图。其语法格式类似ggplot2，是通过不断添加图层元素来定义一个Map对象，最后以几种方式将Map对象展现出来。 　　而在Map对象的生成形式上，可以在定义所有的图层内容之后，将其保存为html文件在浏览器中独立显示，也可以基于jupyter notebook在一个ipynb文件内部嵌入对应的交互地图，本文即采用后者对应的方法。 参考文献1 参考文献2 创建地图 　　首先，创建一张指定中心坐标的地图，这里指定中心坐标为重庆交通大学（注意，location的格式为[纬度,经度]，zoom_start表示初始地图的缩放尺寸，数值越大放大程度越大）： import folium import os m = folium.Map(location=[30.3229,120.3577],zoom_start=17) m 可以看出，m的类型为folium中的Map，类似ggplot2中显示图形的方式，接下来直接在jupyter notebook调用m即可显示地图（默认的osm资源地址在国外，需要稍许等待）; 通过这样一个简单的例子，可以了解到，folium.Map()即为folium中绘制地图图层的基本函数，其主要参数如下： location：tuple或list类型输入，用于控制初始地图中心点的坐标，格式为(纬度，经度)或[纬度，经度]，默认为None width：int型或str型，int型时，传入的是地图宽度的像素值；str型时，传入的是地图宽度的百分比，形式为’xx%’。默认为’100%’ height：控制地图的高度，格式同width tiles：str型，用于控制绘图调用的地图样式，默认为’OpenStreetMap’，也有一些其他的内建地图样式，如’Stamen Terrain’、’Stamen Toner’、’Mapbox Bright’、’Mapbox Control Room’等；也可以传入’None’来绘制一个没有风格的朴素地图，或传入一个URL来使用其它的自选osm max_zoom：int型，控制地图可以放大程度的上限，默认为18 attr：str型，当在tiles中使用自选URL内的osm时使用，用于给自选osm命名 control_scale：bool型，控制是否在地图上添加比例尺，默认为False即不添加 no_touch：bool型，控制地图是否禁止接受来自设备的触控事件譬如拖拽等，默认为False，即不禁止 下面针对上述各参数进行调整演示： 　　下面是一个width调整为50%，并施加比例尺的地图： '''创建Map对象''' m = folium.Map(location=[30.3229,120.3579], zoom_start=16, control_scale=True, width='50%') '''显示m''' m 如我们设置的一样，视图只有左半边被地图填充，且在地图的左下角施加了比例尺，标记出了公里和英里的比例尺。 　　下面是一个tiles设置为’Stamen Terrain’的地图： '''创建Map对象''' m = folium.Map(location=[30.3229,120.3579], zoom_start=14, tiles='Stamen Terrain') '''显示m''' m 在图层上添加各种内建的部件 为地图添加标记部件 有了最底层的地图，接下来我们就可以利用手里掌握的地理信息数据，根据需要将其展现在地图图层之上，下面是一个简单的示范： import folium import os '''创建Map对象''' m = folium.Map(location=[30.3229,120.3579], zoom_start=17) '''为m添加标记部件''' folium.Marker([29.488869,106.571034], popup='&lt;i&gt;Mt. Hood Meadows&lt;/i&gt;').add_to(m) '''显示m''' m 我们通过folium.Marker()方法，创建了一个简单的标记小部件，并通过add_to()将定义好的部件施加于先前创建的Map对象m之上，下面我们对folium.Marker()的常用参数进行介绍： 　　location：同folium.Map()中的同名参数，用于确定标记部件的经纬位置 　　popup：str型或folium.Popup()对象输入，用于控制标记部件的具体样式（folium内部自建了许多样式），默认为None，即不显示部件 　　icon：folium.Icon()对象，用于设置popup定义的部件的具体颜色、图标内容等 下面是几个简单的例子： '''创建Map对象''' m = folium.Map(location=[30.3229,120.3577], zoom_start=17) '''为m添加标记部件，并将部件上的图形设置为云朵''' folium.Marker([30.3229,120.3579], popup='Mt. Hood Meadows', icon=folium.Icon(icon='cloud')).add_to(m) '''显示m''' m '''创建Map对象''' m = folium.Map(location=[30.3229,120.3579], zoom_start=16) '''为m添加标记部件''' folium.Marker([30.3229,120.3579], popup='Timberline...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/15tools/%E7%94%BB%E5%9B%BE/python%E7%94%BB%E5%9B%BE%E4%B9%8Bfolium%E5%9C%B0%E5%9B%BE.html",
        "teaser":null},{
        "title": "python画图之pycharts-3D图",
        
        "excerpt":
            "3D柱状图 from pyecharts import Bar3D, Page, Style,online X_TIME=2 Y_WEEK=3 def create_charts(): page = Page() style = Style( ) data = [ [0, 0, 5], [0, 1, 1], [0, 2, 0], [0, 3, 0], [0, 4, 0], [0, 5, 0], [0, 6, 0], [0, 7, 0], [0, 8, 0], [0, 9, 0], [0, 10, 0], [0, 11, 2], [0, 12, 4], [0, 13, 1], [0, 14, 1], [0, 15, 3], [0, 16, 4], [0, 17, 6], [0, 18, 4], [0, 19, 4], [0, 20, 3], [0, 21, 3], [0, 22, 2], [0, 23, 5], [1, 0, 7], [1, 1, 0], [1,...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/15tools/%E7%94%BB%E5%9B%BE/python%E7%94%BB%E5%9B%BE%E4%B9%8Bpycharts-3D%E5%9B%BE.html",
        "teaser":null},{
        "title": "python画图之pycharts-kline",
        
        "excerpt":
            "from pyecharts.globals import CurrentConfig CurrentConfig.NOTEBOOK_TYPE=\"jupyter_lab\" from example.commons import Collector from pyecharts import options as opts from pyecharts.charts import Kline, Page data = [ [2320.26, 2320.26, 2287.3, 2362.94], [2300, 2291.3, 2288.26, 2308.38], [2295.35, 2346.5, 2295.35, 2345.92], [2347.22, 2358.98, 2337.35, 2363.8], [2360.75, 2382.48, 2347.89, 2383.76], [2383.43, 2385.42, 2371.23, 2391.82], [2377.41, 2419.02, 2369.57, 2421.15], [2425.92, 2428.15, 2417.58, 2440.38], [2411, 2433.13, 2403.3, 2437.42], [2432.68, 2334.48, 2427.7, 2441.73], [2430.69, 2418.53, 2394.22, 2433.89], [2416.62, 2432.4, 2414.4, 2443.03], [2441.91, 2421.56, 2418.43, 2444.8], [2420.26, 2382.91, 2373.53, 2427.07], [2383.49, 2397.18, 2370.61, 2397.94], [2378.82, 2325.95, 2309.17, 2378.82], [2322.94, 2314.16, 2308.76, 2330.88], [2320.62, 2325.82, 2315.01, 2338.78], [2313.74, 2293.34, 2289.89, 2340.71], [2297.77,...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/15tools/%E7%94%BB%E5%9B%BE/python%E7%94%BB%E5%9B%BE%E4%B9%8Bpycharts-kline.html",
        "teaser":null},{
        "title": "python画图之pycharts-其他",
        
        "excerpt":
            "饼图 from pyecharts.globals import CurrentConfig CurrentConfig.NOTEBOOK_TYPE=\"jupyter_lab\" from pyecharts.charts.base import Base from example.commons import Collector, Faker from pyecharts import options as opts from pyecharts.charts import Geo, Page from pyecharts.globals import ChartType, SymbolType from example.commons import Collector, Faker from pyecharts import options as opts from pyecharts.charts import Bar, Page #Base().load_javascript() def bar_base() -&gt; Bar: c = ( Bar() .add_xaxis(Faker.choose()) .add_yaxis(\"商家A\", Faker.values()) .add_yaxis(\"商家B\", Faker.values()) .set_global_opts(title_opts=opts.TitleOpts(title=\"Bar-基本示例\", subtitle=\"我是副标题\")) ) return c c= bar_base() c.load_javascript() c.render_notebook() &lt;!DOCTYPE html&gt; def bar_is_selected() -&gt; Bar: c = ( Bar() .add_xaxis(Faker.choose()) .add_yaxis(\"商家A\", Faker.values()) .add_yaxis(\"商家B\", Faker.values(), is_selected=False) .set_global_opts(title_opts=opts.TitleOpts(title=\"Bar-默认取消显示某 Series\")) ) return c c= bar_is_selected() c.load_javascript() c.render_notebook() &lt;!DOCTYPE html&gt; def bar_toolbox() -&gt;...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/15tools/%E7%94%BB%E5%9B%BE/python%E7%94%BB%E5%9B%BE%E4%B9%8Bpycharts-%E5%85%B6%E4%BB%96.html",
        "teaser":null},{
        "title": "python画图之pycharts-其他坐标系",
        
        "excerpt":
            "极坐标系 import random import math from pyecharts import Polar, Page, Style def create_charts(): page = Page() style = Style( ) data = [(i, random.randint(1, 100)) for i in range(101)] chart = Polar(\"极坐标系-散点图\") chart.add(\"\", data, boundary_gap=False, type='scatter', is_splitline_show=False, is_axisline_show=True) page.add(chart) data_1 = [(10, random.randint(1, 100)) for _ in range(300)] data_2 = [(11, random.randint(1, 100)) for _ in range(300)] chart = Polar(\"极坐标系-散点图\") chart.add(\"\", data_1, type='scatter') chart.add(\"\", data_2, type='scatter') page.add(chart) data = [(i, random.randint(1, 100)) for i in range(10)] chart = Polar(\"极坐标系-动态散点图\") chart.add(\"\", data, type='effectScatter', effect_scale=10, effect_period=5) page.add(chart) radius = ['周一', '周二', '周三', '周四', '周五', '周六', '周日'] chart = Polar(\"极坐标系-堆叠柱状图\") _style1 = style.add(...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/15tools/%E7%94%BB%E5%9B%BE/python%E7%94%BB%E5%9B%BE%E4%B9%8Bpycharts-%E5%85%B6%E4%BB%96%E5%9D%90%E6%A0%87%E7%B3%BB.html",
        "teaser":null},{
        "title": "python画图之pycharts-地图",
        
        "excerpt":
            "安装 !pip install echarts-countries-pypkg !pip install echarts-china-provinces-pypkg !pip install echarts-china-cities-pypkg !pip install echarts-china-counties-pypkg !pip install echarts-china-misc-pypkg !pip install echarts-united-kingdom-pypkg from pyecharts.globals import CurrentConfig CurrentConfig.NOTEBOOK_TYPE=\"jupyter_lab\" from pyecharts.charts.base import Base from example.commons import Collector, Faker from pyecharts import options as opts from pyecharts.charts import Geo, Page from pyecharts.globals import ChartType, SymbolType #Base().load_javascript() 地理坐标系 def geo_base() -&gt; Geo: c = ( Geo() .add_schema(maptype=\"china\") .add(\"geo\", [list(z) for z in zip(Faker.provinces, Faker.values())]) .set_series_opts(label_opts=opts.LabelOpts(is_show=False)) .set_global_opts( visualmap_opts=opts.VisualMapOpts(), title_opts=opts.TitleOpts(title=\"Geo-基本示例\"), ) ) return c c = geo_base() c.load_javascript() c.render_notebook() &lt;!DOCTYPE html&gt; 地理坐标系线图 def geo_visualmap_piecewise() -&gt; Geo: c = ( Geo() .add_schema(maptype=\"china\") .add(\"geo\", [list(z) for z in zip(Faker.provinces, Faker.values())]) .set_series_opts(label_opts=opts.LabelOpts(is_show=False))...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/15tools/%E7%94%BB%E5%9B%BE/python%E7%94%BB%E5%9B%BE%E4%B9%8Bpycharts-%E5%9C%B0%E5%9B%BE.html",
        "teaser":null},{
        "title": "python画图之pycharts-直角坐标系",
        
        "excerpt":
            "柱状图 from pyecharts import online online() import random from pyecharts import Bar, Page, Style def create_charts(): page = Page() attr = [\"衬衫\", \"羊毛衫\", \"雪纺衫\", \"裤子\", \"高跟鞋\", \"袜子\"] v1 = [5, 20, 36, 10, 75, 90] v2 = [10, 25, 8, 60, 20, 80] chart = Bar(\"柱状图-数据堆叠\") chart.add(\"商家A\", attr, v1, is_stack=True) chart.add(\"商家B\", attr, v2, is_stack=True, is_more_utils=True) page.add(chart) chart = Bar(\"柱状图-数据标记\") chart.add(\"商家A\", attr, v1, mark_point=[\"average\"]) chart.add(\"商家B\", attr, v2, mark_line=[\"min\", \"max\"], is_more_utils=True) page.add(chart) chart = Bar(\"柱状图-xy 轴互换\") chart.add(\"商家A\", attr, v1) chart.add(\"商家B\", attr, v2, is_convert=True) page.add(chart) chart = Bar(\"柱状图-直方图\") chart.add(\"\", attr * 2, v1 + v2, bar_category_gap=0) page.add(chart) attr = [\"{}天\".format(i) for i in...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/15tools/%E7%94%BB%E5%9B%BE/python%E7%94%BB%E5%9B%BE%E4%B9%8Bpycharts-%E7%9B%B4%E8%A7%92%E5%9D%90%E6%A0%87%E7%B3%BB.html",
        "teaser":null},{
        "title": "python画图之pycharts-简介",
        
        "excerpt":
            "        from pyecharts.charts import Bar #from pyecharts.globals import CurrentConfig #CurrentConfig.NOTEBOOK_TYPE=\"jupyter_lab\"  bar = Bar() bar.add_xaxis([\"衬衫\", \"羊毛衫\", \"雪纺衫\", \"裤子\", \"高跟鞋\", \"袜子\"]) bar.add_yaxis(\"商家A\", [5, 20, 36, 10, 75, 90]) # render 会生成本地 HTML 文件，默认会在当前目录生成 render.html 文件 # 也可以传入路径参数，如 bar.render(\"mycharts.html\")  bar.load_javascript() #bar.render_notebook()                         &lt;pyecharts.render.display.Javascript at 0x10c1afc18&gt;                           from IPython.core.display import display, HTML  html = bar.render_embed()  display(HTML(html))                          &lt;!DOCTYPE html&gt;            Awesome-pyecharts                                          参考文献   pyecharts官网实例  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/15tools/%E7%94%BB%E5%9B%BE/python%E7%94%BB%E5%9B%BE%E4%B9%8Bpycharts-%E7%AE%80%E4%BB%8B.html",
        "teaser":null},{
        "title": "DownLoadPicture",
        
        "excerpt":
            "   title: 下载图片到指定路径   author: niult   date: 2019-01-15   category: tools   tags: python           def load(url):     import requests     import os          month = time.strftime('%Y%m%d',time.localtime())     filename = os.path.basename(url)          fileroot = 'data/image/blog/' + month# + '/' + filename          filename = os.path.basename(url)     filepath = fileroot + \"/\" + filename     try:         # 目录不存在则新建         if not os.path.exists(fileroot):             print(\"目录不存在则创建\")             os.makedirs(fileroot)                  #文件存在则删除         if os.path.exists(filepath):             os.remove(filepath)             print(\"文件存在，删除并重新下载\"+filepath)         r = requests.get(url)         r.raise_for_status()         with open(filepath,\"wb\") as f:             f.write(r.content)             print(\"下载成功\")             return filepath.replace(\"data/image/blog\",\"https://raw.githubusercontent.com/1007530194/data/master/image/blog\")     except Exception as e:         print(\"下载失败:\"+str(e))     return \"\"                     import time import os  url = 'https://images0.cnblogs.com/blog/326731/201308/19223512-2d487c10033442248c2fb7b847821d89.png'  load(url)                        文件存在，删除并重新下载data/image/blog/20190116/19223512-2d487c10033442248c2fb7b847821d89.png 下载成功                               'https://raw.githubusercontent.com/1007530194/data/master/image/blog/20190116/19223512-2d487c10033442248c2fb7b847821d89.png'                           https://raw.githubusercontent.com/1007530194/data/master/image/blog/20190116           ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/DownLoadPicture.html",
        "teaser":null},{
        "title": "Example",
        
        "excerpt":
            "title: example实例 author: niult date: 2018-01-01 category: tool1,tool2 tags: python,numpy title: algorithms-HMM author: niult date: 2019-01-16 category: 文本挖掘 tags: python,numpy,文本挖掘 title: author: niult date: 2019-01-16 category: deep-learning-with-python tags: python,numpy,deep-learning Scientific Python Cheatsheet IPGP / Scientific Python Cheatsheet NumPy 测试玩下 import numpy as np a = 2 print(a) 2 import matplotlib.pyplot as plt import numpy as np %matplotlib inline x = np.arange(20) y = x**2 plt.plot(x, y) print(x) [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19] #!wget -P data/data/pyspark_example.json -c s3n://path/to/data.json b=[1,2] a=(1,2,b) b.append(3) b.clear() a (1, 2,...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/Example.html",
        "teaser":null},{
        "title": "LICENSE",
        
        "excerpt":
            " ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/LICENSE.html",
        "teaser":null},{
        "title": "Test1",
        
        "excerpt":
            "a=input(\"a:\") a=int(a) b=input(\"b:\") b=int(b) if(a &gt; b): print(a,\"&gt;\",b) print(a,\"&lt;\",b) a:2 b:3 (2, '&lt;', 3) a=2 b=3 if a&gt;b: print(a, \"&gt;\", b) else: print(a, \"&lt;\", b) (2, '&lt;', 3) a = input (\"a:\") a = int (a) b = input (\"b:\") b = int (b) if a &gt; b: print (a, \"&gt;\", b) print (a, \"&lt;\", b) a:2 b:3 (2, '&lt;', 3) score = float (input (\"score:\")) if 90 &lt;= score &lt;= 100: print (\"A\") elif 80 &lt;= score &lt; 90: print (\"B\") elif 70 &lt;= score &lt; 80: print (\"C\") elif 60 &lt;= score &lt; 70: print (\"D\") else: print...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/TempDir/Test1.html",
        "teaser":null},{
        "title": "TempDir",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/TempDir/index.html",
        "teaser":null},{
        "title": "个税计算器",
        
        "excerpt":
            "import pandas as pd d1 = [(0 ,36000 , 0.03,0 ) , (36000 ,144000, 0.10,2520 ) , (144000,300000, 0.20,16920) , (300000,420000, 0.25,31920) , (420000,660000, 0.30,52920) , (660000,960000, 0.35,85920) , (960000,990000, 0.45,181920)] d2 = pd.DataFrame(d1) d2.columns=['a','b','c','f'] salary = 18500*2 def old_tax(salary): temp = salary*12 d3 = d2[(d2['a']&lt;temp) &amp; (d2['b']&gt;=temp)] d4 = d3.values[0] tax = temp*d4[2]-d4[3] month_tax = tax/12 sum_tax = tax return (month_tax,sum_tax) def new_tax(salary): month_tax = [] sum_tax = 0 for i in range(1,13): temp = salary*i dt3 = d2[(d2['a']&lt;temp) &amp; (d2['b']&gt;=temp)] dt4 = dt3.values[0] tax = temp*dt4[2]-dt4[3]-sum_tax sum_tax += tax month_tax.append(tax) return (month_tax,sum_tax) def tax(salary_pre, social_insurance, deduction): temp =...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/TempDir/%E4%B8%AA%E7%A8%8E%E8%AE%A1%E7%AE%97%E5%99%A8.html",
        "teaser":null},{
        "title": "强化学习实践",
        
        "excerpt":
            "import numpy as np import math import matplotlib.pyplot as plt def gaussian(x, u, sigma): return math.exp(-(x - u) ** 2 / (2 * sigma * sigma)) / math.sqrt(2 * math.pi * sigma * sigma) def importance_sampling_test(ori_sigma, sample_sigma): origin = [] for n in range(10): sum = 0 for i in range(100000): a = np.random.normal(1.0, ori_sigma) sum += a origin.append(sum) isample = [] for n in range(10): sum2 = 0 for i in range(100000): a= np.random.normal(1.0, sample_sigma) ua = gaussian(a, 1.0, sample_sigma) na = gaussian(a, 1.0, ori_sigma) sum2 += a * na / ua isample.append(sum2) origin = np.array(origin) isample = np.array(isample)...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/TempDir/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html",
        "teaser":null},{
        "title": "Todo",
        
        "excerpt":
            "             # 安装指定版本 # conda install --channel https://conda.anaconda.org/anaconda tensorflow=1.6.0                        2                  1   2   3   4   5   6   7   8   9   10   多层神经网络  11   12   13   14                              找事做   https://github.com/innovation-cat/DeepLearningBook   http://playground.tensorflow.org/   https://www.jiqizhixin.com   推荐实现 https://www.jiqizhixin.com/articles/2017-12-28-20   https://www.jiqizhixin.com/articles/alibaba-pku-ATRank   其他 http://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/79155896 https://www.jiqizhixin.com/articles/2017-12-05-2 http://blog.csdn.net/aliceyangxi1987/article/details/71079448   https://github.com/ujjwalkarn/Machine-Learning-Tutorials   1 一起读懂传说中的经典：受限玻尔兹曼机   大数据/数据挖掘/推荐系统/机器学习相关资源  https://github.com/weiweifan/Big-Data-Resources   https://github.com/search?o=desc&amp;q=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0&amp;s=stars&amp;type=Repositories&amp;utf8=%E2%9C%93 https://github.com/Kivy-CN/Stanford-CS-229-CN   推荐系统相关的Paper http://blog.csdn.net/mycafe_/article/details/79123757   https://cyent.github.io/markdown-with-mkdocs-material/   https://github.com/ageron/handson-ml   博客生成  https://wklchris.github.io/Data-science-support-blog-skills.html   https://github.com/ctgk/PRML/blob/master/notebooks/ch09_Mixture_Models_and_EM.ipynb https://github.com/ugik/notebooks https://github.com/explosion/spacy-notebooks https://github.com/ageron/handson-ml https://github.com/googledatalab/notebooks https://github.com/search?o=desc&amp;q=notebooks&amp;s=stars&amp;type=Repositories&amp;utf8=%E2%9C%93 https://github.com/ecomfe/spec   http://www.huaxiaozhuan.com/           https://github.com/princewen/tensorflow_practice           ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/Todo.html",
        "teaser":null},{
        "title": "Untitled",
        
        "excerpt":
            "Python for Android 安装 # pip install kivy # 打包工具 pip install buildozer from kivy.interactive import InteractiveLauncher from kivy.app import App from kivy.uix.button import Button class MyApp(App): def build(self): return Button(text='Hello Shell') launcher = InteractiveLauncher(MyApp()) launcher.run() from kivy.interactive import InteractiveLauncher from kivy.app import App from kivy.uix.widget import Widget from kivy.graphics import Color, Ellipse class MyPaintWidget(Widget): def on_touch_down(self, touch): with self.canvas: Color(1, 1, 0) d = 30. Ellipse(pos=(touch.x - d/2, touch.y - d/2), size=(d, d)) class TestApp(App): def build(self): return Widget() i = InteractiveLauncher(TestApp()) i.run() i. # press 'tab' to list attributes of the app i.root. # press 'tab' to list...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/Untitled.html",
        "teaser":null},{
        "title": "README",
        
        "excerpt":
            "read  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/colabHub/README.html",
        "teaser":null},{
        "title": "colabHub",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/colabHub/index.html",
        "teaser":null},{
        "title": "初始化",
        
        "excerpt":
            "        !pwd                        /Users/weidian/Documents/MyDiary                          !pip install noteblog           ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/colabHub/%E5%88%9D%E5%A7%8B%E5%8C%96.html",
        "teaser":null},{
        "title": "01_overview",
        
        "excerpt":
            "The Jupyter Book Guide This is a guide for creating your own book using Jupyter Notebooks and Jekyll. Book content is written in markdown and Jupyter Notebooks, and jupyter-book converts these into a book fit for hosting on the web. Install the command-line interface First off, make sure you have the CLI installed so that you can work with Jupyter Book. The Jupyter-Book CLI allows you to create, build, upgrade, and otherwise control your Jupyter Book. You can install it via pip with the following command: pip install jupyter-book A quick tour of a Jupyter Book Jupyter-Book comes with a...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/guide/01_overview.html",
        "teaser":null},{
        "title": "02_create",
        
        "excerpt":
            "Create your Jupyter Book This page covers how to create a Jupyter Book with your own content. You’ve got three primary ways to create your Jupyter Book. by starting with a minimal book Running the following command will create a new Jupyter Book with a few content pages and a Table of Contents to get you started: jupyter-book create mybookname This will create a new book using your content in mybookname/. You’ll then need to Add your content to mybookname/content/ Modify mybookname/_data/toc.yml to match your content Modify mybookname/_config.yml to the configuration you’d like Note that if you choose to create...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/guide/02_create.html",
        "teaser":null},{
        "title": "03_build",
        
        "excerpt":
            "Building and publishing your book Once you’ve added content and configured your book, it’s time to build the raw material that Jekyll will use to turn your book into a website. We’ll also cover how to turn this book into the HTML for a website that can be served online. Build the book’s markdown Now that you’ve got the files installed content is in the book, you can build your book. Build your book by running the following command: jupyter-book build mybookname/ This will: Use the links specified in the _data/toc.yml file (pointing to files in /content/) and do the...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/guide/03_build.html",
        "teaser":null},{
        "title": "04_faq",
        
        "excerpt":
            "The following are some common issues and questions that have arisen when building your textbook with Jekyll. How can I update my book? Sometimes Jupyter Book will get updates that you want to incorporate into a book you’ve already built. The easiest way to do this is to use the Command-Line Interface to upgrade your book. To upgrade a pre-existing Jupyter Book, run the following command: jupyter-book upgrade path/to/mybook This will do the following: Generate a fresh Jupyter Book in mybook_UPGRADED using the content files in your current book. If this succeeds, copy over the contents of mybook_UPGRADED into your...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/guide/04_faq.html",
        "teaser":null},{
        "title": "05_advanced",
        
        "excerpt":
            "This page contains more advanced and complete information about the jupyter-book repository. See the sections below. Hide cells or cell outputs in the built site Sometimes you want to use code to generate visualizations or prepare data, but you don’t want users to see it in your built book. To prevent code cells from showing up in your built site, you can use the following two configuration options in your _config.yml file. To remove entire code cells from your book, use the following configuration: hide_cell_text : \"YOUR TEXT\" Any code cell with the value of hide_cell_text (above it is YOUR...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/guide/05_advanced.html",
        "teaser":null},{
        "title": "guide",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/guide/index.html",
        "teaser":null},{
        "title": "intro",
        
        "excerpt":
            "Books with Jupyter and Jekyll Jupyter Books lets you build an online book using a collection of Jupyter Notebooks and Markdown files. Its output is similar to the excellent Bookdown tool, and adds extra functionality for people running a Jupyter stack. For an example of a book built with Jupyter Books, see the textbook for Data 100 at UC Berkeley (or this website!) Here are a few features of Jupyter Books A Command-Line Interface (CLI) to quickly create, build, and upgrade books. Write book content in markdown and Jupyter Notebooks Convert these into Jekyll pages that can be hosted for...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/intro.html",
        "teaser":null},{
        "title": "jupyter-book-start",
        
        "excerpt":
            "1 jupyter-book create books –demo   2 修改 books/_config.yml 中的content_folder_name 为”../content”   3   ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/jupyter-book-start.html",
        "teaser":null},{
        "title": "math",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/math/index.html",
        "teaser":null},{
        "title": "主成分分析",
        
        "excerpt":
            "title: 数学基础-主成分分析 author: niult date: 2019-01-16 category: 数学基础 tags: python,numpy,数学基础 PCA简介 相关背景 上完陈恩红老师的《机器学习与知识发现》和季海波老师的《矩阵代数》两门课之后，颇有体会。最近在做主成分分析和奇异值分解方面的项目，所以记录一下心得体会。 在许多领域的研究与应用中，往往需要对反映事物的多个变量进行大量的观测，收集大量数据以便进行分析寻找规律。多变量大样本无疑会为研究和应用提供了丰富的信息，但也在一定程度上增加了数据采集的工作量，更重要的是在多数情况下，许多变量之间可能存在相关性，从而增加了问题分析的复杂性，同时对分析带来不便。如果分别对每个指标进行分析，分析往往是孤立的，而不是综合的。盲目减少指标会损失很多信息，容易产生错误的结论。 因此需要找到一个合理的方法，在减少需要分析的指标同时，尽量减少原指标包含信息的损失，以达到对所收集数据进行全面分析的目的。由于各变量间存在一定的相关关系，因此有可能用较少的综合指标分别综合存在于各变量中的各类信息。主成分分析与因子分析就属于这类降维的方法。 参考文献 问题描述 下表1是某些学生的语文、数学、物理、化学成绩统计： 首先，假设这些科目成绩不相关，也就是说某一科目考多少分与其他科目没有关系。那么一眼就能看出来，数学、物理、化学这三门课的成绩构成了这组数据的主成分（很显然，数学作为第一主成分，因为数学成绩拉的最开）。为什么一眼能看出来？因为坐标轴选对了！下面再看一组学生的数学、物理、化学、语文、历史、英语成绩统计，见表2，还能不能一眼看出来： 数据太多了，以至于看起来有些凌乱！也就是说，无法直接看出这组数据的主成分，因为在坐标系下这组数据分布的很散乱。究其原因，是因为无法拨开遮住肉眼的迷雾~如果把这些数据在相应的空间中表示出来，也许你就能换一个观察角度找出主成分。如下图1所示： 但是，对于更高维的数据，能想象其分布吗？就算能描述分布，如何精确地找到这些主成分的轴？如何衡量你提取的主成分到底占了整个数据的多少信息？所以，我们就要用到主成分分析的处理方法。 数据降维 为了说明什么是数据的主成分，先从数据降维说起。数据降维是怎么回事儿？假设三维空间中有一系列点，这些点分布在一个过原点的斜面上，如果你用自然坐标系x,y,z这三个轴来表示这组数据的话，需要使用三个维度，而事实上，这些点的分布仅仅是在一个二维的平面上，那么，问题出在哪里？如果你再仔细想想，能不能把x,y,z坐标系旋转一下，使数据所在平面与x,y平面重合？这就对了！如果把旋转后的坐标系记为x’,y’,z’，那么这组数据的表示只用x’和y’两个维度表示即可！当然了，如果想恢复原来的表示方式，那就得把这两个坐标之间的变换矩阵存下来。这样就能把数据维度降下来了！但是，我们要看到这个过程的本质，如果把这些数据按行或者按列排成一个矩阵，那么这个矩阵的秩就是2！这些数据之间是有相关性的，这些数据构成的过原点的向量的最大线性无关组包含2个向量，这就是为什么一开始就假设平面过原点的原因！那么如果平面不过原点呢？这就是数据中心化的缘故！将坐标原点平移到数据中心，这样原本不相关的数据在这个新坐标系中就有相关性了！有趣的是，三点一定共面，也就是说三维空间中任意三点中心化后都是线性相关的，一般来讲n维空间中的n个点一定能在一个n-1维子空间中分析！ 上一段文字中，认为把数据降维后并没有丢弃任何东西，因为这些数据在平面以外的第三个维度的分量都为0。现在，假设这些数据在z’轴有一个很小的抖动，那么我们仍然用上述的二维表示这些数据，理由是我们可以认为这两个轴的信息是数据的主成分，而这些信息对于我们的分析已经足够了，z’轴上的抖动很有可能是噪声，也就是说本来这组数据是有相关性的，噪声的引入，导致了数据不完全相关，但是，这些数据在z’轴上的分布与原点构成的夹角非常小，也就是说在z’轴上有很大的相关性，综合这些考虑，就可以认为数据在x’,y’ 轴上的投影构成了数据的主成分！ 课堂上老师谈到的特征选择的问题，其实就是要剔除的特征主要是和类标签无关的特征。而这里的特征很多是和类标签有关的，但里面存在噪声或者冗余。在这种情况下，需要一种特征降维的方法来减少特征数，减少噪音和冗余，减少过度拟合的可能性。 PCA的思想是将n维特征映射到k维上（k&lt;n），这k维是全新的正交特征。这k维特征称为主成分，是重新构造出来的k维特征，而不是简单地从n维特征中去除其余n-k维特征。 PCA实例 现在假设有一组数据如下： 行代表了样例，列代表特征，这里有10个样例，每个样例两个特征。可以这样认为，有10篇文档，x是10篇文档中“learn”出现的TF-IDF，y是10篇文档中“study”出现的TF-IDF。 第一步，分别求x和y的平均值，然后对于所有的样例，都减去对应的均值。这里x的均值是1.81，y的均值是1.91，那么一个样例减去均值后即为（0.69,0.49），得到 第二步，求特征协方差矩阵，如果数据是3维，那么协方差矩阵是 这里只有$x$和$y$，求解得 对角线上分别是$x$和$y$的方差，非对角线上是协方差。协方差是衡量两个变量同时变化的变化程度。协方差大于0表示$x$和$y$若一个增，另一个也增；小于0表示一个增，一个减。如果$ｘ$和$ｙ$是统计独立的，那么二者之间的协方差就是０；但是协方差是０，并不能说明$ｘ$和$ｙ$是独立的。协方差绝对值越大，两者对彼此的影响越大，反之越小。协方差是没有单位的量，因此，如果同样的两个变量所采用的量纲发生变化，它们的协方差也会产生树枝上的变化。 第三步，求协方差的特征值和特征向量，得到 上面是两个特征值，下面是对应的特征向量，特征值$0.0491$对应特征向量为 $(-0.7352, 0.6779)^T$ )，这里的特征向量都归一化为单位向量。 第四步，将特征值按照从大到小的顺序排序，选择其中最大的$k$个，然后将其对应的$k$个特征向量分别作为列向量组成特征向量矩阵。 这里特征值只有两个，我们选择其中最大的那个，这里是1.2840 ，对应的特征向量是$(-0.6779, -0.7352)^T$。 第五步，将样本点投影到选取的特征向量上。假设样例数为m，特征数为n，减去均值后的样本矩阵为DataAdjust(m*n)，协方差矩阵是n*n，选取的k个特征向量组成的矩阵为EigenVectors(n*k)。那么投影后的数据FinalData为 FinalData(10*1) = DataAdjust(10*2矩阵) x 特征向量(-0.677873399, -0.735178656)T 得到的结果是 这样，就将原始样例的n维特征变成了k维，这k维就是原始特征在k维上的投影。 上面的数据可以认为是learn和study特征融合为一个新的特征叫做LS特征，该特征基本上代表了这两个特征。上述过程如下图2描述： 正号表示预处理后的样本点，斜着的两条线就分别是正交的特征向量（由于协方差矩阵是对称的，因此其特征向量正交），最后一步的矩阵乘法就是将原始样本点分别往特征向量对应的轴上做投影。 整个PCA过程貌似及其简单，就是求协方差的特征值和特征向量，然后做数据转换。但是有没有觉得很神奇，为什么求协方差的特征向量就是最理想的k维向量？其背后隐藏的意义是什么？整个PCA的意义是什么？ PCA推导 先看下面这幅图： 在第一部分中，我们举了一个学生成绩的例子，里面的数据点是六维的，即每个观测值是6维空间中的一个点。我们希望将6维空间用低维空间表示。 先假定只有二维，即只有两个变量，它们由横坐标和纵坐标所代表；因此每个观测值都有相应于这两个坐标轴的两个坐标值；如果这些数据形成一个椭圆形状的点阵，那么这个椭圆有一个长轴和一个短轴。在短轴方向上，数据变化很少；在极端的情况，短轴如果退化成一点，那只有在长轴的方向才能够解释这些点的变化了；这样，由二维到一维的降维就自然完成了。 上图中，u1就是主成分方向，然后在二维空间中取和u1方向正交的方向，就是u2的方向。则n个数据在u1轴的离散程度最大（方差最大），数据在u1上的投影代表了原始数据的绝大部分信息，即使不考虑u2，信息损失也不多。而且，u1、u2不相关。只考虑u1时，二维降为一维。 椭圆的长短轴相差得越大，降维也越有道理。 最大方差理论 在信号处理中认为信号具有较大的方差，噪声有较小的方差，信噪比就是信号与噪声的方差比，越大越好。如前面的图，样本在u1上的投影方差较大，在u2上的投影方差较小，那么可认为u2上的投影是由噪声引起的。 因此我们认为，最好的k维特征是将n维样本点转换为k维后，每一维上的样本方差都很大。 比如我们将下图中的5个点投影到某一维上，这里用一条过原点的直线表示（数据已经中心化）： 假设我们选择两条不同的直线做投影，那么左右两条中哪个好呢？根据我们之前的方差最大化理论，左边的好，因为投影后的样本点之间方差最大（也可以说是投影的绝对值之和最大）。 计算投影的方法见下图5： 图中，红色点表示样例，蓝色点表示在u上的投影，u是直线的斜率也是直线的方向向量，而且是单位向量。蓝色点是在u上的投影点，离原点的距离是&lt;x,u&gt;（即xTu或者uTx）。 最小二乘法 我们使用最小二乘法来确定各个主轴（主成分）的方向。 对给定的一组数据（下面的阐述中，向量一般均指列向量）： 其数据中心位于: 数据中心化（将坐标原点移到样本点的中心点）： 中心化后的数据在第一主轴u1方向上分布散的最开，也就是说在u1方向上的投影的绝对值之和最大（也可以说方差最大），计算投影的方法上面已经阐述，就是将x与u1做内积，由于只需要求u1的方向，所以设u1也是单位向量。 在这里，也就是最大化下式： 由矩阵代数相关知识可知，可以对绝对值符号项进行平方处理，比较方便。所以进而就是最大化下式： 两个向量做内积，可以转化成矩阵乘法： 所以目标函数可以表示为： 括号里面就是矩阵乘法表示向量内积，由于列向量转置以后是行向量，行向量乘以列向量得到一个数，一个数的转置还是其本身，所以又可以将目标函数化为： 其中的${\\boldsymbol{u_1}}^T XX^T {\\boldsymbol{u_1}}$就是一个二次型， 我们假设 $XX^T$ 的某一特征值为$\\lambda$，对应的特征向量为$\\xi $，有 所以 $\\lambda \\ge 0$ ，$XX^T$ 是半正定的对称矩阵，即 $u_1XX^Tu_1$ 是半正定阵的二次型，由矩阵代数知识得出，目标函数存在最大值！ 下面我们求解最大值、取得最大值时u1的方向这两个问题。 先解决第一个问题，对于向量x的二范数平方为: 同样，目标函数也可以表示成映射后的向量的二范数平方： 二次型化成一个范数的形式，由于u1取单位向量，最大化目标函数的基本问题也就转化为：对一个矩阵，它对一个向量做变换，变换前后的向量的模长伸缩尺度如何才能最大？我们有矩阵代数中的定理知，向量经矩阵映射前后的向量长度之比的最大值就是这个矩阵的最大奇异值，即： 式中，是矩阵A的最大奇异值（亦是矩阵A的二范数），它等于$AA^T$（或$A^TA$）的最大特征值开平方。 针对本问题来说，$XX^T$是半正定对称阵，也就意味着它的特征值都大于等于0，且不同特征值对应的特征向量是正交的，构成所在空间的一组单位正交基。...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/math/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90.html",
        "teaser":null},{
        "title": "最小二乘法",
        
        "excerpt":
            "   title: 最小二乘法   author: niult   date: 2019-01-16   category: 数学基础   tags: python,numpy,数学基础   最小二乘法   我们从矩阵的角度来理解：  首先我们给出一个矩阵中的定义：     有了上面的定义之后，我们就可以写出最小二乘问题的矩阵形式：     用bi格高一点的说法来说，就是求在欧几里得空间中以2-范数作为距离，使得向量Ax与b之间距离最小的x。  我们的目标是求：     当然我们知道，使得距离最小的向量x与使得距离平方最小的向量x是相同的，于是我们可以将所求的目标改写为：     结合一些矩阵、行列式的知识，我们知道：     根据我们大一学过的高数知识，我们知道，求最极值问题直接对应的就是导数为零，因此我们试图将所给出的原式的矩阵形式求导：   不过首先我们需要补充矩阵微积分(matrix calculus)的一些知识  (PS:是矩阵微积分吧…我没有翻译错吧….)       如果矩阵A是对称的(symmetric matrix):     接下来，我们对原式化简并求其对x的导数：     求导得到：     于是我们就得到了，最小二乘法解的矩阵形式：     当然了，这里是最简答的线性最小二乘法，还有更为复杂的非线性以及矩阵A不满秩的情况(hdq说他老师能默写出这个过程…)，等以后有时间了，我会再补充上去的。           x=1           ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/math/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95.html",
        "teaser":null},{
        "title": "矩阵求导",
        
        "excerpt":
            "   title: 矩阵求导   author: niult   date: 2019-01-16   category: 数学基础   tags: python,numpy,数学基础   简介  第一次遇见矩阵求导，大多数人都是一头雾水，而搜了维基百科看也还是云里雾里，一堆的名词和一堆的表格到底都是什么呢？这里总结了我个人的学习经验，并且通过一个例子可以让你感受如何进行矩阵求导，下次再遇到需要进行矩阵求导的地方就不会措手不及。   在进行概念的解说之前，首先大家需要先知道下面的这个前提：   https://en.wikipedia.org/wiki/Matrix_calculus   基本分类   前提  若$x$为向量，则默认$\\boldsymbol x$为列向量，$\\boldsymbol x^T$为行向量 若$y$为向量，则默认$\\boldsymbol y$为列向量，$\\boldsymbol y^T$为行向量   标量/向量       向量/标量       向量/向量   1 $\\frac {\\partial\\boldsymbol y^T}{\\partial\\boldsymbol x}$按照$\\boldsymbol x$展开     2 $\\frac {\\partial\\boldsymbol y^T}{\\partial\\boldsymbol x}$按照$\\boldsymbol y$展开     3 $\\frac {\\partial\\boldsymbol y}{\\partial\\boldsymbol x^T}$按照$\\boldsymbol x$展开     4 $\\frac {\\partial\\boldsymbol y}{\\partial\\boldsymbol x^T}$按照$\\boldsymbol y$展开     标量/矩阵     矩阵/标量     #一个求导的例子   问题     说明： $y$、$w$为列向量，$X$为矩阵 ##式子演化   看到这个例子不要急着去查表求导，先看看它的形式，是$u(w)∗v(w)$的形式，这种形式一般求导较为复杂，因此为了简化运算，我们先把式子展开成下面的样子（注意：$(Xw)^T=w^TX^T$）：     然后就可以写成四个部分求导的形式如下（累加后求导=求导后累加）：     ##求导   $\\frac{\\partial y^Ty}{\\partial w}$求导 :     说明：分子部分为标量，分母部分为向量，找到维基百科中的Scalar-by-vector identities表格，在表格中匹配形式到第1行的位置，因为分母为列向量，因此为分母布局，对应的求导结果就是 0 。   $\\frac{\\partial y^TXw}{\\partial w}$求导 :     说明：同样的，在维基百科中的Scalar-by-vector identities表格，在表格中匹配形式到第11行的位置，对应的求导结果就是 $X^Ty$ 。   $\\frac{\\partial w^TX^Ty}{\\partial w}$求导 :     说明：因为分子为标量，标量的转置等于本身，所以对分子进行转置操作，其等价于第二部分。   $\\frac{\\partial w^TX^TXw}{\\partial w}$求导 :     说明：同样的，在维基百科中的Scalar-by-vector identities表格，在表格中匹配形式到第13行的位置，矩阵的转置乘上本身（$X^TX$）为对称矩阵当做表格中的A ，所以得到求导结果 $2X^TXw$ 。   ##整合   把四个部分求导结果进行相应的加减就可以得到最终的结果：     现在你再看看维基百科里那成堆的表格，是不是觉得异常实用了！                   x=1           ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/math/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC.html",
        "teaser":null},{
        "title": "JupyterNotebookTips",
        
        "excerpt":
            "Jupyter notebook, 前身是 IPython notebook, 它是一个非常灵活的工具，有助于帮助你构建很多可读的分析，你可以在里面同时保留代码，图片，评论，公式和绘制的图像。 Jupyter具有非常强的可扩展性，支持很多编程语言，并且易于部署到你的个人电脑和几乎所有的服务器上 – 你只需要使用ssh或http接入即可。最重要的是，它完全免费。 Jupyter默认设置使用 Python kernel，正因此以前叫做 IPython notebook. Jupyter notebook 源自于 Jupyter 项目, Jupyter这个名字是它被设计所支持三个核心编程语言的缩写词：JUlia,PYThon, 和 R, 启发自木星这个词：Jupiter. 接下来的内容将向你展示27个让 Jupyter 用的更加舒心的建议与技巧。 1. Keyboard Shortcuts 每一个进阶用户都知道，键盘快捷键将会为我们节省许多时间。Jupyter在顶部的菜单里保留了许多快捷键：Help &gt; keyboard Shortcuts. 每次更新Jupyter时，都值得再次进行查看，因为新的快捷键总是不断被添加进来。 另一个查看快捷键的方式是使用命令面板：Cmd + Shift + P(或者Linux和Windows上 Ctrl + Shift + P)。这个对话框将会帮助你通过名称运行任何命令 – 这非常有用，尤其当你不知道一个命令的快捷键或者你想要执行的命令没有快捷键时。这个功能非常类似与Mac上的Spotlight搜索，一旦你开始使用这个功能，你就会发现没有它的日子该怎么办！ 这里是一些我喜欢的快捷键： Esc + F 查找和替换你的代码，但不包括代码的输出内容。 Esc + o 打开代码块输出。 选择多个 cell。 Shift + J 或 Shift + Down 向下选中下一个cell. 你可以通过 Shift + K 或 Shift + Up 向上选中 cell。(译者：jk，与vim的移动方式一致) 一旦 cell 被选中，接着你可以进行批量删除/复制/剪切/粘贴.当你需要移动一部分notebook时，这非常有用。 你也可以执行 Shift + M (译者：m记为merge)对多个cell进行合并。 2. Pretty Display of Varibles 这部分内容可能很多人都知道。如果对带有一个变量或是未赋值语句的cell执行操作，Jupyter 将会自动打印该变量而无需一个输出语句。这非常有用，尤其是使用 Pandas DataFrames 进行处理时，因为输出将会被整齐地格式化为一个表格。 接下来的内容可能没那么人知道：你可以选择修改 ast_note_iteractively kernal 选项来使得 Jupyter 为每一行的变量或语句执行这个操作，以便你可以立即看到多条语句一起输出。 line1...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/notebooks/JupyterNotebookTips.html",
        "teaser":null},{
        "title": "LinearRegression",
        
        "excerpt":
            "%matplotlib inline import matplotlib.pyplot as plt import numpy as np from sklearn import datasets, linear_model # Load the diabetes dataset diabetes = datasets.load_diabetes() # Use only one feature diabetes_X = diabetes.data[:, np.newaxis, 2] # Split the data into training/testing sets diabetes_X_train = diabetes_X[:-20] diabetes_X_test = diabetes_X[-20:] # Split the targets into training/testing sets diabetes_y_train = diabetes.target[:-20] diabetes_y_test = diabetes.target[-20:] # Create linear regression object regr = linear_model.LinearRegression() # Train the model using the training sets regr.fit(diabetes_X_train, diabetes_y_train) # The coefficients print('Coefficients: \\n', regr.coef_) # The mean square error print(\"Residual sum of squares: %.2f\" % np.mean((regr.predict(diabetes_X_test) - diabetes_y_test) ** 2)) #...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/notebooks/LinearRegression.html",
        "teaser":null},{
        "title": "notebooks",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/notebooks/index.html",
        "teaser":null},{
        "title": "notechats",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/notechats/index.html",
        "teaser":null},{
        "title": "notedata测试数据",
        
        "excerpt":
            "安装 !pip install --upgrade notedata Collecting notedata Downloading https://files.pythonhosted.org/packages/c2/de/89b69ae2bdf9d0bad1c0383de09dc995ad71e622686fa1ca8bbc5d29141c/notedata-0.0.9-py3-none-any.whl Installing collected packages: notedata Found existing installation: notedata 0.0.8 Uninstalling notedata-0.0.8: Successfully uninstalled notedata-0.0.8 Successfully installed notedata-0.0.9 import logging #logging.basicConfig(level=logging.INFO) logging.basicConfig(format=\"%(asctime)s %(name)s:%(levelname)s:%(message)s\", datefmt=\"%Y-%M-%d %H:%M:%S\", level=logging.INFO) 数据集 adult_data import notedata.notedata.dataset.data as data train_x, train_y, test_x, test_y = data.get_adult_data() print('train_x\\t{}\\ntrain_y\\t{}\\ntest_x\\t{}\\ntest_y\\t{}\\n'.format(len(train_x),len(train_y),len(test_x),len(test_y))) train_x 32561 train_y 32561 test_x 16281 test_y 16281 reviews_Electronics数据 from notedata.notedata.dataset.ElectronicsData import ElectronicsData import notedata.notedata.utils as utils import logging #logging.basicConfig(level=logging.INFO) logging.basicConfig(format=\"%(asctime)s %(name)s:%(levelname)s:%(message)s\", datefmt=\"%Y-%M-%d %H:%M:%S\", level=logging.INFO) ele = ElectronicsData() ele.init_data() porto_seguro数据 import notedata.notedata.dataset.data as data data.get_porto_seguro_data() ele.pkl_dataset '/content/tmp//data/electronics//raw_data/dataset.pkl' import pandas as pd d1 = pd.read_csv('/content/tmp///data/porto_seguro_train.csv') d1.columns Index(['id', 'target', 'ps_ind_01', 'ps_ind_02_cat', 'ps_ind_03', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_ind_06_bin',...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/notechats/notedata%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE.html",
        "teaser":null},{
        "title": "folium",
        
        "excerpt":
            "        import folium from folium.features import DivIcon  m = folium.Map(     location=[31, 110],     zoom_start=5,     #tiles='Mapbox Bright' )  d5=[] for i in range(4,9):     for j in range(21,28):         d5.append((i,j,str(i)+'_'+str(j)))  #d5 = [(6,23,'1_2')]  for line in d5:          p1 = [float(line[0])*5, float(line[1])*5]     folium.Marker(p1, icon=DivIcon(             icon_size=(150,36),             icon_anchor=(7,20),             html='&lt;div style=\"font-size: 15pt; color : black\"&gt;'+line[2]+'&lt;/div&gt;',             )).add_to(m)     m.add_child(folium.CircleMarker(p1, radius=15)) m                                      ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/package/folium.html",
        "teaser":null},{
        "title": "graphviz之安装使用",
        
        "excerpt":
            "安装 !conda install python-graphviz from graphviz import Digraph dot = Digraph(comment='The Test Table') # 添加圆点A,A的标签是Dot A dot.node('A', 'Dot A') # 添加圆点 B, B的标签是Dot B dot.node('B', 'Dot B') # dot.view() # 添加圆点 C, C的标签是Dot C dot.node('C', 'Dot C') # dot.view() # 创建一堆边，即连接AB的两条边，连接AC的一条边。 dot.edges(['AB', 'AC', 'AB']) # dot.view() # 在创建两圆点之间创建一条边 dot.edge('B', 'C', 'test') # dot.view() # 获取DOT source源码的字符串形式 # print(dot.source) # 保存source到文件，并提供Graphviz引擎 # dot.render('test-table.gv', view=True) dot Graphviz实例 开始Graphviz # 导入模块 import graphviz as gv # 创建图对象g1和两个节点A、B，并连接A、B g1 = gv.Graph(format='png') g1.node('A') g1.node('B') g1.edge('A', 'B') # 生成PNG图片 # filename = g1.render(filename='img/p0204_1') g1 有向图 g2 = gv.Digraph(format='png') g2.node('A') g2.node('B') g2.edge('A', 'B') g2 进阶 import functools...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/package/graphviz%E4%B9%8B%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8.html",
        "teaser":null},{
        "title": "package",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/package/index.html",
        "teaser":null},{
        "title": "pygithub",
        
        "excerpt":
            "!pip install pyGithub Requirement already satisfied: pyGithub in /Users/weidian/.local/share/virtualenvs/env-_W_pTsr3/lib/python3.7/site-packages (1.43.7) Requirement already satisfied: pyjwt in /Users/weidian/.local/share/virtualenvs/env-_W_pTsr3/lib/python3.7/site-packages (from pyGithub) (1.7.1) Requirement already satisfied: deprecated in /Users/weidian/.local/share/virtualenvs/env-_W_pTsr3/lib/python3.7/site-packages (from pyGithub) (1.2.5) Requirement already satisfied: requests&gt;=2.14.0 in /Users/weidian/.local/share/virtualenvs/env-_W_pTsr3/lib/python3.7/site-packages (from pyGithub) (2.21.0) Requirement already satisfied: wrapt&lt;2,&gt;=1 in /Users/weidian/.local/share/virtualenvs/env-_W_pTsr3/lib/python3.7/site-packages (from deprecated-&gt;pyGithub) (1.11.1) Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /Users/weidian/.local/share/virtualenvs/env-_W_pTsr3/lib/python3.7/site-packages (from requests&gt;=2.14.0-&gt;pyGithub) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /Users/weidian/.local/share/virtualenvs/env-_W_pTsr3/lib/python3.7/site-packages (from requests&gt;=2.14.0-&gt;pyGithub) (2019.3.9) Requirement already satisfied: idna&lt;2.9,&gt;=2.5 in /Users/weidian/.local/share/virtualenvs/env-_W_pTsr3/lib/python3.7/site-packages (from requests&gt;=2.14.0-&gt;pyGithub) (2.8) Requirement already satisfied: urllib3&lt;1.25,&gt;=1.21.1 in /Users/weidian/.local/share/virtualenvs/env-_W_pTsr3/lib/python3.7/site-packages (from requests&gt;=2.14.0-&gt;pyGithub) (1.24.3) from github import Github as git # or using an access token g = git(\"55197bb4f051ce544228e40e544b2ddd6be8b3ac\") # Github Enterprise...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/package/pygithub.html",
        "teaser":null},{
        "title": "word2vec",
        
        "excerpt":
            "word2vec This notebook is equivalent to demo-word.sh, demo-analogy.sh, demo-phrases.sh and demo-classes.sh from Google. %load_ext autoreload %autoreload 2 Training Download some data, for example: http://mattmahoney.net/dc/text8.zip import word2vec data_root = 'tmp/nlp/word2vec/' Run word2phrase to group up similar words “Los Angeles” to “Los_Angeles” word2vec.word2phrase(data_root + 'text8', data_root + 'text8-phrases', verbose=True) Starting training using file tmp/nlp/word2vec/text8 Words processed: 17000K Vocab size: 4399K Vocab size (unigrams + bigrams): 2419827 Words in train file: 17005206 Words written: 17000K This created a text8-phrases file that we can use as a better input for word2vec. Note that you could easily skip this previous step and use the text...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/package/word2vec.html",
        "teaser":null},{
        "title": "向量引擎faiss",
        
        "excerpt":
            "安装 #安装cpu版本 #更新conda conda update conda #先安装mkl conda install mkl #安装faiss-cpu conda install faiss-cpu -c pytorch #测试安装是否成功 python -c \"import faiss\" Quick Start 创建数据 这里先给出官方提供的demo来感受一下faiss的使用。 首先构建训练数据和测试数据 import numpy as np d = 64 # dimension nb = 100000 # database size nq = 10000 # nb of queries np.random.seed(1234) # make reproducible xb = np.random.random((nb, d)).astype('float32') xb[:, 0] += np.arange(nb) / 1000. xq = np.random.random((nq, d)).astype('float32') xq[:, 0] += np.arange(nq) / 1000. # 上面代码中，我们定义返回每个需要查询向量的最近4个向量。 # 查询返回两个numpy array对象D和I。D表示与相似向量的距离(distance)，I表示相似对象的ID。 def verify(index): k = 4 D, I = index.search(xq, k) print(I[:5]) print(D[:5]) xq 上面我们构建了shape为[100000,64]的训练数据xb，和shape为[10000,64]的查询数据xq。 然后创建索引(Index)。faiss创建索引对向量预处理，提高查询效率。 faiss提供多种索引方法，这里选择最简单的暴力检索L2距离的索引：IndexFlatL2。 创建索引时必须指定向量的维度d。大部分索引需要训练的步骤。IndexFlatL2跳过这一步。 当索引创建好并训练(如果需要)之后，我们就可以执行add和search方法了。add方法一般添加训练时的样本，search就是寻找相似相似向量了。 一些索引可以保存整型的ID，每个向量可以指定一个ID，当查询相似向量时，会返回相似向量的ID及相似度(或距离)。如果不指定，将按照添加的顺序从0开始累加。其中IndexFlatL2不支持指定ID。 IndexFlatL2-暴力检索 import faiss index...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/package/%E5%90%91%E9%87%8F%E5%BC%95%E6%93%8Efaiss.html",
        "teaser":null},{
        "title": "词向量训练",
        
        "excerpt":
            "          # http://mattmahoney.net/dc/text8.zip                    import word2vec  data_root = 'tmp/nlp/word2vec/'  word2vec.word2phrase(data_root + 'text8', data_root + 'text8-phrases', verbose=True)            ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/package/%E8%AF%8D%E5%90%91%E9%87%8F%E8%AE%AD%E7%BB%83.html",
        "teaser":null},{
        "title": "Atrank",
        
        "excerpt":
            "ATrank 论文链接 中文链接 github地址 摘要：本文提出一种基于注意力机制的用户异构行为序列的建模框架，并将其应用到推荐场景中。我们将不同种类的用户行为序列进行分组编码，并映射到不同子空间中。我们利用 self-attention 对行为间的互相影响进行建模。最终我们得到用户的行为表征，下游任务就可以使用基本的注意力模型进行有更具指向性的决策。我们尝试用同一种模型同时预测多种类型的用户行为，使其达到多个单独模型预测单类型行为的效果。另外，由于我们的方法中没有使用 RNN,CNN 等方法，因此在提高效果的同时，该方法能够有更快的训练速度。 下载数据 shell下载数据并解压，数据分为两部分 # 下载训练数据 # cd ../raw_data # wget -c http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz # gzip -d reviews_Electronics_5.json.gz # wget -c http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/meta_Electronics.json.gz # gzip -d meta_Electronics.json.gz !pwd !wget -P tmp/datas/paper/atrank -c http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz !gzip -d tmp/datas/paper/atrank/reviews_Electronics_5.json.gz !wget -P tmp/datas/paper/atrank -c http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/meta_Electronics.json.gz !gzip -d tmp/datas/paper/atrank/meta_Electronics.json.gz /Users/weidian/Documents/MyDiary --2018-10-20 17:12:12-- http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz 正在解析主机 snap.stanford.edu (snap.stanford.edu)... 171.64.75.80 正在连接 snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:80... 已连接。 已发出 HTTP 请求，正在等待回应... 200 OK 长度：495854086 (473M) [application/x-gzip] 正在保存至: “tmp/datas/paper/atrank/reviews_Electronics_5.json.gz” reviews_El 0%[ ] 104.86K 6.99KB/s 剩余 43h 14m ^C tmp/datas/paper/atrank/reviews_Electronics_5.json already exists -- do you wish to overwrite (y or n)? ^C --2018-10-20 17:12:54-- http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/meta_Electronics.json.gz 正在解析主机 snap.stanford.edu (snap.stanford.edu)... 171.64.75.80...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/paper/atrank/Atrank.html",
        "teaser":null},{
        "title": "Attention",
        
        "excerpt":
            "序列编码 2017年中，有两篇类似同时也是笔者非常欣赏的论文，分别是FaceBook的《Convolutional Sequence to Sequence Learning》和Google的《Attention is All You Need》，它们都算是Seq2Seq上的创新，本质上来说，都是抛弃了RNN结构来做Seq2Seq任务。 这篇博文中，笔者对《Attention is All You Need》做一点简单的分析。当然，这两篇论文本身就比较火，因此网上已经有很多解读了（不过很多解读都是直接翻译论文的，鲜有自己的理解），因此这里尽可能多自己的文字，尽量不重复网上各位大佬已经说过的内容。 深度学习做NLP的方法，基本上都是先将句子分词，然后每个词转化为对应的词向量序列。这样一来，每个句子都对应的是一个矩阵$\\boldsymbol{X}=(\\boldsymbol{x}_1,\\boldsymbol{x}_2,\\dots,\\boldsymbol{x}_t)$，其中$\\boldsymbol{x}_i$都代表着第$i$个词的词向量（行向量），维度为$d$维，故$\\boldsymbol{X}\\in \\mathbb{R}^{n\\times d}$。这样的话，问题就变成了编码这些序列了。 第一个基本的思路是RNN层，RNN的方案很简单，递归式进行： 不管是已经被广泛使用的LSTM、GRU还是最近的SRU，都并未脱离这个递归框架。RNN结构本身比较简单，也很适合序列建模，但RNN的明显缺点之一就是无法并行，因此速度较慢，这是递归的天然缺陷。另外我个人觉得RNN无法很好地学习到全局的结构信息，因为它本质是一个马尔科夫决策过程。 第二个思路是CNN层，其实CNN的方案也是很自然的，窗口式遍历，比如尺寸为3的卷积，就是 在FaceBook的论文中，纯粹使用卷积也完成了Seq2Seq的学习，是卷积的一个精致且极致的使用案例，热衷卷积的读者必须得好好读读这篇文论。CNN方便并行，而且容易捕捉到一些全局的结构信息，笔者本身是比较偏爱CNN的，在目前的工作或竞赛模型中，我都已经尽量用CNN来代替已有的RNN模型了，并形成了自己的一套使用经验，这部分我们以后再谈。 Google的大作提供了第三个思路：纯Attention！单靠注意力就可以！RNN要逐步递归才能获得全局信息，因此一般要双向RNN才比较好；CNN事实上只能获取局部信息，是通过层叠来增大感受野；Attention的思路最为粗暴，它一步到位获取了全局信息！它的解决方案是： 其中$\\boldsymbol{A},\\boldsymbol{B}$是另外一个序列（矩阵）。如果都取$\\boldsymbol{A}=\\boldsymbol{B}=\\boldsymbol{X}$，那么就称为Self Attention，它的意思是直接将$\\boldsymbol{x}_t$与原来的每个词进行比较，最后算出$\\boldsymbol{y}_t$！ Attention层 Attention定义 Google的一般化Attention思路也是一个编码序列的方案，因此我们也可以认为它跟RNN、CNN一样，都是一个序列编码的层。 变量 符号 空间 含义 query $\\boldsymbol{Q}$ $ \\mathbb{R}^{n\\times d_k}$ $n$个query，每个query用$d_k$维向量表示 key $\\boldsymbol{K}$ $ \\mathbb{R}^{m\\times d_k}$ $m$个 key ，每个 key 用$d_k$维向量表示 value $\\boldsymbol{V}$ $ \\mathbb{R}^{m\\times d_v}$ $m$个value，每个value用$d_v$维向量表示 前面给出的是一般化的框架形式的描述，事实上Google给出的方案是很具体的。首先，它先把Attention的定义给了出来： 这里用的是跟Google的论文一致的符号，其中$\\boldsymbol{Q}\\in\\mathbb{R}^{n\\times d_k}, \\boldsymbol{K}\\in\\mathbb{R}^{m\\times d_k}, \\boldsymbol{V}\\in\\mathbb{R}^{m\\times d_v}$。如果忽略激活函数$softmax$的话，那么事实上它就是三个$n\\times d_k,d_k\\times m, m\\times d_v$的矩阵相乘，最后的结果就是一个$n\\times d_v$的矩阵。于是我们可以认为：这是一个Attention层，将$n\\times d_k$的序列$\\boldsymbol{Q}$编码成了一个新的$n\\times d_v$的序列。 那怎么理解这种结构呢？我们不妨逐个向量来看。 其中$Z$是归一化因子。事实上$q,k,v$分别是$query,key,value$的简写，$\\boldsymbol{K},\\boldsymbol{V}$是一一对应的，它们就像是key-value的关系，那么上式的意思就是通过$\\boldsymbol{q}_t$这个query，通过与各个$\\boldsymbol{k}_s$内积的并softmax的方式，来得到$\\boldsymbol{q}_t$与各个$\\boldsymbol{v}_s$的相似度，然后加权求和，得到一个$d_v$维的向量。其中因子$\\sqrt{d_k}$起到调节作用，使得内积不至于太大（太大的话softmax后就非0即1了，不够“soft”了）。 事实上这种Attention的定义并不新鲜，但由于Google的影响力，我们可以认为现在是更加正式地提出了这个定义，并将其视为一个层地看待；此外这个定义只是注意力的一种形式，还有一些其他选择，比如$query$跟$key$的运算方式不一定是点乘（还可以是拼接后再内积一个参数向量），甚至权重都不一定要归一化，等等。 Multi-Head Attention 这个是Google提出的新概念，是Attention机制的完善。不过从形式上看，它其实就再简单不过了，就是把$\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}$通过参数矩阵映射一下，然后再做Attention，把这个过程重复做$h$次，结果拼接起来就行了，可谓“大道至简”了。具体来说 这里$\\boldsymbol{W}_i^Q\\in\\mathbb{R}^{d_k\\times \\tilde{d}_k}, \\boldsymbol{W}_i^K\\in\\mathbb{R}^{d_k\\times \\tilde{d}_k}, \\boldsymbol{W}_i^V\\in\\mathbb{R}^{d_v\\times \\tilde{d}_v}$，然后 最后得到一个$n\\times (h\\tilde{d}_v)$的序列。所谓“多头”（Multi-Head），就是只多做几次同样的事情（参数不共享），然后把结果拼接。 Self Attention 到目前为止，对Attention层的描述都是一般化的，我们可以落实一些应用。比如，如果做阅读理解的话，$\\boldsymbol{Q}$可以是篇章的词向量序列，取$\\boldsymbol{K}=\\boldsymbol{V}$为问题的词向量序列，那么输出就是所谓的Aligned Question Embedding。 而在Google的论文中，大部分的Attention都是Self Attention，即“自注意力”，或者叫内部注意力。 所谓Self Attention，其实就是$Attention(\\boldsymbol{X},\\boldsymbol{X},\\boldsymbol{X})$，$\\boldsymbol{X}$就是前面说的输入序列。也就是说，在序列内部做Attention，寻找序列内部的联系。Google论文的主要贡献之一是它表明了内部注意力在机器翻译（甚至是一般的Seq2Seq任务）的序列编码上是相当重要的，而之前关于Seq2Seq的研究基本都只是把注意力机制用在解码端。类似的事情是，目前SQUAD阅读理解的榜首模型R-Net也加入了自注意力机制，这也使得它的模型有所提升。 当然，更准确来说，Google所用的是Self Multi-Head Attention： Position Embedding 然而，只要稍微思考一下就会发现，这样的模型并不能捕捉序列的顺序！换句话说，如果将$\\boldsymbol{K},\\boldsymbol{V}$按行打乱顺序（相当于句子中的词序打乱），那么Attention的结果还是一样的。这就表明了，到目前为止，Attention模型顶多是一个非常精妙的“词袋模型”而已。 这问题就比较严重了，大家知道，对于时间序列来说，尤其是对于NLP中的任务来说，顺序是很重要的信息，它代表着局部甚至是全局的结构，学习不到顺序信息，那么效果将会大打折扣（比如机器翻译中，有可能只把每个词都翻译出来了，但是不能组织成合理的句子）。 于是Google再祭出了一招——Position Embedding，也就是“位置向量”，将每个位置编号，然后每个编号对应一个向量，通过结合位置向量和词向量，就给每个词都引入了一定的位置信息，这样Attention就可以分辨出不同位置的词了。...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/paper/atrank/Attention.html",
        "teaser":null},{
        "title": "atrank",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/paper/atrank/index.html",
        "teaser":null},{
        "title": "paper",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/paper/index.html",
        "teaser":null},{
        "title": "temp",
        
        "excerpt":
            "简介  ",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/temp/index.html",
        "teaser":null},{
        "title": "merge",
        
        "excerpt":
            "第一部分 请说明线性分类器与非线性分类器有哪些区别，具体应用场景有哪些不同？ 线性分类器： 线性分类是指用一个超平面能将正负样本区分开，表达式为$f(x)=wx+b$，对于二维的情况，超平面可以理解为一条直线，如一次函数。线性分类器的分类算法是基于一个线性的预测函数，决策的边界是平的，比如直线和平面。 非线性分类器： 分类界面没有限制，可以是一个曲面，或者是多个超平面的组合，在二维的情况下可以是一个曲线。 两者区别： 线性分类器速度快、编程方便，但是可能拟合效果不会很好；非线性分类器编程复杂，但是效果好拟合能力强。 应用场景 特征比数据量还大时，选择线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分。 对于维度很高的特征，选择线性分类器，理由同上。 对于维度极低的特征，选择非线性分类器，因为低维空间可能很多特征都跑到一起了，导致线性不可分。 常见应用: 常见的线性分类器有：LR,贝叶斯分类，单层感知机、线性回归 常见的非线性分类器：决策树、RF、GBDT、多层感知机 SVM两种都有（看线性核还是高斯核） 逻辑回归中为什么常常要对特征进行离散化? 简化计算： 离散化后数据表示为稀疏向量，内积乘法运算速度快，计算结果方便存储，容易扩展。 克服异常值对模型的扰动： 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据会给模型造成很大的干扰。 提升模型表达： 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力 引入新特征： 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，引入非线性，提升表达能力。 稳定模型表现： 特征离散化后，模型会更稳定。比如对用户年龄进行离散化处理（也可以认为是一种聚类）使得特征对模型的影响更加稳定。 随机森林如何处理缺失值？随机森林如何评估特征重要性？ RandomForest包里有两种补全缺失值的方法： 方法一（na.roughfix）简单粗暴，对于训练集,同一个类别标签下的数据，如果是分类变量缺失，用众数补上，如果是连续型变量缺失，用中位数补。 方法二（rfImpute）这个方法计算量大，至于比方法一好坏不好判断。先用na.roughfix补上缺失值，然后构建森林并计算proximity matrix，再回头看缺失值，如果是分类变量，则用没有缺失的观测实例的proximity中的权重进行投票。如果是连续型变量，则用proximity矩阵进行加权平均的方法补缺失值。然后迭代4-6次，这个补缺失值的思想和KNN有些类似。 随机森林如何评估特征重要性？ 思路： 随机森林评估每个特征在随机森林中的每颗树上做了多大的贡献，然后取个平均值，最后比一比特征之间的贡献大小。 方法： 1) Decrease GINI： 对于回归问题，直接使用argmax(Var−VarLeft−VarRight)作为评判标准，即当前节点训练集的方差Var减去左节点的方差VarLeft和右节点的方差VarRight。 2) Decrease Accuracy：对于一棵树Tb(x)，我们用OOB(out-of-bag)样本可以得到测试误差1；然后随机改变OOB样本的第j列：保持其他列不变，对第j列进行随机的上下置换，得到误差2。至此，我们可以用误差1-误差2来刻画变量j的重要性。基本思想就是，如果一个变量j足够重要，那么改变它会极大的增加测试误差；反之，如果改变它测试误差没有增大，则说明该变量不是那么的重要。 请说明梯度下降算法如何实现，以及它与牛顿法的不同？ 梯度下降法是用当前位置负梯度方向作为搜索方向，该方向为当前位置的最快下降方向，越接近目标值，步长越小，前进越慢。最终求解在给定模型给定数据之后的目标函数在参数空间中搜索找到的极值。 其迭代过程为：$\\theta^n=\\theta^{n-1}-\\alpha \\nabla J(\\theta)$ 与牛顿法的不同 梯度下降的目的是直接求解目标函数极小值，而牛顿法则变相地通过求解目标函数一阶导为零的参数值，进而求得目标函数最小值。 简要谈下您理解的的机器学习领域的正则化？ 在机器学习模型中的正则化是指结构风险最小化策略的实现，是在经验风险上加一个正则化项或罚项。 正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。 比如，正则化项可以是模型参数向量的范数。不管是L1还是L2，都是针对模型中参数过大的问题引入惩罚项。 而在深度学习中，要优化的变成了一个个矩阵，参数变得多出了几个数量级，过拟合的可能性也相应的提高了。而要惩罚的是神经网络中每个神经元的权重大小，从而避免网络中的神经元走极端抄近路。 带核的SVM为什么能分类非线性问题？ SVM通过核函数映射函数将样本的原始特征映射到一个使样本线性可分的更高维的空间中。将在低维线性不可分问题转化为高维空间线性问题,并通过核函数降低两点之间内积精确计算阶段的成本，最终得到超平面是高维空间的线性分类平面。 请举例有哪些常用核函数，以及核函数的条件 多项式核:$K(x_1, x_2)=(x_1 * x_2 + c)^{d}$ 高斯核:$K(x_1, x_2)=exp(-\\frac{\\gamma *   x_1 - x_2   ^2}{2\\sigma^2})$ sigmoid核：$K(x_1, x_2) = tanh(\\beta*x_i^Tx_j+\\theta)$ 核函数必须是对称函数，核矩阵半正定。 什么是偏差与方差？当你模型受到低偏差和高方差困扰时，如何解决？ 方差Variance和偏差Bias ： 偏差： 指某种模型的学习与表达能力在给定数据集上出现的误差，表现为模型选择上的不正确。 方差： 指某种模型在给定数据集上表现的的不稳定性，如受到数据分布，超参数等影响而表现出的模型不稳定。 模型受到低偏差和高方差困扰时的解决思路： 说明模型选择问题不大，模型是有可能受到了欠/过拟合的影响，可以思考在数据维度，特征选择，正负样本分布，集成学习上加以改善解决。 请说明熵、联合熵、条件熵、相对熵、互信息的定义（要求公式），以及您对这些定义的理解 熵（entropy）：用来衡量整个系统的总体信息量 联合熵：对于服从联合分布P(x,y)的一对离散型随机变量，联合熵定义为 联合熵大于或等于这两个变量中任一个的独立熵，少于或等于独立熵的和。该不等式有且只有在和均为统计独立的时候相等。 这表明，两个变量关联之后不确定性会增大，但是又由于相互有制约关系，不确定程度小于单独两个变量的不确定度之和。 条件熵：就是在事件X的前提下，事件Y的熵： 可以看出在Y的条件下，X的不确定度是多少。 相对熵（KL距离)：两个概率密度函数p(x)和q(x)之间的相对熵定义为 描述两个随机变量距离的度量也叫交叉熵。相对熵越大，两个函数差异越大；反之，相对熵越小，两个函数差异越小。 互信息：给定两个随机变量X，Y，联合密度函数为P(x,y)，其边际函数分别为p(x),q(x)，则互信息为P(x,y) 与p(x)q(x)之间的相对熵。 $I(X,Y) =\\sum_{x\\in...",
        "categories": [],
        "tags": [],
        "url": "https://jupyter.org/jupyter-book/temp/merge.html",
        "teaser":null},]
</script>
              <nav class="c-page__nav">
  

  
</nav>

            </div>
          </div>
        </div>
      </main>
    </div>

  </body>
</html>
