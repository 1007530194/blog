<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!-->
<html class="no-js" lang="en"><!--<![endif]-->
    <head>
<meta charset="utf-8">
<title>魑魅魍魉</title>

<meta name="author" content="niult">




<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">



<link href="./theme/css/main.css" media="screen, projection"
      rel="stylesheet" type="text/css">

<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
      rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
      rel="stylesheet" type="text/css">
</head>

<body>

<script src="./theme/js/modernizr-2.0.js"></script>
<script src="./theme/js/ender.js"></script>
<script src="./theme/js/octopress.js" type="text/javascript"></script>
<script src="./theme/js/echarts.min.js" type="text/javascript"></script>
<script src="./theme/js/require.min.js" type="text/javascript"></script>

<header role="banner"><hgroup>
  <h1><a href="./">魑魅魍魉</a></h1>
</hgroup></header>
<nav role="navigation">    <ul class="subscription" data-subscription="rss">
    </ul>


<ul class="main-navigation">
            <li >
                <a href="./category/01chang yong gong ju.html">01常用工具</a>
            </li>
            <li >
                <a href="./category/02.wo ai du shu.html">02.我爱读书</a>
            </li>
            <li >
                <a href="./category/algorithms.html">Algorithms</a>
            </li>
            <li >
                <a href="./category/book.html">Book</a>
            </li>
            <li >
                <a href="./category/book-pydata.html">Book-pydata</a>
            </li>
            <li >
                <a href="./category/deep-learning-with-python.html">Deep-learning-with-python</a>
            </li>
            <li >
                <a href="./category/ji qi xue xi shi zhan.html">机器学习实战</a>
            </li>
            <li >
                <a href="./category/ke wai du wu.html">课外读物</a>
            </li>
            <li >
                <a href="./category/ling ji chu ru men shen du xue xi.html">零基础入门深度学习</a>
            </li>
            <li >
                <a href="./category/shen du xue xi.html">深度学习</a>
            </li>
            <li >
                <a href="./category/shen jing wang luo.html">神经网络</a>
            </li>
            <li >
                <a href="./category/shu ju wa jue.html">数据挖掘</a>
            </li>
            <li >
                <a href="./category/shu xue ji chu.html">数学基础</a>
            </li>
            <li >
                <a href="./category/tf-example.html">Tf-example</a>
            </li>
            <li >
                <a href="./category/tool1.html">Tool1</a>
            </li>
            <li >
                <a href="./category/tool2.html">Tool2</a>
            </li>
            <li >
                <a href="./category/tools.html">Tools</a>
            </li>
            <li >
                <a href="./category/tui jian xi tong.html">推荐系统</a>
            </li>
            <li >
                <a href="./category/wen ben wa jue.html">文本挖掘</a>
            </li>
</ul>
</nav>
<div id="main">
    <div id="content">
    <div class="blog-index">
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/algorithms-logistic-code.html">algorithms-logistic-code</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing code_cell rendered">
<div class="input"></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/algorithms-logistic-code.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/algorithms-logistic-li-lun.html">algorithms-logistic-理论</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="逻辑回归">&#36923;&#36753;&#22238;&#24402;<a class="anchor-link" href="#逻辑回归">&#182;</a></h1><p>该算法尝试为<strong>给定一个输入特征的线性组合的一个二元变量的结果</strong>建模。</p>
<p>举个例子，可以根据候选人为竞选花费的金钱、时间等信息预测选举结果。</p>
<p>逻辑回归工作流程——</p>
<p>给定：</p>
<ul>
<li>数据集合 $\{(x^{(1)},y^{(1)},\ldots,(x^{(m)},y^{(m)})\}$</li>
<li>每个 $x^{(i)}$ 都是 $d$ 维向量 $x^{(i)}=(x_1^{(i)},\ldots,x_d^{(i)})$</li>
<li>每个 $y^{(i)}$ 都是一个二元目标变量 $y^{(i)} \in \{0,1\}$</li>
</ul>
<p>逻辑回归模型可以用非常简单的神经网络表达：</p>
<ul>
<li>拥有一个实值权重向量 $w=(w^{(1)},\ldots,w^{(d)})$</li>
<li>拥有一个实值偏置 $b$</li>
<li>采用sigmoid函数作为激活函数</li>
</ul>
<p>与<a href="./线性回归.ipynb">线性回归</a>不同，逻辑回归没有封闭解。</p>
<p>但是成本函数是凸的（<em>convex</em>），因此可以采用梯度下降方法来训练该模型。</p>
<p>事实上，梯度下降（或其余任何最优化算法）保证找到全局最小（如果学习率足够小且训练迭代次数足够多）。</p>
<p>训练步骤：</p>
<p>一开始模型参数是初始化了的，接下来重复指定的训练迭代次数或直到参数收敛。</p>
<ul>
<li><p>第一步</p>
<p>把权重向量和偏置初始化为零（或小的随机数）</p>
</li>
<li><p>第二步</p>
<p>计算一个输入特性和权重的线性组合。</p>
<p>采用向量化和广播所有训练样本可一次性搞定： $a=X \cdot w + b$</p>
<p>这里 $X$ 是矩阵 $(n_{samples},n_{features})$ 保佑全部训练样本，而 $\cdot$ 表示点积。</p>
</li>
<li><p>第三步</p>
<p>应用sigmoid激活函数，返回值在0和1之间： $$\hat y=\sigma (a)=\frac{1}{1 + \exp(-a)}$$</p></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/algorithms-logistic-li-lun.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/algorithms-lr.html">algorithms-LR</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="例1">&#20363;1<a class="anchor-link" href="#例1">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input"></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/algorithms-lr.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/algorithms-lr3.html">algorithms-LR3</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing code_cell rendered">
<div class="input"></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/algorithms-lr3.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/algorithms-naivebayes.html">algorithms-naiveBayes</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing code_cell rendered">
<div class="input"></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/algorithms-naivebayes.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/algorithms-pca.html">algorithms-PCA</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing code_cell rendered">
<div class="input"></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/algorithms-pca.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/algorithms-pca2.html">algorithms-PCA2</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing code_cell rendered">
<div class="input"></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/algorithms-pca2.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/algorithms-shi-fen-zhong-shang-shou-xgboost.html">algorithms-十分钟上手xgboost</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="xgboost">xgboost<a class="anchor-link" href="#xgboost">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="参考网址">&#21442;&#32771;&#32593;&#22336;<a class="anchor-link" href="#参考网址">&#182;</a></h2><p>1 XGBoost参数调优完全指南（附Python代码） <a href="https://www.cnblogs.com/mfryf/p/6293814.html">https://www.cnblogs.com/mfryf/p/6293814.html</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="XGBoost的参数">XGBoost&#30340;&#21442;&#25968;<a class="anchor-link" href="#XGBoost的参数">&#182;</a></h1><p>XGBoost的作者把所有的参数分成了三类：<br>
1、通用参数：宏观函数控制。<br>
2、Booster参数：控制每一步的booster(tree/regression)。<br>
3、学习目标参数：控制训练目标的表现。<br>
在这里我会类比GBM来讲解，所以作为一种基础知识。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="通用参数">&#36890;&#29992;&#21442;&#25968;<a class="anchor-link" href="#通用参数">&#182;</a></h2><p>这些参数用来控制XGBoost的宏观功能。</p>
<h3 id="booster[默认gbtree]">booster[&#40664;&#35748;gbtree]<a class="anchor-link" href="#booster[默认gbtree]">&#182;</a></h3><p>选择每次迭代的模型，有两种选择：
gbtree：基于树的模型
gbliner：线性模型</p>
<h3 id="silent[默认0]">silent[&#40664;&#35748;0]<a class="anchor-link" href="#silent[默认0]">&#182;</a></h3><p>当这个参数值为1时，静默模式开启，不会输出任何信息。 一般这个参数就保持默认的0，因为这样能帮我们更好地理解模型。</p>
<h3 id="nthread[默认值为最大可能的线程数]">nthread[&#40664;&#35748;&#20540;&#20026;&#26368;&#22823;&#21487;&#33021;&#30340;&#32447;&#31243;&#25968;]<a class="anchor-link" href="#nthread[默认值为最大可能的线程数]">&#182;</a></h3><p>这个参数用来进行多线程控制，应当输入系统的核数。 如果你希望使用CPU全部的核，那就不要输入这个参数，算法会自动检测它。
还有两个参数，XGBoost会自动设置，目前你不用管它。接下来咱们一起看booster参数。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="booster参数">booster&#21442;&#25968;<a class="anchor-link" href="#booster参数">&#182;</a></h2><p>尽管有两种booster可供选择，我这里只介绍tree booster，因为它的表现远远胜过linear booster，所以linear booster很少用到。</p>
<h3 id="eta[默认0.3]">eta[&#40664;&#35748;0.3]<a class="anchor-link" href="#eta[默认0.3]">&#182;</a></h3><p>和GBM中的 learning rate 参数类似。 通过减少每一步的权重，可以提高模型的鲁棒性。 典型值为0.01-0.2。</p>
<h3 id="min_child_weight[默认1]">min_child_weight[&#40664;&#35748;1]<a class="anchor-link" href="#min_child_weight[默认1]">&#182;</a></h3><p>决定最小叶子节点样本权重和。 和GBM的 min_child_leaf 参数类似，但不完全一样。XGBoost的这个参数是最小样本权重的和，而GBM参数是最小样本总数。 这个参数用于避免过拟合。当它的值较大时，可以避免模型学习到局部的特殊样本。 但是如果这个值过高，会导致欠拟合。这个参数需要使用CV来调整。</p>
<h2 id="max_depth[默认6]">max_depth[&#40664;&#35748;6]<a class="anchor-link" href="#max_depth[默认6]">&#182;</a></h2><p>和GBM中的参数相同，这个值为树的最大深度。 这个值也是用来避免过拟合的。max_depth越大，模型会学到更具体更局部的样本。 需要使用CV函数来进行调优。 典型值：3-10</p>
<h3 id="max_leaf_nodes">max_leaf_nodes<a class="anchor-link" href="#max_leaf_nodes">&#182;</a></h3><p>树上最大的节点或叶子的数量。 可以替代max_depth的作用。因为如果生成的是二叉树，一个深度为n的树最多生成n2个叶子。 如果定义了这个参数，GBM会忽略max_depth参数。</p>
<h3 id="gamma[默认0]">gamma[&#40664;&#35748;0]<a class="anchor-link" href="#gamma[默认0]">&#182;</a></h3><p>在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值。 这个参数的值越大，算法越保守。这个参数的值和损失函数息息相关，所以是需要调整的。</p>
<h3 id="max_delta_step[默认0]">max_delta_step[&#40664;&#35748;0]<a class="anchor-link" href="#max_delta_step[默认0]">&#182;</a></h3><p>这参数限制每棵树权重改变的最大步长。如果这个参数的值为0，那就意味着没有约束。如果它被赋予了某个正值，那么它会让这个算法更加保守。 通常，这个参数不需要设置。但是当各类别的样本十分不平衡时，它对逻辑回归是很有帮助的。 这个参数一般用不到，但是你可以挖掘出来它更多的用处。</p>
<h3 id="subsample[默认1]">subsample[&#40664;&#35748;1]<a class="anchor-link" href="#subsample[默认1]">&#182;</a></h3><p>和GBM中的subsample参数一模一样。这个参数控制对于每棵树，随机采样的比例。 减小这个参数的值，算法会更加保守，避免过拟合。但是，如果这个值设置得过小，它可能会导致欠拟合。 典型值：0.5-1</p>
<h3 id="colsample_bytree[默认1]">colsample_bytree[&#40664;&#35748;1]<a class="anchor-link" href="#colsample_bytree[默认1]">&#182;</a></h3><p>和GBM里面的max_features参数类似。用来控制每棵随机采样的列数的占比(每一列是一个特征)。 典型值：0.5-1</p>
<h3 id="colsample_bylevel[默认1]">colsample_bylevel[&#40664;&#35748;1]<a class="anchor-link" href="#colsample_bylevel[默认1]">&#182;</a></h3><p>用来控制树的每一级的每一次分裂，对列数的采样的占比。 我个人一般不太用这个参数，因为subsample参数和colsample_bytree参数可以起到相同的作用。但是如果感兴趣，可以挖掘这个参数更多的用处。</p></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/algorithms-shi-fen-zhong-shang-shou-xgboost.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/algorithms-svd.html">algorithms-SVD</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing code_cell rendered">
<div class="input"></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/algorithms-svd.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/algorithms-wen-ben-wa-jue-yu-chu-li-zhi-tf-idf.html">algorithms-文本挖掘预处理之TF-IDF</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>在<a href="http://www.cnblogs.com/pinard/p/6688348.html">文本挖掘预处理之向量化与Hash Trick</a>中我们讲到在文本挖掘的预处理中，向量化之后一般都伴随着TF-IDF的处理，那么什么是TF-IDF，为什么一般我们要加这一步预处理呢？这里就对TF-IDF的原理做一个总结。</p>
<h1 id="文本向量化特征的不足">&#25991;&#26412;&#21521;&#37327;&#21270;&#29305;&#24449;&#30340;&#19981;&#36275;<a class="anchor-link" href="#文本向量化特征的不足">&#182;</a></h1><p>在将文本分词并向量化后，我们可以得到词汇表中每个词在各个文本中形成的词向量，比如在<a href="http://www.cnblogs.com/pinard/p/6688348.html">文本挖掘预处理之向量化与Hash Trick</a>这篇文章中，我们将下面4个短文本做了词频统计：</p>

<pre><code>corpus=["I come to China to travel", "This is a car polupar in China", "I love tea and Apple ", "The work is to write some papers in science"] 

</code></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/algorithms-wen-ben-wa-jue-yu-chu-li-zhi-tf-idf.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/algorithms-wen-ben-zhu-ti-mo-xing-zhi-qian-zai-yu-yi-suo-yin-lsi.html">algorithms-文本主题模型之潜在语义索引(LSI)</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="简介">&#31616;&#20171;<a class="anchor-link" href="#简介">&#182;</a></h1><p>在文本挖掘中，主题模型是比较特殊的一块，它的思想不同于我们常用的机器学习算法，因此这里我们需要专门来总结文本主题模型的算法。本文关注于潜在语义索引算法(LSI)的原理。</p>
<h1 id="文本主题模型的问题特点">&#25991;&#26412;&#20027;&#39064;&#27169;&#22411;&#30340;&#38382;&#39064;&#29305;&#28857;<a class="anchor-link" href="#文本主题模型的问题特点">&#182;</a></h1><p>在数据分析中，我们经常会进行非监督学习的聚类算法，它可以对我们的特征数据进行非监督的聚类。而主题模型也是非监督的算法，目的是得到文本按照主题的概率分布。从这个方面来说，主题模型和普通的聚类算法非常的类似。但是两者其实还是有区别的。</p>
<p>聚类算法关注于从样本特征的相似度方面将数据聚类。比如通过数据样本之间的欧式距离，曼哈顿距离的大小聚类等。而主题模型，顾名思义，就是对文字中隐含主题的一种建模方法。比如从“人民的名义”和“达康书记”这两个词我们很容易发现对应的文本有很大的主题相关度，但是如果通过词特征来聚类的话则很难找出，因为聚类方法不能考虑到到隐含的主题这一块。</p>
<p>那么如何找到隐含的主题呢？这个一个大问题。常用的方法一般都是基于统计学的生成方法。即假设以一定的概率选择了一个主题，然后以一定的概率选择当前主题的词。最后这些词组成了我们当前的文本。所有词的统计概率分布可以从语料库获得，具体如何以“一定的概率选择”，这就是各种具体的主题模型算法的任务了。</p>
<p>当然还有一些不是基于统计的方法，比如我们下面讲到的LSI。</p>
<h1 id="潜在语义索引(LSI)概述">&#28508;&#22312;&#35821;&#20041;&#32034;&#24341;(LSI)&#27010;&#36848;<a class="anchor-link" href="#潜在语义索引(LSI)概述">&#182;</a></h1><p>潜在语义索引(Latent Semantic Indexing,以下简称LSI)，有的文章也叫Latent Semantic Analysis（LSA）。其实是一个东西，后面我们统称LSI，它是一种简单实用的主题模型。LSI是基于奇异值分解（SVD）的方法来得到文本的主题的。而SVD及其应用我们在前面的文章也多次讲到，比如：<a href="http://www.cnblogs.com/pinard/p/6251584.html">奇异值分解(SVD)原理与在降维中的应用</a>和<a href="http://www.cnblogs.com/pinard/p/6351319.html">矩阵分解在协同过滤推荐算法中的应用</a>。如果大家对SVD还不熟悉，建议复习<a href="http://www.cnblogs.com/pinard/p/6251584.html">奇异值分解(SVD)原理与在降维中的应用</a>后再读下面的内容。</p>
<p>这里我们简要回顾下SVD：对于一个$m \times n$的矩阵$A$，可以分解为下面三个矩阵：</p>
<p>$$A_{m \times n} = U_{m \times m}\Sigma_{m \times n} V^T_{n \times n}$$</p>
<p>有时为了降低矩阵的维度到k，SVD的分解可以近似的写为：</p>
<p>$$A_{m \times n} \approx U_{m \times k}\Sigma_{k \times k} V^T_{k \times n}$$</p>
<p>如果把上式用到我们的主题模型，则SVD可以这样解释：我们输入的有m个文本，每个文本有n个词。而$A_{ij}$则对应第i个文本的第j个词的特征值，这里最常用的是基于预处理后的标准化TF-IDF值。k是我们假设的主题数，一般要比文本数少。SVD分解后，$U_{il}$对应第i个文本和第l个主题的相关度。$V_{jm}$对应第j个词和第m个词义的相关度。$\Sigma_{lm}$对应第l个主题和第m个词义的相关度。</p>
<p>也可以反过来解释：我们输入的有m个词，对应n个文本。而$A_{ij}$则对应第i个词档的第j个文本的特征值，这里最常用的是基于预处理后的标准化TF-IDF值。k是我们假设的主题数，一般要比文本数少。SVD分解后，$U_{il}$对应第i个词和第l个词义的相关度。$V_{jm}$对应第j个文本和第m个主题的相关度。$\Sigma_{lm}$对应第l个词义和第m个主题的相关度。</p>
<p>这样我们通过一次SVD，就可以得到文档和主题的相关度，词和词义的相关度以及词义和主题的相关度。</p>
<h1 id="LSI简单实例">LSI&#31616;&#21333;&#23454;&#20363;<a class="anchor-link" href="#LSI简单实例">&#182;</a></h1><p>这里举一个简单的LSI实例，假设我们有下面这个有11个词三个文本的词频TF对应矩阵如下：</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201705/1042406-20170504134451664-1723370358.png" alt=""></p>
<p>这里我们没有使用预处理，也没有使用TF-IDF，在实际应用中最好使用预处理后的TF-IDF值矩阵作为输入。</p>
<p>我们假定对应的主题数为2，则通过SVD降维后得到的三矩阵为：</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201705/1042406-20170504135321726-2116029824.png" alt=""></p>
<p>从矩阵$U_k$我们可以看到词和词义之间的相关性。而从$V_k$可以看到3个文本和两个主题的相关性。大家可以看到里面有负数，所以这样得到的相关度比较难解释。</p>
<h1 id="LSI用于文本相似度计算">LSI&#29992;&#20110;&#25991;&#26412;&#30456;&#20284;&#24230;&#35745;&#31639;<a class="anchor-link" href="#LSI用于文本相似度计算">&#182;</a></h1><p>在上面我们通过LSI得到的文本主题矩阵可以用于文本相似度计算。而计算方法一般是通过余弦相似度。比如对于上面的三文档两主题的例子。我们可以计算第一个文本和第二个文本的余弦相似度如下 ：$$sim(d1,d2) = \frac{(-0.4945)*(-0.6458) + (0.6492)*(-0.7194)}{\sqrt{(-0.4945)^2+0.6492^2}\sqrt{(-0.6458)^2+(-0.7194)^2}}$$</p></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/algorithms-wen-ben-zhu-ti-mo-xing-zhi-qian-zai-yu-yi-suo-yin-lsi.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/algorithms-xgboost1.html">algorithms-xgboost1</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="xgBoost">xgBoost<a class="anchor-link" href="#xgBoost">&#182;</a></h1><p><a href="https://www.cnblogs.com/mfryf/p/6238185.html">https://www.cnblogs.com/mfryf/p/6238185.html</a></p>
<p><a href="https://www.zhihu.com/question/41354392">https://www.zhihu.com/question/41354392</a></p>
<p><a href="https://blog.csdn.net/github_38414650/article/details/76061893">https://blog.csdn.net/github_38414650/article/details/76061893</a></p>
<p><a href="https://www.cnblogs.com/csyuan/p/6537255.html">https://www.cnblogs.com/csyuan/p/6537255.html</a></p>
<p><a href="https://www.cnblogs.com/infaraway/p/7890558.html">https://www.cnblogs.com/infaraway/p/7890558.html</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Gradient-Boosting-Tree-算法原理">Gradient Boosting Tree &#31639;&#27861;&#21407;&#29702;<a class="anchor-link" href="#Gradient-Boosting-Tree-算法原理">&#182;</a></h2><p>Friedman于论文”GreedyFunctionApproximation...”中最早提出GBDT</p>
<ul>
<li>其模型F定义为加法模型:</li>
</ul>
<p>$$F(x;w)=\sum^T_{t=0}{\alpha_ih_i}\big(x;w_i\big)=\sum^T_{t=0}f_i\big(x;w_i\big) $$</p>
<p>其中，$x$为输入样本，$h$为分类回归树，$w$是分类回归树的参数， $\alpha$是每棵树的权重。</p>
<ul>
<li>通过最小化损失函数求解最优模型:</li>
</ul>
<p>$$F^*=\arg_F{\min\sum^N_{i=0}L\Big(y_i,F\big(x_i;w\big)\Big)}$$</p>
<p>NP难问题 -&gt; 通过贪心法，迭代求局部最优解</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>输入：$\big(x_i,y_i\big),T,L$</p>
<p>1 初始化$f_0$</p>
<p>2 for t = 1 to T do
  2.1 计算响应：$\tilde {y_i}= -{\Bigg[\frac{\partial L\big(y_i,F(x_i) \big)}{\partial F(x_i)} \Bigg]}_{F(x)=F_{t-1}(x)},i=1,2,...,N$<br>
  2.2 学习第$t$棵树：$w^*=\arg_w{\min\sum\limits^N_{i=1}\Big(\tilde{y_i}-h_i\big(x_i;w \big) \Big)}$<br>
  2.3 line search找步长：
  $\rho^*=\arg_{\rho}\min\sum\limits^N_{i=1}L\Big(y_i,F_{t-1}(x_i)+\rho h_i\big(x_i;w^* \big) \Big)$<br>
  2.4 令$f_t=\rho^*h_i(x;w^*)$，更新模型  $F_t=F_{t-1}+f_t$</p></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/algorithms-xgboost1.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/algorithms-xie-tong-guo-lu-svd.html">algorithms-协同过滤SVD</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing code_cell rendered">
<div class="input"></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/algorithms-xie-tong-guo-lu-svd.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/algorithms-yin-ma-er-ke-fu-mo-xing-hmm.html">algorithms-隐马尔科夫模型HMM</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="HMM模型基础">HMM&#27169;&#22411;&#22522;&#30784;<a class="anchor-link" href="#HMM模型基础">&#182;</a></h1><p>隐马尔科夫模型（Hidden Markov Model，以下简称HMM）是比较经典的机器学习模型了，它在语言识别，自然语言处理，模式识别等领域得到广泛的应用。当然，随着目前深度学习的崛起，尤其是<a href="http://www.cnblogs.com/pinard/p/6509630.html">RNN</a>，<a href="http://www.cnblogs.com/pinard/p/6519110.html">LSTM</a>等神经网络序列模型的火热，HMM的地位有所下降。但是作为一个经典的模型，学习HMM的模型和对应算法，对我们解决问题建模的能力提高以及算法思路的拓展还是很好的。本文是HMM系列的第一篇，关注于HMM模型的基础。</p>
<h2 id="什么样的问题需要HMM模型">&#20160;&#20040;&#26679;&#30340;&#38382;&#39064;&#38656;&#35201;HMM&#27169;&#22411;<a class="anchor-link" href="#什么样的问题需要HMM模型">&#182;</a></h2><p>首先我们来看看什么样的问题解决可以用HMM模型。使用HMM模型时我们的问题一般有这两个特征：１）我们的问题是基于序列的，比如时间序列，或者状态序列。２）我们的问题中有两类数据，一类序列数据是可以观测到的，即观测序列；而另一类数据是不能观察到的，即隐藏状态序列，简称状态序列。</p>
<p>有了这两个特征，那么这个问题一般可以用HMM模型来尝试解决。这样的问题在实际生活中是很多的。比如：我现在在打字写博客，我在键盘上敲出来的一系列字符就是观测序列，而我实际想写的一段话就是隐藏序列，输入法的任务就是从敲入的一系列字符尽可能的猜测我要写的一段话，并把最可能的词语放在最前面让我选择，这就可以看做一个HMM模型了。再举一个，我在和你说话，我发出的一串连续的声音就是观测序列，而我实际要表达的一段话就是状态序列，你大脑的任务，就是从这一串连续的声音中判断出我最可能要表达的话的内容。</p>
<p>从这些例子中，我们可以发现，HMM模型可以无处不在。但是上面的描述还不精确，下面我们用精确的数学符号来表述我们的HMM模型。</p>
<h2 id="HMM模型的定义">HMM&#27169;&#22411;&#30340;&#23450;&#20041;<a class="anchor-link" href="#HMM模型的定义">&#182;</a></h2><p>对于HMM模型，首先我们假设$Q$是所有可能的隐藏状态的集合，$V$是所有可能的观测状态的集合，即：$$Q = \{q_1,q_2,...,q_N\}, \; V =\{v_1,v_2,...v_M\}$$</p>
<p>其中，$N$是可能的隐藏状态数，$M$是所有的可能的观察状态数。</p>
<p>对于一个长度为$T$的序列，$I$对应的状态序列, $O$是对应的观察序列，即：$$I = \{i_1,i_2,...,i_T\}, \; O =\{o_1,o_2,...o_T\}$$</p>
<p>其中，任意一个隐藏状态$i_t \in Q$,任意一个观察状态$o_t \in V$</p>
<p>HMM模型做了两个很重要的假设如下：</p>
<p>1） 齐次马尔科夫链假设。即任意时刻的隐藏状态只依赖于它前一个隐藏状态，这个我们在<a href="http://www.cnblogs.com/pinard/p/6632399.html">MCMC(二)马尔科夫链</a>中有详细讲述。当然这样假设有点极端，因为很多时候我们的某一个隐藏状态不仅仅只依赖于前一个隐藏状态，可能是前两个或者是前三个。但是这样假设的好处就是模型简单，便于求解。如果在时刻$t$的隐藏状态是$i_t= q_i$,在时刻$t+1$的隐藏状态是$i_{t+1} = q_j$, 则从时刻$t$到时刻$t+1$的HMM状态转移概率$a_{ij}$可以表示为：$$a_{ij} = P(i_{t+1} = q_j | i_t= q_i)$$</p></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/algorithms-yin-ma-er-ke-fu-mo-xing-hmm.html">Read On &crarr;</a>
        </footer>


                </article>
                <article>
<header>
        <h1 class="entry-title">
            <a href="./pages/2019/01/cnn-juan-ji-shen-jing-wang-luo-li-lun.html">CNN-卷积神经网络理论</a>
        </h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="CNN的基本结构">CNN&#30340;&#22522;&#26412;&#32467;&#26500;<a class="anchor-link" href="#CNN的基本结构">&#182;</a></h1><p>首先我们来看看CNN的基本结构。一个常见的CNN例子如下图：</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201703/1042406-20170301104438813-230726230.png" alt=""></p>
<p>图中是一个图形识别的CNN模型。可以看出最左边的船的图像就是我们的输入层，计算机理解为输入若干个矩阵，这点和DNN基本相同。</p>
<p>接着是卷积层（Convolution Layer）,这个是CNN特有的，我们后面专门来讲。卷积层的激活函数使用的是ReLU。我们在DNN中介绍过ReLU的激活函数，它其实很简单，就是$ReLU(x) = max(0,x)$。在卷积层后面是池化层(Pooling layer)，这个也是CNN特有的，我们后面也会专门来讲。需要注意的是，池化层没有激活函数。</p>
<p>卷积层+池化层的组合可以在隐藏层出现很多次，上图中出现两次。而实际上这个次数是根据模型的需要而来的。当然我们也可以灵活使用使用卷积层+卷积层，或者卷积层+卷积层+池化层的组合，这些在构建模型的时候没有限制。但是最常见的CNN都是若干卷积层+池化层的组合，如上图中的CNN结构。</p>
<p>在若干卷积层+池化层后面是全连接层（Fully Connected Layer, 简称FC），全连接层其实就是我们前面讲的DNN结构，只是输出层使用了Softmax激活函数来做图像识别的分类，这点我们在DNN中也有讲述。</p>
<p>从上面CNN的模型描述可以看出，CNN相对于DNN，比较特殊的是卷积层和池化层，如果我们熟悉DNN，只要把卷积层和池化层的原理搞清楚了，那么搞清楚CNN就容易很多了。</p>
<h2 id="初识卷积">&#21021;&#35782;&#21367;&#31215;<a class="anchor-link" href="#初识卷积">&#182;</a></h2><p>首先，我们去学习卷积层的模型原理，在学习卷积层的模型原理前，我们需要了解什么是卷积，以及CNN中的卷积是什么样子的。</p>
<p>大家学习数学时都有学过卷积的知识，微积分中卷积的表达式为：$$S(t) = \int x(t-a)w(a) da$$</p>
<p>离散形式是：$$s(t) = \sum\limits_ax(t-a)w(a)$$</p>
<p>这个式子如果用矩阵表示可以为：$$s(t)=(X*W)(t)$$</p>
<p>其中星号表示卷积。</p>
<p>如果是二维的卷积，则表示式为：$$s(i,j)=(X*W)(i,j) = \sum\limits_m \sum\limits_n x(i-m,j-n) w(m,n)$$</p>
<p>在CNN中，虽然我们也是说卷积，但是我们的卷积公式和严格意义数学中的定义稍有不同,比如对于二维的卷积，定义为：$$s(i,j)=(X*W)(i,j) = \sum\limits_m \sum\limits_n x(i+m,j+n) w(m,n)$$</p>
<p>这个式子虽然从数学上讲不是严格意义上的卷积，但是大牛们都这么叫了，那么我们也跟着这么叫了。后面讲的CNN的卷积都是指的上面的最后一个式子。</p>
<p>其中，我们叫W为我们的卷积核，而X则为我们的输入。如果X是一个二维输入的矩阵，而W也是一个二维的矩阵。但是如果X是多维张量，那么W也是一个多维的张量。</p>
<h2 id="卷积层">&#21367;&#31215;&#23618;<a class="anchor-link" href="#卷积层">&#182;</a></h2><p>有了卷积的基本知识，我们现在来看看CNN中的卷积，假如是对图像卷积，回想我们的上一节的卷积公式，其实就是对输入的图像的不同局部的矩阵和卷积核矩阵各个位置的元素相乘，然后相加得到。</p>
<p>举个例子如下，图中的输入是一个二维的3x4的矩阵，而卷积核是一个2x2的矩阵。这里我们假设卷积是一次移动一个像素来卷积的，那么首先我们对输入的左上角2x2局部和卷积核卷积，即各个位置的元素相乘再相加，得到的输出矩阵S的$S_{00}$的元素，值为$aw+bx+ey+fz$。接着我们将输入的局部向右平移一个像素，现在是(b,c,f,g)四个元素构成的矩阵和卷积核来卷积，这样我们得到了输出矩阵S的$S_{01}$的元素，同样的方法，我们可以得到输出矩阵S的$S_{02}，S_{10}，S_{11}， S_{12}$的元素。</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201703/1042406-20170301113126032-1940584843.png" alt=""></p>
<p>最终我们得到卷积输出的矩阵为一个2x3的矩阵S。</p>
<p>再举一个动态的卷积过程的例子如下：</p>
<p>我们有下面这个绿色的5x5输入矩阵，卷积核是一个下面这个黄色的3x3的矩阵。卷积的步幅是一个像素。则卷积的过程如下面的动图。卷积的结果是一个3x3的矩阵。</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201703/1042406-20170301113742876-1293419888.png" alt=""></p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201703/1042406-20170301113754391-355407175.png" alt=""></p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201703/1042406-20170301113808532-2109486090.gif" alt=""></p>
<p>上面举的例子都是二维的输入，卷积的过程比较简单，那么如果输入是多维的呢？比如在前面一组卷积层+池化层的输出是3个矩阵，这3个矩阵作为输入呢，那么我们怎么去卷积呢？又比如输入的是对应RGB的彩色图像，即是三个分布对应R，G和B的矩阵呢？</p>
<p>在斯坦福大学的cs231n的课程上，有一个动态的例子，<a href="http://cs231n.github.io/assets/conv-demo/index.html">链接在这</a>。建议大家对照着例子中的动图看下面的讲解。</p>
<p>大家打开这个例子可以看到，这里面输入是3个7x7的矩阵。实际上原输入是3个5x5的矩阵。只是在原来的输入周围加上了1的padding，即将周围都填充一圈的0，变成了3个7x7的矩阵。</p>
<p>例子里面使用了两个卷积核，我们先关注于卷积核W0。和上面的例子相比，由于输入是3个7x7的矩阵，或者说是7x7x3的张量，则我们对应的卷积核W0也必须最后一维是3的张量，这里卷积核W0的单个子矩阵维度为3x3。那么卷积核W0实际上是一个3x3x3的张量。同时和上面的例子比，这里的步幅为2，也就是每次卷积后会移动2个像素的位置。</p>
<p>最终的卷积过程和上面的2维矩阵类似，上面是矩阵的卷积，即两个矩阵对应位置的元素相乘后相加。这里是张量的卷积，即两个张量的3个子矩阵卷积后，再把卷积的结果相加后再加上偏倚b。</p>
<p>7x7x3的张量和3x3x3的卷积核张量W0卷积的结果是一个3x3的矩阵。由于我们有两个卷积核W0和W1，因此最后卷积的结果是两个3x3的矩阵。或者说卷积的结果是一个3x3x2的张量。</p>
<p>仔细回味下卷积的过程，输入是7x7x3的张量，卷积核是两个3x3x3的张量。卷积步幅为2，最后得到了输出是3x3x2的张量。如果把上面的卷积过程用数学公式表达出来就是：$$s(i,j)=(X*W)(i,j) + b = \sum\limits_{k=1}^{n\_in}(X_k*W_k)(i,j) +b$$</p></div>
        <footer>
            <a rel="full-article" href="./pages/2019/01/cnn-juan-ji-shen-jing-wang-luo-li-lun.html">Read On &crarr;</a>
        </footer>


                </article>
<div class="pagination">
    <a class="prev" href="./index4.html">&larr; Older</a>

    <a class="next" href="./index2.html">Newer &rarr;</a>
  <br />
</div>    </div>
<aside class="sidebar">
    <section>
        <h1>Recent Posts</h1>
        <ul id="recent_posts">
                <li class="post">
                    <a href="./pages/2019/01/21-a-first-look-at-a-neural-network.html">2.1-a-first-look-at-a-neural-network</a>
                </li>
                <li class="post">
                    <a href="./pages/2019/01/35-classifying-movie-reviews.html">3.5-classifying-movie-reviews</a>
                </li>
                <li class="post">
                    <a href="./pages/2019/01/36-classifying-newswires.html">3.6-classifying-newswires</a>
                </li>
                <li class="post">
                    <a href="./pages/2019/01/37-predicting-house-prices.html">3.7-predicting-house-prices</a>
                </li>
                <li class="post">
                    <a href="./pages/2019/01/44-overfitting-and-underfitting.html">4.4-overfitting-and-underfitting</a>
                </li>
        </ul>
    </section>
        <section>

            <h1>Categories</h1>
            <ul id="recent_posts">
                    <li><a href="./category/01chang yong gong ju.html">01常用工具</a></li>
                    <li><a href="./category/02.wo ai du shu.html">02.我爱读书</a></li>
                    <li><a href="./category/algorithms.html">algorithms</a></li>
                    <li><a href="./category/book.html">book</a></li>
                    <li><a href="./category/book-pydata.html">book-pydata</a></li>
                    <li><a href="./category/deep-learning-with-python.html">deep-learning-with-python</a></li>
                    <li><a href="./category/ji qi xue xi shi zhan.html">机器学习实战</a></li>
                    <li><a href="./category/ke wai du wu.html">课外读物</a></li>
                    <li><a href="./category/ling ji chu ru men shen du xue xi.html">零基础入门深度学习</a></li>
                    <li><a href="./category/shen du xue xi.html">深度学习</a></li>
                    <li><a href="./category/shen jing wang luo.html">神经网络</a></li>
                    <li><a href="./category/shu ju wa jue.html">数据挖掘</a></li>
                    <li><a href="./category/shu xue ji chu.html">数学基础</a></li>
                    <li><a href="./category/tf-example.html">tf-example</a></li>
                    <li><a href="./category/tool1.html">tool1</a></li>
                    <li><a href="./category/tool2.html">tool2</a></li>
                    <li><a href="./category/tools.html">tools</a></li>
                    <li><a href="./category/tui jian xi tong.html">推荐系统</a></li>
                    <li><a href="./category/wen ben wa jue.html">文本挖掘</a></li>
            </ul>
        </section>


    <section>
        <h1>Tags</h1>
            <a href="./tag/python.html">python</a>, 
            <a href="./tag/numpy.html">numpy</a>, 
            <a href="./tag/deep-learning.html">deep-learning</a>, 
            <a href="./tag/algorithms.html">algorithms</a>, 
            <a href="./tag/wen-ben-wa-jue.html">文本挖掘</a>, 
            <a href="./tag/shen-jing-wang-luo.html">神经网络</a>, 
            <a href="./tag/shu-xue-ji-chu.html">数学基础</a>, 
            <a href="./tag/nlp.html">nlp</a>, 
            <a href="./tag/tf-example.html">tf-example</a>, 
            <a href="./tag/tui-jian-xi-tong.html">推荐系统</a>, 
            <a href="./tag/tf.html">tf</a>, 
            <a href="./tag/ji-huo-han-shu.html">激活函数</a>, 
            <a href="./tag/mapreduce.html">mapreduce</a>, 
            <a href="./tag/spark.html">spark</a>, 
            <a href="./tag/handbook.html">handbook</a>, 
            <a href="./tag/matplotlib.html">matplotlib</a>, 
            <a href="./tag/scikit-learn.html">scikit-learn</a>, 
            <a href="./tag/latex.html">latex</a>, 
            <a href="./tag/pandas.html">pandas</a>, 
            <a href="./tag/jupyter.html">jupyter</a>, 
            <a href="./tag/plot.html">plot</a>, 
            <a href="./tag/pip.html">pip</a>, 
            <a href="./tag/geng-xin-suo-you-mo-kuai.html">更新所有模块</a>, 
            <a href="./tag/shen-du-xue-xi.html">深度学习</a>, 
            <a href="./tag/xun-huan-shen-jing-wang-luo.html">循环神经网络</a>, 
            <a href="./tag/pangrank.html">PangRank</a>, 
            <a href="./tag/book.html">book</a>, 
            <a href="./tag/pydata.html">pydata</a>, 
            <a href="./tag/shell.html">shell</a>, 
            <a href="./tag/pyhton.html">pyhton</a>
    </section>



    <section>
        <h1>GitHub Repos</h1>
            <a href="https://github.com/1007530194">@1007530194</a> on GitHub
    </section>

</aside>    </div>
</div>
<footer role="contentinfo"><p>
        活到老，学到老，玩到老
    <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>

    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-132396898-1', 'auto');

    ga('require', 'displayfeatures');
    ga('send', 'pageview');
    </script>
</body>
</html>