<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!-->
<html class="no-js" lang="en"><!--<![endif]-->
    <head>
<meta charset="utf-8">
<title>algorithms-Boost简介 &mdash; 魑魅魍魉</title>

<meta name="author" content="niult">






<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">



<link href="../../../theme/css/main.css" media="screen, projection"
      rel="stylesheet" type="text/css">

<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
      rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
      rel="stylesheet" type="text/css">
</head>

<body>

<script src="../../../theme/js/modernizr-2.0.js"></script>
<script src="../../../theme/js/ender.js"></script>
<script src="../../../theme/js/octopress.js" type="text/javascript"></script>
<script src="../../../theme/js/echarts.min.js" type="text/javascript"></script>
<script src="../../../theme/js/require.min.js" type="text/javascript"></script>

<header role="banner"><hgroup>
  <h1><a href="../../../">魑魅魍魉</a></h1>
</hgroup></header>
<nav role="navigation">    <ul class="subscription" data-subscription="rss">
    </ul>


<ul class="main-navigation">
            <li >
                <a href="../../../category/01chang yong gong ju.html">01常用工具</a>
            </li>
            <li >
                <a href="../../../category/02.wo ai du shu.html">02.我爱读书</a>
            </li>
            <li class="active">
                <a href="../../../category/algorithms.html">Algorithms</a>
            </li>
            <li >
                <a href="../../../category/book.html">Book</a>
            </li>
            <li >
                <a href="../../../category/book-pydata.html">Book-pydata</a>
            </li>
            <li >
                <a href="../../../category/deep-learning-with-python.html">Deep-learning-with-python</a>
            </li>
            <li >
                <a href="../../../category/ji qi xue xi shi zhan.html">机器学习实战</a>
            </li>
            <li >
                <a href="../../../category/ke wai du wu.html">课外读物</a>
            </li>
            <li >
                <a href="../../../category/ling ji chu ru men shen du xue xi.html">零基础入门深度学习</a>
            </li>
            <li >
                <a href="../../../category/shen du xue xi.html">深度学习</a>
            </li>
            <li >
                <a href="../../../category/shen jing wang luo.html">神经网络</a>
            </li>
            <li >
                <a href="../../../category/shu ju wa jue.html">数据挖掘</a>
            </li>
            <li >
                <a href="../../../category/shu xue ji chu.html">数学基础</a>
            </li>
            <li >
                <a href="../../../category/tf-example.html">Tf-example</a>
            </li>
            <li >
                <a href="../../../category/tool1.html">Tool1</a>
            </li>
            <li >
                <a href="../../../category/tool2.html">Tool2</a>
            </li>
            <li >
                <a href="../../../category/tools.html">Tools</a>
            </li>
            <li >
                <a href="../../../category/tui jian xi tong.html">推荐系统</a>
            </li>
            <li >
                <a href="../../../category/wen ben wa jue.html">文本挖掘</a>
            </li>
</ul>
</nav>
<div id="main">
    <div id="content">
    <div>

        <h4>Contents</h4>
        

        <article class="hentry" role="article">
<header>
        <h1 class="entry-title">algorithms-Boost简介</h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content"><style type="text/css">/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI colors. */
.ansibold {
  font-weight: bold;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  border-left-width: 1px;
  padding-left: 5px;
  background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%);
}
div.cell.jupyter-soft-selected {
  border-left-color: #90CAF9;
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected {
  border-color: #ababab;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%);
}
@media print {
  div.cell.selected {
    border-color: transparent;
  }
}
div.cell.selected.jupyter-soft-selected {
  border-left-width: 0;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%);
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%);
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  padding: 0.4em;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */
  /* .CodeMirror-lines */
  padding: 0;
  border: 0;
  border-radius: 0;
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area 
div.output_area 
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}



.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}






.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}








.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}


.rendered_html pre,



.rendered_html tr,
.rendered_html th,

.rendered_html td,


.rendered_html * + table {
  margin-top: 1em;
}

.rendered_html * + p {
  margin-top: 1em;
}

.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,

.rendered_html img.unconfined,

div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered 
.text_cell.unrendered .text_cell_render {
  display: none;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
</style>
<style type="text/css">.highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */</style>
<style type="text/css">
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }
</style><body>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Boost-简介">Boost 简介<a class="anchor-link" href="#Boost-简介">¶</a></h1><h2 id="写在前面">写在前面<a class="anchor-link" href="#写在前面">¶</a></h2><p>提升（boosting）方法是一类应用广泛且非常有效的统计学习方法。</p>
<p>在2006年，Caruana和Niculescu-Mizil等人完成了一项实验，比较当今世界上现成的分类器（off-the-shelf classifiers）中哪个最好？实现结果表明Boosted Decision Tree（提升决策树）不管是在misclassification error还是produce well-calibrated probabilities方面都是最好的分离器，以ROC曲线作为衡量指标。（效果第二好的方法是随机森林）</p>
<blockquote><p>参见paper：《An Empirical Comparison of Supervised Learning Algorithms》ICML2006.</p>
</blockquote>
<p>下图给出的是Adaboost算法（Decision Stump as Weak Learner）在处理二类分类问题时，随着弱分类器的个数增加，训练误差与测试误差的曲线图。</p>
<table>
<tr>
<td><img src="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_6_0_0_adboost_binary_classification_1.png"/>损失函数示意图</td>
<td><img src="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_6_0_0_adboost_binary_classification_2.png"/>损失函数示意图</td>
</tr>
</table><p>从图中可以看出，Adaboost算法随着模型复杂度的增加，测试误差（红色点线）基本保持稳定，并没有出现过拟合的现象。</p>
<p>其实不仅是Adaboost算法有这种表现，Boosting方法的学习思想和模型结构上可以保证其不容易产生过拟合（除非Weak Learner本身出现过拟合）。</p>
<p>下面我们主要是从损失函数的差异，来介绍Boosting的家族成员；然后我们针对每个具体的家族成员，详细介绍其学习过程和核心公式；最后从算法应用场景和工具方法给出简单的介绍。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Boosting介绍">Boosting介绍<a class="anchor-link" href="#Boosting介绍">¶</a></h2><h3 id="基本思想">基本思想<a class="anchor-link" href="#基本思想">¶</a></h3><p>Boosting方法基于这样一种思想：</p>
<blockquote><p>对于一个复杂任务来说，将多个专家的判定进行适当的综合得出的判断，要比其中任何一个专家单独的判断好。很容易理解，就是”三个臭皮匠顶个诸葛亮”的意思…😄😄😄。</p>
</blockquote>
<h3 id="历史由来">历史由来<a class="anchor-link" href="#历史由来">¶</a></h3><p>历史上，Kearns和Valiant首先提出了”<strong>强可学习（strongly learnable）</strong>”和“<strong>弱可学习（weakly learnable）</strong>”的概念。他们指出：</p>
<blockquote><p><strong>在概率近似正确（probably approximately correct，PAC）学习框架中</strong>：</p>
<p>①. 一个概念（一个类，label），如果存在一个多项式的学习算法能够学习它，<strong>并且正确率很高</strong>，那么就称这个概念是强可学习的；</p>
<p>②. 一个概念（一个类，label），如果存在一个多项式的学习算法能够学习它，<strong>学习的正确率仅比随机猜测略好</strong>，那么就称这个概念是弱可学习的。</p>
<p>Schapire后来证明了: <strong>强可学习和弱可学习是等价的</strong>。 也就是说，在<strong>PAC学习的框架下，一个概念是强可学习的</strong> 充分必要条件 是 <strong>这个概念是弱可学习的</strong>。 表示如下：</p>
<p>强可学习⇔弱可学习</p>
</blockquote>
<p>如此一来，问题便成为：在学习中，如果已经发现了”弱学习算法”，那么能否将它提升为”强学习算法”？ 通常的，发现弱学习算法通常要比发现强学习算法容易得多。那么如何具体实施提升，便成为开发提升方法时所要解决的问题。关于提升方法的研究很多，最具代表性的当数AdaBoost算法（是1995年由Freund和Schapire提出的）。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Boosting学习思路">Boosting学习思路<a class="anchor-link" href="#Boosting学习思路">¶</a></h3><p>对于一个学习问题来说（以分类问题为例），给定训练数据集，求一个弱学习算法要比求一个强学习算法要容易的多。Boosting方法就是从弱学习算法出发，反复学习，得到一系列弱分类器，然后组合弱分类器，得到一个强分类器。Boosting方法在学习过程中通过改变训练数据的权值分布，针对不同的数据分布调用弱学习算法得到一系列弱分类器。</p>
<p>这里面有两个问题需要回答：</p>
<p>1.在每一轮学习之前，如何改变训练数据的权值分布？</p>
<p>2.如何将一组弱分类器组合成一个强分类器？</p>
<blockquote><p>具体不同的boosting实现，主要区别在弱学习算法本身和上面两个问题的回答上。</p>
</blockquote>
<p>针对第一个问题，Adaboost算法的做法是：</p>
<p><strong>提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值</strong>。</p>
<blockquote><p>如此，那些没有得到正确分类的样本，由于其权值加大而受到后一轮的弱分类器的更大关注。</p>
</blockquote>
<p>第二个问题，弱分类器的组合，AdaBoost采取加权多数表决的方法。具体地：</p>
<p><strong>加大 分类误差率小 的弱分类器的权值，使其在表决中起较大的作用；减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用</strong>。</p>
<p>AdaBoost算法的巧妙之处就在于它将这些学习思路自然并且有效地在一个算法里面实现。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="前向分步加法模型">前向分步加法模型<a class="anchor-link" href="#前向分步加法模型">¶</a></h1><p>英文名称：Forward Stagewise Additive Modeling</p>
<h2 id="加法模型（addtive-model）">加法模型（addtive model）<a class="anchor-link" href="#加法模型（addtive-model）">¶</a></h2><p>$$f(x)=\sum^K_{k=1}\beta_k \cdot b(x;\gamma_k)$$</p>
<p>其中，$b(x;\gamma_k)$ 为基函数，$\gamma_k$为基函数的参数，$\beta_k$为基函数的系数。</p>
<h2 id="前向分步算法">前向分步算法<a class="anchor-link" href="#前向分步算法">¶</a></h2><p>在给定训练数据及损失函数$L(y,f(x))$的条件下，学习加法模型$f(x)$成为经验风险极小化即损失函数极小化的问题：</p>
<p>$$\min\limits_{\beta_k,\gamma_k}\sum^M_{i=1}L\Big[y^{(i)},\sum^K_{k=1}\beta_kb(x^{(i)};\gamma_k) \Big]$$
通常这是一个复杂的优化问题。前向分布算法（forward stagwise algorithm）求解这一优化问题的思路是：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式，那么就可以简化优化的复杂度。具体地，每步只需优化如下损失函数：</p>
<blockquote><p>$$\min\limits_{\beta,\gamma}\sum^M_{i=1}L\Big(y^{(i)},\beta b(x^{(i)};\gamma) \Big)$$</p>
</blockquote>
<p>给定训练数据集$D={(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),⋯,(x^{(M)},y^{(M)})},x^{(i)}\in R^n,y^{(i)}\in [−1,+1]$。损失函数$L(y,f(x))$和基函数的集合${b(x;\gamma)}$，学习加法模型$f(x)$的前向分步算法如下：</p>
<blockquote><p>{<br/>
输入：训练数据集$D={(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)})}$;<br/>
 $\qquad$ 损失函数$L(y,f(x))$;<br/>
 $\qquad$ 基函数集${b(x,\gamma)}$;<br/>
输出：加法模型$f(x)$。
计算过程：<br/>
 $\qquad$ (1).初始化$f_0(x)=0$<br/>
 $\qquad$ (2).对于$k=1,2,⋯,K$<br/>
 $\qquad\quad$ (a).极小化损失函数<br/>
 $\qquad\qquad\qquad (\beta_k,\gamma_k)=\arg min_{\beta,\gamma}\sum^M_{i=1}L(y^{(i)},f_{k−1}(x^{(i)})+\beta b(x;\gamma))$<br/>
 $\qquad\quad$ 得到参数$\beta_k,\gamma_k$.<br/>
 $\qquad\quad$ (b).更新<br/>
 $\qquad\qquad\qquad f_k(x)=f_{k−1}(x)+\beta _kb(x;\gamma_k)$<br/>
(3).得到加法模型<br/>
 $\qquad\qquad\qquad f(x)=f_K(x)=\sum^K_{k=1}\beta_kb(x;\gamma_k)$<br/>
}</p>
</blockquote>
<p>这样前向分步算法将同时求解从$k=1$到$K$的所有参数$\beta_k$,$\gamma_k$的优化问题简化为逐次求解各个$\beta_k,\gamma_k$的优化问题。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Boosting四大家族">Boosting四大家族<a class="anchor-link" href="#Boosting四大家族">¶</a></h1><p>Boosting并非是一个方法，而是一类方法。这里按照损失函数的不同，将其细分为若干类算法，下表给出了4种不同损失函数对应的Boosting方法：</p>
<table>
<thead><tr>
<th style="text-align:center">名称(Name)</th>
<th style="text-align:center">损失函数(Loss)</th>
<th style="text-align:center">导数(Derivative)</th>
<th style="text-align:center">目标函数$f^∗$</th>
<th style="text-align:center">算法</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">平方损失(Squared Error)</td>
<td style="text-align:center">$\frac{1}{2}(y^{(i)}−f(x^{(i)}))^2$</td>
<td style="text-align:center">$y^{(i)}−f(x^{(i)})$</td>
<td style="text-align:center">$E[y\</td>
<td style="text-align:center">x(i)]$</td>
<td>L2Boosting</td>
</tr>
<tr>
<td style="text-align:center">绝对损失(Absolute Error)</td>
<td style="text-align:center">$y^{(i)}−f(x^{(i)})$</td>
<td style="text-align:center">$sign(y^{(i)}−f(x^{(i)}$)</td>
<td style="text-align:center">$median(y\</td>
<td style="text-align:center">x^{(i)})$</td>
<td>Gradient Boosting</td>
</tr>
<tr>
<td style="text-align:center">指数损失(Exponentail Loss)</td>
<td style="text-align:center">$exp(−y^{(i)}~f(x^{(i)}))$</td>
<td style="text-align:center">$−y^{(i)}~exp(−y^{(i)}~f(x^{(i)}))$</td>
<td style="text-align:center">$\frac{1}{2}log\frac{πi}{1−πi} $</td>
<td style="text-align:center">AdaBoost</td>
</tr>
<tr>
<td style="text-align:center">对数损失(LogLoss)</td>
<td style="text-align:center">$log(1+e^{−y^{(i)}~f^i})$</td>
<td style="text-align:center">$y^{(i)}−πi$</td>
<td style="text-align:center">$\frac{1}{2}log\frac{πi}{1−πi}$</td>
<td style="text-align:center">LogitBoost</td>
</tr>
</tbody>
</table>
<blockquote><p>说明：<br/>
该表来自于Machine Learning: A Probabilistic PerspectiveP587页<br/>
L2Boosting全称：Least Squares Boosting；该算法由Buhlmann和Yu在2003年提出。</p>
</blockquote>
<p>二分类问题时损失函数示意图：
<img src="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_6_1_1_loss_function_graph.png" width="400/"/></p>
<p>下面主要以AdaBoost算法作为示例，给出以下3个问题的解释：</p>
<p>AdaBoost为什么能够提升学习精度？</p>
<ul>
<li>如何解释AdaBoost算法？</li>
<li>Boosting方法更具体的实例－Boosting Tree。</li>
<li>下面首先介绍Adaboost算法。</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Adaboost">Adaboost<a class="anchor-link" href="#Adaboost">¶</a></h1><h2 id="算法学习过程">算法学习过程<a class="anchor-link" href="#算法学习过程">¶</a></h2><p>Adaboost算法在分类问题中的主要特点：通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类性能。 AdaBoost－算法描述（伪代码）如下：</p>
<p>(
\begin{align}
&amp;\{ \\
&amp;\quad 输入：训练数据集D=\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(M)},y^{(M)})\}，x^{(i)} \in \mathcal{X} \subseteq R^N，\\
&amp;\qquad\qquad y^{(i)} \in \mathcal{Y} = \{-1, +1\}, 弱分类器; \\
&amp;\quad 输出：最终分类器G(x). \\
&amp;\quad 过程：\\
&amp;\qquad (1). 初始化训练数据的权值分布 \\
&amp;\qquad\qquad\qquad D_1=(w_{11}, w_{12}, \cdots, w_{1M}), \quad w_{1i}=\frac{1}{M}, \; i=1,2,\cdots,M \\
&amp;\qquad (2). 训练K个弱分类器 k=1,2,\cdots,K \\
&amp;\qquad\qquad (a). 使用具有权值分布D_k的训练数据集学习，得到基本分类器 \\
&amp;\qquad\qquad\qquad\qquad G_k(x): \mathcal{X} \rightarrow \{-1, +1\} \\
&amp;\qquad\qquad (b). 计算G_k(x)在训练数据集上的分类误差率 \\
&amp;\qquad\qquad\qquad\qquad e_k = P(G_k(x^{(i)}) \not= y^{(i)}) = \sum_{i=1}^{M} w_{ki} I(G_k(x^{(i)}) \not= y^{(i)})\\
&amp;\qquad\qquad (c). 计算G_k(x)的系数 \\
&amp;\qquad\qquad\qquad\qquad \alpha_k = \frac{1}{2} \log \frac{1-e_k}{e_k} \quad(e是自然对数) \\
&amp;\qquad\qquad (d). 更新训练数据集的权值分布 \\
&amp;\qquad\qquad\qquad\qquad D_{k+1} = (w_{k+1,1}, w_{k+1,2}, \cdots, w_{k+1,M})\\
&amp;\qquad\qquad\qquad\qquad w_{k+1,i} = \frac{w_{k,i}}{Z_k} \exp(-\alpha_k y^{(i)} G_k(x^{(i)})), \quad i=1,2,\cdots,M \quad\\
&amp;\qquad\qquad\qquad Z_k是规范化因子 \\
&amp;\qquad\qquad\qquad\qquad Z_k = \sum_{i=1}^{M} w_{k,i} \cdot \exp(-\alpha_k y^{(i)} G_k(x^{(i)})) \\
&amp;\qquad\qquad\qquad 使D_{k+1}成为一个概率分布。\\
&amp;\qquad (3). 构建基本分类器的线性组合 \\
&amp;\qquad\qquad\qquad f(x) = \sum_{k=1}^{K} \alpha_k G_k(x) \\
&amp;\qquad 得到最终的分类器 \\
&amp;\qquad\qquad G(x) = sign (f(x)) = sign \left(\sum_{k=1}^{K} \alpha_k G_k(x) \right) \\
&amp;\}
\end{align}
)</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="AdaBoost算法描述说明">AdaBoost算法描述说明<a class="anchor-link" href="#AdaBoost算法描述说明">¶</a></h2><ul>
<li>步骤（1）假设训练数据集具有均匀（相同）的权值分布，即每个训练样本在基本分类器的学习中作用相同。</li>
</ul>
<blockquote><p>这一假设保证，第一步能在原始数据上学习基本分类器$G_1(x)$。</p>
</blockquote>
<ul>
<li>步骤（2）AdaBoost反复学习基本分类器，在每一轮$k=1,2,\cdots,K$顺序地执行下列操作：</li>
</ul>
<blockquote><p>（a）学习基本分类器：使用当前分布$D_k$加权的训练数据集，学习基本分类器$G_k(x)$；<br/>
（b）误差率：计算基本分类器$G_k(x)$在加权训练数据集上的分类误差率</p>
<p>$$
e_k = P(G_k(x^{(i)}) \not= y^{(i)}) = \sum_{G_k(x^{(i)}) \not= y^{(i)}} w_{ki}
$$</p>
<p>这里，$w_{ki}$表示第$k$轮中第$i$个样本的权值，$\sum_{i=1}^{M} w_{ki} = 1$。</p>
<p>这表明，$G_k(x)$在加权的训练数据集上的分类误差率 是 被$G_k(x)$误分类样本的权值之和。由此可以看出数据权值分布$D_k$与基本分类器$G_k(x)$的分类误差率的关系。</p>
<p>（c）分类器权重：计算基本分类器$G_k(x)$的系数$\alpha_k$，$\alpha_k$表示$G_k(x)$在最终分类器中的重要性</p>
<p>根据公式可知，当$e_k \le 0.5$时，$\alpha_k \ge 0$，并且$\alpha_k$随着$e_k$的减小而增大，所以分类误差率越小的基本分类器在最终分类器中的作用越大。</p>
<p>（d）更新训练数据的权值分布，为下一轮做准备，公式可以写成：</p>
<p>$$
w_{k+1, i} =
\begin{cases}
\frac{w_{ki}} {Z_k} e^{-\alpha_k}, &amp;\quad G_k(x^{(i)}) = y^{(i)} \\
\frac{w_{ki}} {Z_k} e^{\alpha_k}, &amp;\quad G_k(x^{(i)}) \not= y^{(i)}
\end{cases}
$$</p>
<p>由此可知，被基本分类器$G_k(x)$误分类样本的权值得以扩大，而被正确分类样本的权值却得以缩小。相比较来说，误分类样本的权值被放大$e^{2\alpha_k} = \frac{e_k}{1-e_k}$倍。因此，误分类样本在下一轮学习中起更大的作用。</p>
<p>不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器中起不同的作用，这也是AdaBoost的一个特点。</p>
</blockquote>
<ul>
<li>步骤（3）线性组合$f(x)$实现$K$个基本分类器的加权表决。系数$\alpha_k$表示了极本分类器$G_k(x)$的重要性。</li>
</ul>
<p>注意：在这里所有$\alpha_k$之和并不为1。$f(x)$的符号决定实例$x$的类别，$f(x)$的绝对值表示分类的精确度。</p>
<p>利用基本分类器的线性组合构建最终分类器是AdaBoost的另一个特点。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="示例：AdaBoost算法">示例：AdaBoost算法<a class="anchor-link" href="#示例：AdaBoost算法">¶</a></h2><p>此示例参考李航老师的《统计学习方法》.</p>
<p>给定下表所示训练数据。</p>
<table>
<thead><tr>
<th style="text-align:center">序号</th>
<th style="text-align:center">1</th>
<th style="text-align:center">2</th>
<th style="text-align:center">3</th>
<th style="text-align:center">4</th>
<th style="text-align:center">5</th>
<th style="text-align:center">6</th>
<th style="text-align:center">7</th>
<th style="text-align:center">8</th>
<th>9</th>
<th>10</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">x</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">2</td>
<td style="text-align:center">3</td>
<td style="text-align:center">4</td>
<td style="text-align:center">5</td>
<td style="text-align:center">6</td>
<td style="text-align:center">7</td>
<td>8</td>
<td>9</td>
</tr>
<tr>
<td style="text-align:center">y</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">-1</td>
<td style="text-align:center">-1</td>
<td style="text-align:center">-1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td>1</td>
<td>-1</td>
</tr>
</tbody>
</table>
<p>假设弱分类器由(x \le v 或 x&gt;v)产生，其阈值(v)使该分类器在训练数据集上分类误差率最低。试用AdaBoost算法学习一个强分类器。</p>
<p>解： 首先初始化数据权值分布（均匀分布）：</p>
<p>$$
D_1 = (w_{1,1}, w_{1,2}, \cdots, w_{1,10}), \quad w_{1,i}=0.1, \; i=1,2,\cdots,10
$$</p>
<p>对(k=1)，</p>
<p>(a). 在权值分布为$D_1$的训练数据上，阈值$v$取2.5时，分类误差率最低，故基本分类器为：</p>
<p>$$
G_1(x) =
\begin{cases}
1,&amp; \quad x &lt; 2.5 \\
-1, &amp; \quad x &gt; 2.5
\end{cases}
$$</p>
<p>(b). (G_1(x))在训练数据集上的误差率$e_1=P(G_1(x^{(i)}) \not= y^{(i)})=0.3$;</p>
<p>(c). 计算(G_1(x))的系数：$\alpha_1 = \frac{1}{2} \log \frac{1-e_1}{e_1} = 0.4236$；</p>
<p>(d). 更新训练数据的权值分布:</p>
<p>$$
\begin{align}
&amp; D_2 = (w_{2,1}, w_{2,2}, \cdots, w_{2,10}) \\
&amp; w_{2,i} = \frac{w_{1,i}} {Z_1} \exp(-\alpha_1 y^{(i)} G_1(x^{(i)})), \quad i=1,2,\cdots,10 \\
&amp; D_2 = (0.0715, 0.0715, 0.0715, 0.0715, 0.0715, 0.0715, \\
&amp;\qquad\;\; 0.1666, 0.1666, 0.1666, 0.0715) \\
&amp; f_1(x) = 0.4236 G_1(x)
\end{align}
$$</p>
<p>分类器(sign[f_1(x)])在训练数据上有3个误分类点。</p>
<p>对(k=2)，</p>
<p>(a). 在权值分布为(D_2)的训练数据上，阈值(v)取8.5时，分类误差率最低，基本分类器为：</p>
<p>$$
G_2(x) =
\begin{cases}
1,&amp; \quad x &lt; 8.5 \\
-1, &amp; \quad x &gt; 8.5
\end{cases}
$$</p>
<p>(b). (G_2(x))在训练数据集上的误差率(e_2=0.2143);</p>
<p>(c). 计算(G_2(x))的系数：(\alpha_2 = 0.6496)；</p>
<p>(d). 更新训练数据的权值分布:</p>
<p>$$
\begin{align}
&amp; D_3=(0.0455, 0.0455, 0.0455, 0.1667, 0.1667, 0.1667, \\
&amp;\qquad\;\; 0.1060, 0.1060, 0.1060, 0.0455) \\
&amp; f_2(x) = 0.4236 \cdot G_1(x) + 0.6496 \cdot G_2(x)
\end{align}
$$</p>
<p>分类器(sign[f_2(x)])在训练数据上有3个误分类点。</p>
<p>对(k=3)，</p>
<p>(a). 在权值分布为(D_3)的训练数据上，阈值(v)取5.5时，分类误差率最低，基本分类器为：</p>
<p>$$
G_3(x) =
\begin{cases}
1,&amp; \quad x &lt; 5.5 \\
-1, &amp; \quad x &gt; 5.5
\end{cases}
$$</p>
<p>(b). (G_3(x))在训练数据集上的误差率(e_3=0.1820);</p>
<p>(c). 计算(G_3(x))的系数：(\alpha_2 = 0.7514)；</p>
<p>(d). 更新训练数据的权值分布:</p>
<p>$$
\begin{align}
&amp; D_4=(0.125, 0.125, 0.125, 0.102, 0.102, 0.102, 0.065, 0.065, 0.065, 0.125)
\end{align}
$$</p>
<p>于是得到模型线性组合</p>
<p>$$
f_3(x) = 0.4236 \cdot G_1(x) + 0.6496 \cdot G_2(x) + 0.7514 \cdot G_3(x)
$$</p>
<p>分类器(sign[f_3(x)])在训练数据上误分类点个数为0。</p>
<p>于是最终分类器为：</p>
<p>$$
\begin{align}
G(x) &amp;= sign[f_3(x)] \\
&amp;= sign[0.4236 \cdot G_1(x) + 0.6496 \cdot G_2(x) + 0.7514 \cdot G_3(x)]
\end{align}
$$</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="训练误差分析">训练误差分析<a class="anchor-link" href="#训练误差分析">¶</a></h2><p>AdaBoost算法最基本的性质是它能在学习过程中不断减少训练误差，即在训练数据集上的分类误差率。 对于这个问题，有个定理可以保证分类误差率在减少－AdaBoost的训练误差界。</p>
<p>定理：AdaBoost训练误差界
［定理］AdaBoost训练误差界
$$ \frac{1}{M} \sum_{i=1}^{M} I(G(x^{(i)}) \not= y^{(i)}) \le \frac{1}{M} \sum_{i} \exp(-y^{(i)} f(x^{(i)})) = \prod_{k=1}^{K} Z_k $$
其中，$G(x), f(x)$和$Z_k$分别由公式给出
证明如下：</p>
<p>当$G(x^{(i)}) \not= y^{(i)}$时，$y^{(i)} f(x^{(i)}) &lt; 0$，因而$\exp(-y^{(i)} f(x^{(i)})) \ge 1$。由此，可以直接推导出前半部分。</p>
<p>后半部分的推导要用到$Z_k$的定义式$(ml.1.6.7)$和$(ml.1.6.6)$的变形:</p>
<p>$$
w_{k,i} \exp(-\alpha_k y^{(i)} G_k(x^{(i)})) = Z_k w_{k+1,i}
$$</p>
<p>推导如下：</p>
<p>$$
\begin{align}
\frac{1}{M} \sum_{i=1}^{M} \exp(-y^{(i)} f(x^{(i)})) &amp;= \underline{ \frac{1}{M} \sum_{i=1}^{M} } \exp \left(-\sum_{k=1}^{K} \alpha_k y^{(i)} G_k(x^{(i)}) \right) \\
&amp;= \underline{ \sum_{i=1}^{M} w_{1,i} } \prod_{k=1}^{K} \exp(-\alpha_k y^{(i)} G_k(x^{(i)})) \\
&amp;= \sum_{i=1}^{M} w_{1,i} \underline{ \exp(-\alpha_1 y^{(i)}) G_k(x^{(i)}) \prod_{k=2}^{K} \exp(-\alpha_k y^{(i)} G_k(x^{(i)})) } \\
&amp;= Z_1 \sum_{i=1}^{M} w_{2,i} \prod_{k=2}^{K} \exp(-\alpha_k y^{(i)} G_k(x^{(i)})) \\
&amp;= Z_1 Z_2 \sum_{i=1}^{M} w_{3,i} \prod_{k=3}^{K} \exp(-\alpha_k y^{(i)} G_k(x^{(i)})) \\
&amp;= \cdots \cdots \\
&amp;= Z_1 Z_2 \cdots Z_{K-1} \sum_{i=1}^{M} w_{K,i} \exp(-\alpha_K y^{(i)} G_K(x^{(i)})) \\
&amp;= \prod_{k=1}^{K} Z_k
\end{align}
$$</p>
<p>注意：$w_{1,i} = \frac{1}{M}$</p>
<p>这一定理说明：可以在每一轮选取适当的$G_k$使得$Z_k$最小，从而使训练误差下降最快。 对于二类分类问题，有如下定理。</p>
<p>定理：二类分类问题AdaBoost训练误差界
［定理］二类分类问题AdaBoost训练误差界
$$\prod_{k=1}^{K} Z_k = \prod_{k=1}^{K} \left[2 \sqrt{e_k(1-e_k)} \;\right] = \prod_{k=1}^{K} \sqrt{(1-4\gamma_k^2)} \le \exp \left(-2 \sum_{k=1}^{K} \gamma_k^2 \right)$$
这里，$\gamma_k = 0.5 - e_k $
证明：由公式可得：</p>
<p>$$
\begin{align}
Z_k &amp;= \sum_{i=1}^{M} w_{k,i} \exp(-\alpha_k y^{(i)} G_k(x^{(i)})) \\
&amp;= \sum_{y^{(i)} = G_k(x^{(i)})} w_{k,i} \cdot e^{-\alpha_k} + \sum_{y^{(i)} \not= G_k(x^{(i)})} w_{k,i} \cdot e^{\alpha_k} \\
&amp;= (1-e_k) \cdot e^{-\alpha_k} + e_k \cdot e^{\alpha_k} \\
&amp;= 2 \sqrt{e_k (1-e_k)} = \sqrt{1-4\gamma_m^2}
\end{align}
$$</p>
<p>注：$\alpha_k = \frac{1}{2} \log \frac{1-e_k}{e_k}, e^{\alpha_k} = \sqrt{\frac{1-e_k}{e_k}}$</p>
<p>对于不等式部分</p>
<p>$$
\prod_{k=1}^{K} \sqrt{1-4\gamma_m^2} \le \exp \left(-2 \sum_{k=1}^{K} \gamma_k^2 \right) 
$$</p>
<p>则可根据$e^x$和$\sqrt{1-x}$在点$x=0$的泰勒展开式推出不等式$\sqrt{1-4\gamma_m^2} \le \exp(-2 \gamma_m^2)$。</p>
<p>[推论] AdaBoost训练误差指数速率下降
如果存在$\gamma &gt; 0$，对所有的$m$有$\gamma_k \ge \gamma$，则有
$$\frac{1}{M} \sum_{i=1}^{M} I(G(x^{(i)}) \not= y^{(i)}) \le \exp(-2K\gamma^2)$$|</p>
<p>推论表明，在此条件下，AdaBoost的训练误差是以指数速率下降的。这一性质对于AdaBoost计算（迭代）效率是利好消息。</p>
<p>注意：AdaBoost算法不需要知道下界$\gamma$，这正是Freund和Schapire设计AdaBoost时所考虑的。与一些早期的提升方法不同，AdaBoost具有适应性，即它能适应弱分类器各自的训练误差率。这也是其算法名称的由来（适应的提升）。Ada是Adaptive的简写。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="前向分步加法模型与Adaboost">前向分步加法模型与Adaboost<a class="anchor-link" href="#前向分步加法模型与Adaboost">¶</a></h1><p>AdaBoost算法还有另一个解释，即可以认为AdaBoost算法是模型为加法模型、损失函数为指数函数、学习算法为前向分步算法时的学习方法。</p>
<p>根据前向分步算法可以推导出AdaBoost，用一句话叙述这一关系.</p>
<p>AdaBoost算法是前向分步加法算法的特例
此时，模型是由基本分类器组成的加法模型，损失函数是指数函数。</p>
<p>证明：前向分步算法学习的是加法模型，当基函数为基本分类器时，该加法模型等价于AdaBoost的最终分类器：</p>
<p>$$
f(x) = \sum_{k=1}^{K} \alpha_k G_k(x)
$$</p>
<p>由基本分类器$G_k(x)$及其系数$\alpha_k$组成，$k=1,2,\cdots,K$。前向分步算法逐一学习基函数，这一过程与AdaBoost算法逐一学习基本分类器的过程一致。</p>
<p>下面证明：</p>
<p>前向分步算法的损失函数是指数损失函数（Exponential）$L(y, f(x)) = \exp [-y f(x)]$ 时，其学习的具体操作等价于AdaBoost算法学习的具体操作。</p>
<p>假设经过$k-1$轮迭代，前向分步算法已经得到$f_{k-1}(x)$:</p>
<p>$$
\begin{align}
f_{k-1} (x) &amp;= f_{k-2}(x) + \alpha_{k-1} G_{k-1}(x) \\
&amp;= \alpha_1 G_1(x) + \cdots + \alpha_{m-1} G_{m-1}(x)
\end{align}
$$</p>
<p>在第$k$轮迭代得到$\alpha_k, G_k(x)$和$f_k(x)$。</p>
<p>$$
f_{k} (x) = f_{k-1}(x) + \alpha_{k} G_{k}(x)
$$</p>
<p>目标是使前向分步算法得到的$\alpha_k$和$G_k(x)$使$f_k(x)$在训练数据集$D$上的指数损失最小，即</p>
<p>$$
(\alpha_k, G_k(x)) = \arg \min_{\alpha, G} \underbrace{ \sum_{i=1}^{M} \exp [-y^{(i)} (f_{k-1}(x) + \alpha G(x^{(i)}))] }_{指数损失表达式}
$$</p>
<p>进一步可表示为：</p>
<p>$$
(\alpha_k, G_k(x)) = \arg \min_{\alpha, G} \sum_{i=1}^{M} \overline{w}_{k,i} \cdot \exp [-y^{(i)} \alpha G(x^{(i)})]
$$</p>
<p>其中，$\overline{w}_{k,i} = \exp [-y^{(i)} f_{k-1} (x^{(i)})]$表示第$i$样本在之前模型上的指数损失。因为$\overline{w}_{k,i}$既不依赖$\alpha$也不依赖$G$，所以与最小化无关。但$\overline{w}_{k,i}$依赖于$f_{k-1}(x)$，随着每一轮迭代而发生变化。</p>
<p>现在使公式$(n.ml.1.6.15)$达到最小的$\alpha_k^{\ast}$和$G_k^{\ast}$就是AdaBoost算法所得到的$\alpha_k$和$G_k(x)$。求解公式可分为两步：</p>
<p>第一步：求$G_k^{\ast}$. 对于任意$\alpha &gt; 0$，使公式最小的$G(x)$由下式得到：</p>
<p>$$
G_k^{\ast}(x) = \arg \min_{G} \sum_{i=1}^{M} \overline{w}_{k,i} \cdot I(y^{(i)} \not= G(x^{(i)})) \qquad(n.ml.1.6.16)
$$</p>
<p>此分类器$G_k^{\ast}(x)$即为AdaBoost算法的基本分类器$G_k(x)$，因为它是使第$k$轮加权训练数据分类误差率最小的基本分类器。</p>
<p>之后，求$\alpha_k^{\ast}$。参考公式$(n.ml.1.6.5)$，公式$(n.ml.1.6.15)$中的：</p>
<p>$$
\begin{align}
\sum_{i=1}^{M} \overline{w}_{k,i} \cdot \exp [-y^{(i)} \alpha G(x^{(i)})] &amp;= \sum_{y^{(i)} = G_k(x^{(i)})} \overline{w}_{k,i} \cdot e^{-\alpha} + \sum_{y^{(i)} \neq G_k(x^{(i)})} \overline{w}_{k,i} \cdot e^{\alpha} \\\
&amp;= (e^{\alpha} - e^{-\alpha}) \sum_{i=1}^{M} \overline{w}_{k,i} I(y^{(i)} \neq G(x^{(i)})) + e^{-\alpha} \sum_{i=1}^{M} \overline{w}_{k,i}
\end{align}
$$</p>
<p>把已得到的$G_k^{\ast}$带入公式$(n.ml.1.6.17)$，并对$\alpha$求导（导数为0），即得到使公式$(n.ml.1.6.15)$最小的$\alpha$。</p>
<p>$$
\alpha_{k}^{\ast} = \frac{1}{2} \log \frac{1-e_k}{e_k}
$$</p>
<p>$e_k$为分类误差率</p>
<p>$$
e_k = \frac{ \sum_{i=1}^{M} \overline{w}_{k,i} I(y^{(i)} \neq G_k(x^{(i)})) } {\sum_{i=1}^{M} \overline{w}_{k,i} } = \sum_{i=1}^{M} w_{k,i} I(y^{(i)} \neq G_k(x^{(i)}))
$$</p>
<p>可以看出，这里求得的$\alpha_k^{\ast}$与AdaBoost算法$(2)-(c)$步的$\alpha_k$完全一致。</p>
<p>最后看一下每一轮样本权值的更新。由：$f_k(x) = f_{k-1}(x) + \alpha_k G_k(x)$以及$\overline{w}_{k,i} = \exp[-y^{(i)} f_{k-1}(x^{(i)})]$，可得：</p>
<p>$$
\overline{w}_{k+1,i} = \overline{w}_{k,i} \exp[-y^{(i)} \alpha_k G_k(x)]
$$</p>
<p>这与Adaboost算法的第$(2)-(d)$步的样本权值的更新，可看出二者是等价的（只相差规范化因子）。</p>
<p>AdaBoost算法缺点</p>
<p>对异常点敏感</p>
<p>指数损失存在的一个问题是不断增加误分类样本的权重（指数上升）。如果数据样本是异常点（outlier），会极大的干扰后面基本分类器学习效果；</p>
<p>模型无法用于概率估计</p>
<p>MLAPP中的原话：”$e^{-\tilde{y}f}$ is not the logarithm of any pmf for binary variables $\tilde{y} \in \{-1, +1\}$; consequently we cannot recover probability estimate from $f(x)$.”</p>
<p>意思就是说对于取值为$\tilde{y} \in \{-1, +1\}$的随机变量来说，$e^{-\tilde{y}f}$不是任何概率密度函数的对数形式，模型$f(x)$的结果无法用概率解释。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Boosted-Decision-Tree">Boosted Decision Tree<a class="anchor-link" href="#Boosted-Decision-Tree">¶</a></h1><p>提升决策树是指以分类与回归树（CART）为基本分类器的提升方法，被认为是统计学习中性能最好的方法之一。</p>
<p>提升决策树简称提升树，Boosting Tree.</p>
<p>提升树模型
提升树模型实际采用加法模型（即基函数的线性组合）与前向分步算法，以决策树为基函数的提升方法称为提升树（Boosting Tree）。</p>
<p>对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。在6.1.3节AdaBoost例子中，基本分类器是$xv$，可以看作是由一个跟结点直接连接两个叶结点的简单决策树，即所谓的决策树桩（Decision Stump）。</p>
<p>提升树模型可以表示为CART决策树的加法模型：</p>
<p>$$
f_K(x) = \sum_{k=1}^{K} T(x; \Theta_k) \qquad(ml.1.6.13)
$$</p>
<p>其中，$T(x; \Theta_k)$表示二叉决策树，$\Theta_k$为决策树的参数，$K$为树的个数。</p>
<p>基本学习器－CART决策树，请参考第03章：深入浅出ML之Tree-Based家族</p>
<p>提升树算法
提升树算法采用前向分步算法。首先确定初始提升树$f_0(x) = 0$，第$k$步的模型为：</p>
<p>$$
f_k(x) = f_{k-1}(x) + T(x; \Theta_k) \qquad(ml.1.6.14)
$$</p>
<p>其中，$f_{k-1}(x)$为当前模型，通过经验风险极小化确定下一颗决策树的参数$\Theta_k$，</p>
<p>$$
\hat{\Theta}_k = \arg \min_{\Theta_k} \sum_{i=1}^{M} L(y^{(i)}, \; f_{k-1}(x) + T(x^{(i)}; \Theta_k)) \qquad(ml.1.6.15)
$$</p>
<p>由于树的线性组合可以很好的拟合训练数据，即使数据中的输入和输出之间的关系很复杂也是如此，所以提升树是一个高功能的学习算法。</p>
<p>提升树家族</p>
<p>不同问题的提升树学习算法，其主要区别在于损失函数不同。平方损失函数常用于回归问题，用指数损失函数用于分类问题，以及绝对损失函数用于决策问题。</p>
<p>二叉分类树</p>
<p>对于二类分类问题，提升树算法只需要将AdaBoost算法例子中的基本分类器限制为二叉分类树即可，可以说此时的决策树算法时AdaBoost算法的特殊情况。</p>
<p>损失函数仍为指数损失，提升树模型仍为前向加法模型。</p>
<p>二叉回归树</p>
<p>已知训练数据集$D=\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(M)}, y^{(M)})\}, x^{(i)} \in \mathcal{X} \subseteq R^n, y^{(i)} \in \mathcal{Y} $ $ \subseteq R, \mathcal{Y}$为输出空间。如果将输入空间$\mathcal{X}$划分为$J$个互不相交的区域$R_1, R_2, \cdots, R_J$，并且在每个区域上确定输出的常量$c_j$，那么树可以表示为：</p>
<p>$$
T(x; \Theta) = \sum_{j=1}^{J} c_j I(x \in R_j) \qquad(ml.1.6.16)
$$</p>
<p>其中，参数$\Theta=\{(R_1, c_1), (R_2, c_2), \cdots, (R_J, c_J)\}$表示树的区域划分和各区域上的常数。$J$是回归树的复杂度即叶结点的个数。</p>
<p>回归问题提升树－前向分步算法</p>
<p>回归问题提升树使用以下前向分步算法：</p>
<p>$$
\begin{align}
f_0(x) &amp;= 0 \\
f_k(x) &amp;= f_{k-1}(x) + T(x; \Theta_k) \quad k=1,2,\cdots,K \\
f_K(x) &amp;= \sum_{k=1}^{K} T(x; \Theta_k)
\end{align} \qquad(n.ml1.6.17)
$$</p>
<p>在前向分布算法的第$k$步，给定当前模型$f_{k-1}(x)$，需求解：</p>
<p>$$
\hat{\Theta}_k = \arg \min_{\Theta_k} \sum_{i=1}^{M} \underbrace{ L(y^{(i)}, \; f_{k-1}(x) + T(x^{(i)}; \Theta_k)) }_{损失函数} \qquad(n.ml.1.6.18)
$$</p>
<p>得到$\hat{\Theta}_k$，即第$k$颗树的参数。</p>
<p>当采用平方误差损失函数时，</p>
<p>$$
L(y, f(x)) = (y - f(x))^2 \qquad (n.ml.1.6.19)
$$</p>
<p>将平方误差损失函数展开为：</p>
<p>$$
\begin{align}
&amp;L(y, f_{k-1}(x) + T(x; \Theta_k)) \\
&amp;\qquad = [y - f_{k-1}(x) - T(x; \Theta_k)]^2 \\
&amp;\qquad = [r - T(x; \Theta_k)]^2
\end{align} \qquad(n.ml.1.6.20)
$$</p>
<p>这里$ r = y - f_{k-1}(x) $，表示当前模型的拟合数据的残差（residual）。所以，对回归问题的提升树算法来说，只需要简单地拟合当前模型的残差。</p>
<p>由于损失函数是平方损失，因此该方法属于L2Boosting的一种实现。</p>
<p>回归问题提升树－算法描述</p>
<p>$
\{ \\
\quad\, 输入：训练数据集D=\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(M)}, y^{(M)})\}, x^{(i)} \in \mathcal{X} \subseteq R^n, y^{(i)} \in \mathcal{Y}; \\
\quad 输出：提升树f_K(x). \\
\quad 过程: \\
\qquad (1). 初始化模型f_0(x) = 0； \\
\qquad\; (2). 循环训练K个模型 k=1,2,\cdots,K \\
\qquad\qquad (a). 计算残差：r_{ki} = y^{(i)} - f_{k-1}(x^{(i)}), \quad i=1,2,\cdots,M \\
\qquad\qquad (b). 拟合残差r_{ki}学习一个回归树，得到T(x;\Theta_k) \\
\qquad\qquad (c). 更新f_k(x) = f_{k-1}(x) + T(x; \Theta_k) \\
\qquad\; (3). 得到回归提升树 \\
\qquad\qquad f_K(x) = \sum_{k=1}^{K} T(x; \Theta_k) \\
\}
$</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Gradient-Boosting">Gradient Boosting<a class="anchor-link" href="#Gradient-Boosting">¶</a></h1><p>提升树方法是利用加法模型与前向分布算法实现整个优化学习过程。Adaboost的指数损失和回归提升树的平方损失，在前向分布中的每一步都比较简单。但对于一般损失函数而言（比如绝对损失），每一个优化并不容易。</p>
<p>针对这一问题。Freidman提出了梯度提升（gradient boosting）算法。该算法思想：</p>
<p>利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中残差的近似值，拟合一个回归树。</p>
<p>损失函数的负梯度为：</p>
<p>$$
-\left[ \frac{\partial L(y^{(i)}, f(x^{(i)}))} {\partial f(x^{(i)})} \right]_{f(x) = f_{k-1}(x)} \approx r_{m,i}
$$</p>
<p>Gradient Boosting－算法描述</p>
<p>$
\{ \\
\quad\, 输入：训练数据集D=\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(M)}, y^{(M)})\}, x^{(i)} \in \mathcal{X} \subseteq R^n, y^{(i)} \in \mathcal{Y}; \\
\qquad\quad\; 损失函数L(y, f(x)); \\
\quad 输出：提升树\hat{f}(x). \\
\quad 过程: \\
\qquad (1). 初始化模型 \\
\qquad\qquad\qquad f_0(x) = \arg \min_c \sum_{i=1}^{M} L(y^{(i)}, c)； \\
\qquad\; (2). 循环训练K个模型 k=1,2,\cdots,K \\
\qquad\qquad (a). 计算残差：对于i=1,2,\cdots,M \\
\qquad\qquad\qquad\qquad r_{ki} = -\left[ \frac{\partial L(y^{(i)}, \; f(x^{(i)}))} {\partial f(x^{(i)})} \right]_{f(x) = f_{k-1}(x)} \\
\qquad\qquad (b). 拟合残差r_{ki}学习一个回归树，得到第k颗树的叶结点区域R_{kj}，\quad j=1,2,\cdots,J \\
\qquad\qquad (c). 对j=1,2,\cdots,J, 计算：\\
\qquad\qquad\qquad\qquad c_{kj} = \arg \min_c \sum_{x^{(i)} \in R_{kj}} L(y^{(i)}, \; f_{k-1}(x^{(i)}) + c)\\
\qquad\qquad (d). 更新模型：\\
\qquad\qquad\qquad\qquad f_k(x) = f_{k-1}(x) + \sum_{j=1}^{J} c_{kj} I(x \in R_{kj}) \\
\qquad\; (3). 得到回归提升树 \\
\qquad\qquad\qquad \hat{f}(x) = f_K(x) = \sum_{k=1}^{K} \sum_{j=1}^{J} c_{kj} I(x \in R_{kj}) \\
\}
$</p>
<p>算法解释：</p>
<p>第（1）步初始化，估计使损失函数极小化的常数值（是一个只有根结点的树）；
第(2)(a)步计算损失函数的负梯度在当前模型的值，将它作为残差的估计。(对于平方损失函数，他就是残差；对于一般损失函数，它就是残差的近似值)
第(2)(b)步估计回归树的结点区域，以拟合残差的近似值；
第(2)(c)步利用线性搜索估计叶结点区域的值，使损失函数极小化；
第(2)(d)步更新回归树。</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Boosting利器">Boosting利器<a class="anchor-link" href="#Boosting利器">¶</a></h1><p>Boosting类方法在不仅在二分类、多分类上有着出色的表现，在预估问题上依然出类拔萃。</p>
<p>2012年KDD cup竞赛和Kaggle上的许多数据挖掘竞赛，Boosting类方法帮助参赛者取得好成绩提供了强有力的支持。</p>
<p>“工欲善其事，必先利其器”。Github上和机器学习工具包（如sklearn）中有很多优秀的开源boosting实现。在这里重点介绍两个Boosting开源工具。</p>
<p>XGBoost</p>
<p>说到Boosting开源工具，首推@陈天奇怪同学的XGBoost (eXtreme Gradient Boosting)。上面说的各种竞赛很多优秀的战果都是用@陈天奇同学的神器。</p>
<p>从名称可以看出，该版本侧重于Gradient Boosting方面，提供了Graident Boosting算法的框架，给出了GBDT，GBRT，GBM具体实现。提供了多语言接口（C++, Python, Java, R等），供大家方便使用。</p>
<p>更令人振奋的一件事情是，最新版本的xgboost是基于分布式通信协议rabit开发的，可部署在分布式资源调度系统上（如yarn，s3等）。我们完全可以利用最新版的xgboost在分布式环境下解决分类、预估等场景问题。</p>
<p>注：</p>
<p>XGBoost是DMLC（即分布式机器学习社区）下面的一个子项目，由@陈天奇怪，@李沐等机器学习大神发起。
Rabit是一个为分布式机器学习提供Allreduce和Broadcast编程范式和容错功能的开源库（也是@陈天奇同学的又一神器）。它主要是解决MPI系统机器之间无容错功能的问题，并且主要针对Allreduce和Broadcast接口提供可容错功能。
题外话：</p>
<p>2014年第一次用XGBoost时，用于Kaggle移动CTR预估竞赛。印象比较深刻的是，同样的训练数据（特征工程后），分别用XGBoost中的GBDT和MLLib中的LR模型（LBFGS优化），在验证集上的表现前者比后者好很多（logloss和auc都是如此）。线上提交结果时，名次直接杀进前100名，当时给我留下了非常好的印象。后来，因为项目原因，没有过多的使用xgboost，但一直关注着。</p>
<p>目前，我个人更加关注的是：基于Rabit开发/封装一些在工业界真正能发挥重要价值的（分布式）机器学习工具，用于解决超大规模任务的学习问题。这里面会涉及到分布式环境下的编程范式，可以高效地在分布式环境下工作的优化算法（admm等）和模型(loss + regularization term)等。</p>
<p>关于大数据下的机器学习发展，个人更看好将计算引擎模块与资源调度模块独立开来，专注做各自的事情。计算引擎可以在任意的分布式资源调度系统上工作，实现真正的可插拔，是一个不错的方向。</p>
<p>与之观念对应的是，spark上集成的graphx和mllib中的许多计算模块，虽然使用起来很简便（几十行核心代码就能搭建一个学习任务的pipeline）。但可以想象的是，随着spark的进一步发展，该分布式计算平台会变的非常重，功能也会越来越多。离专注、专一和极致的解决某类问题越来越远，对每一类问题给出的解决方案并不会特别好。</p>
<p>MultiBoost</p>
<p>MultiBoost工具的侧重点不同于XGBoost，是Adaboost算法的多分类版本实现，更偏向于解决multi-class / multi-label / multi-task的分类问题。</p>
<p>我们曾经基于该工具训练了用于用户兴趣画像的多标签（multi-label）分类模型，其分类效果（Precision / Recall作为指标）要比Naive Bayes好。</p>
<p>MultiBoost是用C++实现的。值得一提的是，由我们组的算法大神和男神@BaiGang实现了MulitBoost的spark版本（Scala语言），详见Github: Spark_MultiBoost</p>
</div>
</div>
</div>
</body>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['$','$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        " linebreaks: { automatic: true, width: '95% container' }, " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
</div>


            <footer>
<span class="byline author vcard">
    Posted by
    <span class="fn">
            niult
    </span>
</span><time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time><span class="categories">
        <a href="../../../category/algorithms.html">algorithms</a>
</span>

    <span class="categories">
            <a class="category" href="../../../tag/python.html">python</a>, 
            <a class="category" href="../../../tag/numpy.html">numpy</a>, 
            <a class="category" href="../../../tag/algorithms.html">algorithms</a>
    </span>

<div class="sharing">
</div>            </footer>
        </article>

    </div>
<aside class="sidebar">
    <section>
        <h1>Recent Posts</h1>
        <ul id="recent_posts">
                <li class="post">
                    <a href="../../../pages/2019/01/21-a-first-look-at-a-neural-network.html">2.1-a-first-look-at-a-neural-network</a>
                </li>
                <li class="post">
                    <a href="../../../pages/2019/01/35-classifying-movie-reviews.html">3.5-classifying-movie-reviews</a>
                </li>
                <li class="post">
                    <a href="../../../pages/2019/01/36-classifying-newswires.html">3.6-classifying-newswires</a>
                </li>
                <li class="post">
                    <a href="../../../pages/2019/01/37-predicting-house-prices.html">3.7-predicting-house-prices</a>
                </li>
                <li class="post">
                    <a href="../../../pages/2019/01/44-overfitting-and-underfitting.html">4.4-overfitting-and-underfitting</a>
                </li>
        </ul>
    </section>
        <section>

            <h1>Categories</h1>
            <ul id="recent_posts">
                    <li><a href="../../../category/01chang yong gong ju.html">01常用工具</a></li>
                    <li><a href="../../../category/02.wo ai du shu.html">02.我爱读书</a></li>
                    <li><a href="../../../category/algorithms.html">algorithms</a></li>
                    <li><a href="../../../category/book.html">book</a></li>
                    <li><a href="../../../category/book-pydata.html">book-pydata</a></li>
                    <li><a href="../../../category/deep-learning-with-python.html">deep-learning-with-python</a></li>
                    <li><a href="../../../category/ji qi xue xi shi zhan.html">机器学习实战</a></li>
                    <li><a href="../../../category/ke wai du wu.html">课外读物</a></li>
                    <li><a href="../../../category/ling ji chu ru men shen du xue xi.html">零基础入门深度学习</a></li>
                    <li><a href="../../../category/shen du xue xi.html">深度学习</a></li>
                    <li><a href="../../../category/shen jing wang luo.html">神经网络</a></li>
                    <li><a href="../../../category/shu ju wa jue.html">数据挖掘</a></li>
                    <li><a href="../../../category/shu xue ji chu.html">数学基础</a></li>
                    <li><a href="../../../category/tf-example.html">tf-example</a></li>
                    <li><a href="../../../category/tool1.html">tool1</a></li>
                    <li><a href="../../../category/tool2.html">tool2</a></li>
                    <li><a href="../../../category/tools.html">tools</a></li>
                    <li><a href="../../../category/tui jian xi tong.html">推荐系统</a></li>
                    <li><a href="../../../category/wen ben wa jue.html">文本挖掘</a></li>
            </ul>
        </section>


    <section>
        <h1>Tags</h1>
            <a href="../../../tag/python.html">python</a>, 
            <a href="../../../tag/numpy.html">numpy</a>, 
            <a href="../../../tag/deep-learning.html">deep-learning</a>, 
            <a href="../../../tag/algorithms.html">algorithms</a>, 
            <a href="../../../tag/wen-ben-wa-jue.html">文本挖掘</a>, 
            <a href="../../../tag/shen-jing-wang-luo.html">神经网络</a>, 
            <a href="../../../tag/shu-xue-ji-chu.html">数学基础</a>, 
            <a href="../../../tag/nlp.html">nlp</a>, 
            <a href="../../../tag/tf-example.html">tf-example</a>, 
            <a href="../../../tag/tui-jian-xi-tong.html">推荐系统</a>, 
            <a href="../../../tag/tf.html">tf</a>, 
            <a href="../../../tag/ji-huo-han-shu.html">激活函数</a>, 
            <a href="../../../tag/mapreduce.html">mapreduce</a>, 
            <a href="../../../tag/spark.html">spark</a>, 
            <a href="../../../tag/handbook.html">handbook</a>, 
            <a href="../../../tag/matplotlib.html">matplotlib</a>, 
            <a href="../../../tag/scikit-learn.html">scikit-learn</a>, 
            <a href="../../../tag/latex.html">latex</a>, 
            <a href="../../../tag/pandas.html">pandas</a>, 
            <a href="../../../tag/jupyter.html">jupyter</a>, 
            <a href="../../../tag/plot.html">plot</a>, 
            <a href="../../../tag/pip.html">pip</a>, 
            <a href="../../../tag/geng-xin-suo-you-mo-kuai.html">更新所有模块</a>, 
            <a href="../../../tag/shen-du-xue-xi.html">深度学习</a>, 
            <a href="../../../tag/xun-huan-shen-jing-wang-luo.html">循环神经网络</a>, 
            <a href="../../../tag/pangrank.html">PangRank</a>, 
            <a href="../../../tag/book.html">book</a>, 
            <a href="../../../tag/pydata.html">pydata</a>, 
            <a href="../../../tag/shell.html">shell</a>, 
            <a href="../../../tag/pyhton.html">pyhton</a>
    </section>



    <section>
        <h1>GitHub Repos</h1>
            <a href="https://github.com/1007530194">@1007530194</a> on GitHub
    </section>

</aside>    </div>
</div>
<footer role="contentinfo"><p>
        活到老，学到老，玩到老
    <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>

    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-132396898-1', 'auto');

    ga('require', 'displayfeatures');
    ga('send', 'pageview');
    </script>
</body>
</html>