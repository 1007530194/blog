<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!-->
<html class="no-js" lang="en"><!--<![endif]-->
    <head>
<meta charset="utf-8">
<title>6.1-using-word-embeddings &mdash; 魑魅魍魉</title>

<meta name="author" content="niult">






<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">



<link href="../../../theme/css/main.css" media="screen, projection"
      rel="stylesheet" type="text/css">

<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
      rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
      rel="stylesheet" type="text/css">
</head>

<body>

<script src="../../../theme/js/modernizr-2.0.js"></script>
<script src="../../../theme/js/ender.js"></script>
<script src="../../../theme/js/octopress.js" type="text/javascript"></script>
<script src="../../../theme/js/echarts.min.js" type="text/javascript"></script>
<script src="../../../theme/js/require.min.js" type="text/javascript"></script>

<header role="banner"><hgroup>
  <h1><a href="../../../">魑魅魍魉</a></h1>
</hgroup></header>
<nav role="navigation">    <ul class="subscription" data-subscription="rss">
    </ul>


<ul class="main-navigation">
            <li >
                <a href="../../../category/01chang yong gong ju.html">01常用工具</a>
            </li>
            <li >
                <a href="../../../category/02.wo ai du shu.html">02.我爱读书</a>
            </li>
            <li >
                <a href="../../../category/algorithms.html">Algorithms</a>
            </li>
            <li >
                <a href="../../../category/book.html">Book</a>
            </li>
            <li >
                <a href="../../../category/book-pydata.html">Book-pydata</a>
            </li>
            <li class="active">
                <a href="../../../category/deep-learning-with-python.html">Deep-learning-with-python</a>
            </li>
            <li >
                <a href="../../../category/ji qi xue xi shi zhan.html">机器学习实战</a>
            </li>
            <li >
                <a href="../../../category/ke wai du wu.html">课外读物</a>
            </li>
            <li >
                <a href="../../../category/ling ji chu ru men shen du xue xi.html">零基础入门深度学习</a>
            </li>
            <li >
                <a href="../../../category/shen du xue xi.html">深度学习</a>
            </li>
            <li >
                <a href="../../../category/shen jing wang luo.html">神经网络</a>
            </li>
            <li >
                <a href="../../../category/shu ju wa jue.html">数据挖掘</a>
            </li>
            <li >
                <a href="../../../category/shu xue ji chu.html">数学基础</a>
            </li>
            <li >
                <a href="../../../category/tf-example.html">Tf-example</a>
            </li>
            <li >
                <a href="../../../category/tool1.html">Tool1</a>
            </li>
            <li >
                <a href="../../../category/tool2.html">Tool2</a>
            </li>
            <li >
                <a href="../../../category/tools.html">Tools</a>
            </li>
            <li >
                <a href="../../../category/tui jian xi tong.html">推荐系统</a>
            </li>
            <li >
                <a href="../../../category/wen ben wa jue.html">文本挖掘</a>
            </li>
</ul>
</nav>
<div id="main">
    <div id="content">
    <div>

        <h4>Contents</h4>
        

        <article class="hentry" role="article">
<header>
        <h1 class="entry-title">6.1-using-word-embeddings</h1>
    <p class="meta">
<time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time>    </p>
</header>

    <div class="entry-content"><style type="text/css">/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI colors. */
.ansibold {
  font-weight: bold;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  border-left-width: 1px;
  padding-left: 5px;
  background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%);
}
div.cell.jupyter-soft-selected {
  border-left-color: #90CAF9;
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected {
  border-color: #ababab;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%);
}
@media print {
  div.cell.selected {
    border-color: transparent;
  }
}
div.cell.selected.jupyter-soft-selected {
  border-left-width: 0;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%);
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%);
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  padding: 0.4em;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */
  /* .CodeMirror-lines */
  padding: 0;
  border: 0;
  border-radius: 0;
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area 
div.output_area 
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}



.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}






.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}








.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}


.rendered_html pre,



.rendered_html tr,
.rendered_html th,

.rendered_html td,


.rendered_html * + table {
  margin-top: 1em;
}

.rendered_html * + p {
  margin-top: 1em;
}

.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,

.rendered_html img.unconfined,

div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered 
.text_cell.unrendered .text_cell_render {
  display: none;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
</style>
<style type="text/css">.highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */</style>
<style type="text/css">
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }
</style><body>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [1]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">keras</span>
<span class="n">keras</span><span class="o">.</span><span class="n">__version__</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stderr output_text">
<pre>Using TensorFlow backend.
</pre>
</div>
</div>
<div class="output_area">
<div class="prompt output_prompt">Out[1]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>'2.0.8'</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Using-word-embeddings">Using word embeddings<a class="anchor-link" href="#Using-word-embeddings">¶</a></h1><p>This notebook contains the second code sample found in Chapter 6, Section 1 of <a href="https://www.manning.com/books/deep-learning-with-python?a_aid=keras&amp;a_bid=76564dff">Deep Learning with Python</a>. Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.</p>
<hr/>
<p>Another popular and powerful way to associate a vector with a word is the use of dense "word vectors", also called "word embeddings". 
While the vectors obtained through one-hot encoding are binary, sparse (mostly made of zeros) and very high-dimensional (same dimensionality as the 
number of words in the vocabulary), "word embeddings" are low-dimensional floating point vectors 
(i.e. "dense" vectors, as opposed to sparse vectors). 
Unlike word vectors obtained via one-hot encoding, word embeddings are learned from data. 
It is common to see word embeddings that are 256-dimensional, 512-dimensional, or 1024-dimensional when dealing with very large vocabularies. 
On the other hand, one-hot encoding words generally leads to vectors that are 20,000-dimensional or higher (capturing a vocabulary of 20,000 
token in this case). So, word embeddings pack more information into far fewer dimensions.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img alt="word embeddings vs. one hot encoding" src="https://s3.amazonaws.com/book.keras.io/img/ch6/word_embeddings.png"/></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There are two ways to obtain word embeddings:</p>
<ul>
<li>Learn word embeddings jointly with the main task you care about (e.g. document classification or sentiment prediction). 
In this setup, you would start with random word vectors, then learn your word vectors in the same way that you learn the weights of a neural network.</li>
<li>Load into your model word embeddings that were pre-computed using a different machine learning task than the one you are trying to solve. 
These are called "pre-trained word embeddings". </li>
</ul>
<p>Let's take a look at both.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Learning-word-embeddings-with-the-Embedding-layer">Learning word embeddings with the <code>Embedding</code> layer<a class="anchor-link" href="#Learning-word-embeddings-with-the-Embedding-layer">¶</a></h2><p>The simplest way to associate a dense vector to a word would be to pick the vector at random. The problem with this approach is that the 
resulting embedding space would have no structure: for instance, the words "accurate" and "exact" may end up with completely different 
embeddings, even though they are interchangeable in most sentences. It would be very difficult for a deep neural network to make sense of 
such a noisy, unstructured embedding space.</p>
<p>To get a bit more abstract: the geometric relationships between word vectors should reflect the semantic relationships between these words. 
Word embeddings are meant to map human language into a geometric space. For instance, in a reasonable embedding space, we would expect 
synonyms to be embedded into similar word vectors, and in general we would expect the geometric distance (e.g. L2 distance) between any two 
word vectors to relate to the semantic distance of the associated words (words meaning very different things would be embedded to points 
far away from each other, while related words would be closer). Even beyond mere distance, we may want specific <strong>directions</strong> in the 
embedding space to be meaningful.</p>
<p>[...]</p>
<p>In real-world word embedding spaces, common examples of meaningful geometric transformations are "gender vectors" and "plural vector". For 
instance, by adding a "female vector" to the vector "king", one obtain the vector "queen". By adding a "plural vector", one obtain "kings". 
Word embedding spaces typically feature thousands of such interpretable and potentially useful vectors.</p>
<p>Is there some "ideal" word embedding space that would perfectly map human language and could be used for any natural language processing 
task? Possibly, but in any case, we have yet to compute anything of the sort. Also, there isn't such a thing as "human language", there are 
many different languages and they are not isomorphic, as a language is the reflection of a specific culture and a specific context. But more 
pragmatically, what makes a good word embedding space depends heavily on your task: the perfect word embedding space for an 
English-language movie review sentiment analysis model may look very different from the perfect embedding space for an English-language 
legal document classification model, because the importance of certain semantic relationships varies from task to task.</p>
<p>It is thus reasonable to <strong>learn</strong> a new embedding space with every new task. Thankfully, backpropagation makes this really easy, and Keras makes it 
even easier. It's just about learning the weights of a layer: the <code>Embedding</code> layer.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [3]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="k">import</span> <span class="n">Embedding</span>

<span class="c1"># The Embedding layer takes at least two arguments:</span>
<span class="c1"># the number of possible tokens, here 1000 (1 + maximum word index),</span>
<span class="c1"># and the dimensionality of the embeddings, here 64.</span>
<span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <code>Embedding</code> layer is best understood as a dictionary mapping integer indices (which stand for specific words) to dense vectors. It takes 
as input integers, it looks up these integers into an internal dictionary, and it returns the associated vectors. It's effectively a dictionary lookup.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <code>Embedding</code> layer takes as input a 2D tensor of integers, of shape <code>(samples, sequence_length)</code>, where each entry is a sequence of 
integers. It can embed sequences of variable lengths, so for instance we could feed into our embedding layer above batches that could have 
shapes <code>(32, 10)</code> (batch of 32 sequences of length 10) or <code>(64, 15)</code> (batch of 64 sequences of length 15). All sequences in a batch must 
have the same length, though (since we need to pack them into a single tensor), so sequences that are shorter than others should be padded 
with zeros, and sequences that are longer should be truncated.</p>
<p>This layer returns a 3D floating point tensor, of shape <code>(samples, sequence_length, embedding_dimensionality)</code>. Such a 3D tensor can then 
be processed by a RNN layer or a 1D convolution layer (both will be introduced in the next sections).</p>
<p>When you instantiate an <code>Embedding</code> layer, its weights (its internal dictionary of token vectors) are initially random, just like with any 
other layer. During training, these word vectors will be gradually adjusted via backpropagation, structuring the space into something that the 
downstream model can exploit. Once fully trained, your embedding space will show a lot of structure -- a kind of structure specialized for 
the specific problem you were training your model for.</p>
<p>Let's apply this idea to the IMDB movie review sentiment prediction task that you are already familiar with. Let's quickly prepare 
the data. We will restrict the movie reviews to the top 10,000 most common words (like we did the first time we worked with this dataset), 
and cut the reviews after only 20 words. Our network will simply learn 8-dimensional embeddings for each of the 10,000 words, turn the 
input integer sequences (2D integer tensor) into embedded sequences (3D float tensor), flatten the tensor to 2D, and train a single <code>Dense</code> 
layer on top for classification.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [4]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">keras.datasets</span> <span class="k">import</span> <span class="n">imdb</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="k">import</span> <span class="n">preprocessing</span>

<span class="c1"># Number of words to consider as features</span>
<span class="n">max_features</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="c1"># Cut texts after this number of words </span>
<span class="c1"># (among top max_features most common words)</span>
<span class="n">maxlen</span> <span class="o">=</span> <span class="mi">20</span>

<span class="c1"># Load the data as lists of integers.</span>
<span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">imdb</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="n">max_features</span><span class="p">)</span>

<span class="c1"># This turns our lists of integers</span>
<span class="c1"># into a 2D integer tensor of shape `(samples, maxlen)`</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">sequence</span><span class="o">.</span><span class="n">pad_sequences</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">maxlen</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">sequence</span><span class="o">.</span><span class="n">pad_sequences</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">maxlen</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz
12460032/17464789 [====================&gt;.........] - ETA: 0s</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [5]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">keras.models</span> <span class="k">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="k">import</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">Dense</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="c1"># We specify the maximum input length to our Embedding layer</span>
<span class="c1"># so we can later flatten the embedded inputs</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="n">maxlen</span><span class="p">))</span>
<span class="c1"># After the Embedding layer, </span>
<span class="c1"># our activations have shape `(samples, maxlen, 8)`.</span>

<span class="c1"># We flatten the 3D tensor of embeddings </span>
<span class="c1"># into a 2D tensor of shape `(samples, maxlen * 8)`</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>

<span class="c1"># We add the classifier on top</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">'rmsprop'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">'binary_crossentropy'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'acc'</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                    <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_2 (Embedding)      (None, 20, 8)             80000     
_________________________________________________________________
flatten_1 (Flatten)          (None, 160)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 161       
=================================================================
Total params: 80,161
Trainable params: 80,161
Non-trainable params: 0
_________________________________________________________________
Train on 20000 samples, validate on 5000 samples
Epoch 1/10
20000/20000 [==============================] - 2s - loss: 0.6560 - acc: 0.6482 - val_loss: 0.5906 - val_acc: 0.7146
Epoch 2/10
20000/20000 [==============================] - 2s - loss: 0.5189 - acc: 0.7595 - val_loss: 0.5117 - val_acc: 0.7364
Epoch 3/10
20000/20000 [==============================] - 2s - loss: 0.4512 - acc: 0.7933 - val_loss: 0.4949 - val_acc: 0.7470
Epoch 4/10
20000/20000 [==============================] - 2s - loss: 0.4190 - acc: 0.8069 - val_loss: 0.4905 - val_acc: 0.7538
Epoch 5/10
20000/20000 [==============================] - 2s - loss: 0.3965 - acc: 0.8198 - val_loss: 0.4914 - val_acc: 0.7572
Epoch 6/10
20000/20000 [==============================] - 2s - loss: 0.3784 - acc: 0.8311 - val_loss: 0.4953 - val_acc: 0.7594
Epoch 7/10
20000/20000 [==============================] - 2s - loss: 0.3624 - acc: 0.8419 - val_loss: 0.5004 - val_acc: 0.7574
Epoch 8/10
20000/20000 [==============================] - 2s - loss: 0.3474 - acc: 0.8484 - val_loss: 0.5058 - val_acc: 0.7572
Epoch 9/10
20000/20000 [==============================] - 2s - loss: 0.3330 - acc: 0.8582 - val_loss: 0.5122 - val_acc: 0.7528
Epoch 10/10
20000/20000 [==============================] - 2s - loss: 0.3194 - acc: 0.8669 - val_loss: 0.5183 - val_acc: 0.7554
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We get to a validation accuracy of ~76%, which is pretty good considering that we only look at the first 20 words in every review. But 
note that merely flattening the embedded sequences and training a single <code>Dense</code> layer on top leads to a model that treats each word in the 
input sequence separately, without considering inter-word relationships and structure sentence (e.g. it would likely treat both <em>"this movie 
is shit"</em> and <em>"this movie is the shit"</em> as being negative "reviews"). It would be much better to add recurrent layers or 1D convolutional 
layers on top of the embedded sequences to learn features that take into account each sequence as a whole. That's what we will focus on in 
the next few sections.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Using-pre-trained-word-embeddings">Using pre-trained word embeddings<a class="anchor-link" href="#Using-pre-trained-word-embeddings">¶</a></h2><p>Sometimes, you have so little training data available that could never use your data alone to learn an appropriate task-specific embedding 
of your vocabulary. What to do then?</p>
<p>Instead of learning word embeddings jointly with the problem you want to solve, you could be loading embedding vectors from a pre-computed 
embedding space known to be highly structured and to exhibit useful properties -- that captures generic aspects of language structure. The 
rationale behind using pre-trained word embeddings in natural language processing is very much the same as for using pre-trained convnets 
in image classification: we don't have enough data available to learn truly powerful features on our own, but we expect the features that 
we need to be fairly generic, i.e. common visual features or semantic features. In this case it makes sense to reuse features learned on a 
different problem.</p>
<p>Such word embeddings are generally computed using word occurrence statistics (observations about what words co-occur in sentences or 
documents), using a variety of techniques, some involving neural networks, others not. The idea of a dense, low-dimensional embedding space 
for words, computed in an unsupervised way, was initially explored by Bengio et al. in the early 2000s, but it only started really taking 
off in research and industry applications after the release of one of the most famous and successful word embedding scheme: the Word2Vec 
algorithm, developed by Mikolov at Google in 2013. Word2Vec dimensions capture specific semantic properties, e.g. gender.</p>
<p>There are various pre-computed databases of word embeddings that can download and start using in a Keras <code>Embedding</code> layer. Word2Vec is one 
of them. Another popular one is called "GloVe", developed by Stanford researchers in 2014. It stands for "Global Vectors for Word 
Representation", and it is an embedding technique based on factorizing a matrix of word co-occurrence statistics. Its developers have made 
available pre-computed embeddings for millions of English tokens, obtained from Wikipedia data or from Common Crawl data.</p>
<p>Let's take a look at how you can get started using GloVe embeddings in a Keras model. The same method will of course be valid for Word2Vec 
embeddings or any other word embedding database that you can download. We will also use this example to refresh the text tokenization 
techniques we introduced a few paragraphs ago: we will start from raw text, and work our way up.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Putting-it-all-together:-from-raw-text-to-word-embeddings">Putting it all together: from raw text to word embeddings<a class="anchor-link" href="#Putting-it-all-together:-from-raw-text-to-word-embeddings">¶</a></h2><p>We will be using a model similar to the one we just went over -- embedding sentences in sequences of vectors, flattening them and training a 
<code>Dense</code> layer on top. But we will do it using pre-trained word embeddings, and instead of using the pre-tokenized IMDB data packaged in 
Keras, we will start from scratch, by downloading the original text data.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Download-the-IMDB-data-as-raw-text">Download the IMDB data as raw text<a class="anchor-link" href="#Download-the-IMDB-data-as-raw-text">¶</a></h3><p>First, head to <code>http://ai.stanford.edu/~amaas/data/sentiment/</code> and download the raw IMDB dataset (if the URL isn't working anymore, just 
Google "IMDB dataset"). Uncompress it.</p>
<p>Now let's collect the individual training reviews into a list of strings, one string per review, and let's also collect the review labels 
(positive / negative) into a <code>labels</code> list:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [6]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="n">imdb_dir</span> <span class="o">=</span> <span class="s1">'/home/ubuntu/data/aclImdb'</span>
<span class="n">train_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">imdb_dir</span><span class="p">,</span> <span class="s1">'train'</span><span class="p">)</span>

<span class="n">labels</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">texts</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">label_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">'neg'</span><span class="p">,</span> <span class="s1">'pos'</span><span class="p">]:</span>
    <span class="n">dir_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">train_dir</span><span class="p">,</span> <span class="n">label_type</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">fname</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">dir_name</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">fname</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">:]</span> <span class="o">==</span> <span class="s1">'.txt'</span><span class="p">:</span>
            <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dir_name</span><span class="p">,</span> <span class="n">fname</span><span class="p">))</span>
            <span class="n">texts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
            <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">label_type</span> <span class="o">==</span> <span class="s1">'neg'</span><span class="p">:</span>
                <span class="n">labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Tokenize-the-data">Tokenize the data<a class="anchor-link" href="#Tokenize-the-data">¶</a></h3><p>Let's vectorize the texts we collected, and prepare a training and validation split.
We will merely be using the concepts we introduced earlier in this section.</p>
<p>Because pre-trained word embeddings are meant to be particularly useful on problems where little training data is available (otherwise, 
task-specific embeddings are likely to outperform them), we will add the following twist: we restrict the training data to its first 200 
samples. So we will be learning to classify movie reviews after looking at just 200 examples...</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [7]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">keras.preprocessing.text</span> <span class="k">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.sequence</span> <span class="k">import</span> <span class="n">pad_sequences</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">maxlen</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># We will cut reviews after 100 words</span>
<span class="n">training_samples</span> <span class="o">=</span> <span class="mi">200</span>  <span class="c1"># We will be training on 200 samples</span>
<span class="n">validation_samples</span> <span class="o">=</span> <span class="mi">10000</span>  <span class="c1"># We will be validating on 10000 samples</span>
<span class="n">max_words</span> <span class="o">=</span> <span class="mi">10000</span>  <span class="c1"># We will only consider the top 10,000 words in the dataset</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="n">max_words</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>
<span class="n">sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>

<span class="n">word_index</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Found </span><span class="si">%s</span><span class="s1"> unique tokens.'</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_index</span><span class="p">))</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">maxlen</span><span class="p">)</span>

<span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Shape of data tensor:'</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Shape of label tensor:'</span><span class="p">,</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Split the data into a training set and a validation set</span>
<span class="c1"># But first, shuffle the data, since we started from data</span>
<span class="c1"># where sample are ordered (all negative first, then all positive).</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>

<span class="n">x_train</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:</span><span class="n">training_samples</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[:</span><span class="n">training_samples</span><span class="p">]</span>
<span class="n">x_val</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">training_samples</span><span class="p">:</span> <span class="n">training_samples</span> <span class="o">+</span> <span class="n">validation_samples</span><span class="p">]</span>
<span class="n">y_val</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">training_samples</span><span class="p">:</span> <span class="n">training_samples</span> <span class="o">+</span> <span class="n">validation_samples</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Found 88582 unique tokens.
Shape of data tensor: (25000, 100)
Shape of label tensor: (25000,)
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Download-the-GloVe-word-embeddings">Download the GloVe word embeddings<a class="anchor-link" href="#Download-the-GloVe-word-embeddings">¶</a></h3><p>Head to <code>https://nlp.stanford.edu/projects/glove/</code> (where you can learn more about the GloVe algorithm), and download the pre-computed 
embeddings from 2014 English Wikipedia. It's a 822MB zip file named <code>glove.6B.zip</code>, containing 100-dimensional embedding vectors for 
400,000 words (or non-word tokens). Un-zip it.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Pre-process-the-embeddings">Pre-process the embeddings<a class="anchor-link" href="#Pre-process-the-embeddings">¶</a></h3><p>Let's parse the un-zipped file (it's a <code>txt</code> file) to build an index mapping words (as strings) to their vector representation (as number 
vectors).</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [9]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">glove_dir</span> <span class="o">=</span> <span class="s1">'/home/ubuntu/data/'</span>

<span class="n">embeddings_index</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">glove_dir</span><span class="p">,</span> <span class="s1">'glove.6B.100d.txt'</span><span class="p">))</span>
<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="n">word</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">coefs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'float32'</span><span class="p">)</span>
    <span class="n">embeddings_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">coefs</span>
<span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Found </span><span class="si">%s</span><span class="s1"> word vectors.'</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings_index</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Found 400000 word vectors.
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now let's build an embedding matrix that we will be able to load into an <code>Embedding</code> layer. It must be a matrix of shape <code>(max_words, 
embedding_dim)</code>, where each entry <code>i</code> contains the <code>embedding_dim</code>-dimensional vector for the word of index <code>i</code> in our reference word index 
(built during tokenization). Note that the index <code>0</code> is not supposed to stand for any word or token -- it's a placeholder.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [10]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">max_words</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">))</span>
<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">word_index</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">embedding_vector</span> <span class="o">=</span> <span class="n">embeddings_index</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">max_words</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">embedding_vector</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Words not found in embedding index will be all-zeros.</span>
            <span class="n">embedding_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding_vector</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Define-a-model">Define a model<a class="anchor-link" href="#Define-a-model">¶</a></h3><p>We will be using the same model architecture as before:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [15]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">keras.models</span> <span class="k">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="k">import</span> <span class="n">Embedding</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">Dense</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_words</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="n">maxlen</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_4 (Embedding)      (None, 100, 100)          1000000   
_________________________________________________________________
flatten_3 (Flatten)          (None, 10000)             0         
_________________________________________________________________
dense_4 (Dense)              (None, 32)                320032    
_________________________________________________________________
dense_5 (Dense)              (None, 1)                 33        
=================================================================
Total params: 1,320,065
Trainable params: 1,320,065
Non-trainable params: 0
_________________________________________________________________
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Load-the-GloVe-embeddings-in-the-model">Load the GloVe embeddings in the model<a class="anchor-link" href="#Load-the-GloVe-embeddings-in-the-model">¶</a></h3><p>The <code>Embedding</code> layer has a single weight matrix: a 2D float matrix where each entry <code>i</code> is the word vector meant to be associated with 
index <code>i</code>. Simple enough. Let's just load the GloVe matrix we prepared into our <code>Embedding</code> layer, the first layer in our model:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [16]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_weights</span><span class="p">([</span><span class="n">embedding_matrix</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Additionally, we freeze the embedding layer (we set its <code>trainable</code> attribute to <code>False</code>), following the same rationale as what you are 
already familiar with in the context of pre-trained convnet features: when parts of a model are pre-trained (like our <code>Embedding</code> layer), 
and parts are randomly initialized (like our classifier), the pre-trained parts should not be updated during training to avoid forgetting 
what they already know. The large gradient update triggered by the randomly initialized layers would be very disruptive to the already 
learned features.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Train-and-evaluate">Train and evaluate<a class="anchor-link" href="#Train-and-evaluate">¶</a></h3><p>Let's compile our model and train it:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [17]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">'rmsprop'</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s1">'binary_crossentropy'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'acc'</span><span class="p">])</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">save_weights</span><span class="p">(</span><span class="s1">'pre_trained_glove_model.h5'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Train on 200 samples, validate on 10000 samples
Epoch 1/10
200/200 [==============================] - 1s - loss: 1.9075 - acc: 0.5050 - val_loss: 0.7027 - val_acc: 0.5102
Epoch 2/10
200/200 [==============================] - 0s - loss: 0.7329 - acc: 0.7100 - val_loss: 0.8200 - val_acc: 0.5000
Epoch 3/10
200/200 [==============================] - 0s - loss: 0.4876 - acc: 0.7400 - val_loss: 0.6917 - val_acc: 0.5616
Epoch 4/10
200/200 [==============================] - 0s - loss: 0.3640 - acc: 0.8400 - val_loss: 0.7005 - val_acc: 0.5557
Epoch 5/10
200/200 [==============================] - 0s - loss: 0.2673 - acc: 0.8950 - val_loss: 1.2560 - val_acc: 0.4999
Epoch 6/10
200/200 [==============================] - 0s - loss: 0.1936 - acc: 0.9400 - val_loss: 0.7294 - val_acc: 0.5704
Epoch 7/10
200/200 [==============================] - 0s - loss: 0.2455 - acc: 0.8800 - val_loss: 0.7187 - val_acc: 0.5659
Epoch 8/10
200/200 [==============================] - 0s - loss: 0.0591 - acc: 0.9950 - val_loss: 0.7393 - val_acc: 0.5723
Epoch 9/10
200/200 [==============================] - 0s - loss: 0.0399 - acc: 1.0000 - val_loss: 0.8691 - val_acc: 0.5522
Epoch 10/10
200/200 [==============================] - 0s - loss: 0.0283 - acc: 1.0000 - val_loss: 0.9322 - val_acc: 0.5413
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's plot its performance over time:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [18]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">'acc'</span><span class="p">]</span>
<span class="n">val_acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">'val_acc'</span><span class="p">]</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">'loss'</span><span class="p">]</span>
<span class="n">val_loss</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">'val_loss'</span><span class="p">]</span>

<span class="n">epochs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span> <span class="s1">'bo'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Training acc'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">,</span> <span class="s1">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Validation acc'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Training and validation accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">'bo'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Training loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="s1">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Validation loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Training and validation loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_png output_subarea">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"/>
</div>
</div>
<div class="output_area">
<div class="prompt"></div>
<div class="output_png output_subarea">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAX4AAAEICAYAAABYoZ8gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"/>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The model quickly starts overfitting, unsurprisingly given the small number of training samples. Validation accuracy has high variance for 
the same reason, but seems to reach high 50s.</p>
<p>Note that your mileage may vary: since we have so few training samples, performance is heavily dependent on which exact 200 samples we 
picked, and we picked them at random. If it worked really poorly for you, try picking a different random set of 200 samples, just for the 
sake of the exercise (in real life you don't get to pick your training data).</p>
<p>We can also try to train the same model without loading the pre-trained word embeddings and without freezing the embedding layer. In that 
case, we would be learning a task-specific embedding of our input tokens, which is generally more powerful than pre-trained word embeddings 
when lots of data is available. However, in our case, we have only 200 training samples. Let's try it:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [20]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">keras.models</span> <span class="k">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="k">import</span> <span class="n">Embedding</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">Dense</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_words</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="n">maxlen</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">'rmsprop'</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s1">'binary_crossentropy'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'acc'</span><span class="p">])</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_5 (Embedding)      (None, 100, 100)          1000000   
_________________________________________________________________
flatten_4 (Flatten)          (None, 10000)             0         
_________________________________________________________________
dense_6 (Dense)              (None, 32)                320032    
_________________________________________________________________
dense_7 (Dense)              (None, 1)                 33        
=================================================================
Total params: 1,320,065
Trainable params: 1,320,065
Non-trainable params: 0
_________________________________________________________________
Train on 200 samples, validate on 10000 samples
Epoch 1/10
200/200 [==============================] - 1s - loss: 0.6941 - acc: 0.4750 - val_loss: 0.6920 - val_acc: 0.5213
Epoch 2/10
200/200 [==============================] - 0s - loss: 0.5050 - acc: 0.9900 - val_loss: 0.6949 - val_acc: 0.5138
Epoch 3/10
200/200 [==============================] - 0s - loss: 0.2807 - acc: 1.0000 - val_loss: 0.7131 - val_acc: 0.5125
Epoch 4/10
200/200 [==============================] - 0s - loss: 0.1223 - acc: 1.0000 - val_loss: 0.6997 - val_acc: 0.5214
Epoch 5/10
200/200 [==============================] - 0s - loss: 0.0584 - acc: 1.0000 - val_loss: 0.7043 - val_acc: 0.5183
Epoch 6/10
200/200 [==============================] - 0s - loss: 0.0307 - acc: 1.0000 - val_loss: 0.7051 - val_acc: 0.5248
Epoch 7/10
200/200 [==============================] - 0s - loss: 0.0166 - acc: 1.0000 - val_loss: 0.7345 - val_acc: 0.5282
Epoch 8/10
200/200 [==============================] - 0s - loss: 0.0098 - acc: 1.0000 - val_loss: 0.7173 - val_acc: 0.5199
Epoch 9/10
200/200 [==============================] - 0s - loss: 0.0058 - acc: 1.0000 - val_loss: 0.7201 - val_acc: 0.5253
Epoch 10/10
200/200 [==============================] - 0s - loss: 0.0035 - acc: 1.0000 - val_loss: 0.7244 - val_acc: 0.5264
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [22]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">'acc'</span><span class="p">]</span>
<span class="n">val_acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">'val_acc'</span><span class="p">]</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">'loss'</span><span class="p">]</span>
<span class="n">val_loss</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">'val_loss'</span><span class="p">]</span>

<span class="n">epochs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span> <span class="s1">'bo'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Training acc'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">,</span> <span class="s1">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Validation acc'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Training and validation accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">'bo'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Training loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="s1">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Validation loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Training and validation loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_png output_subarea">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"/>
</div>
</div>
<div class="output_area">
<div class="prompt"></div>
<div class="output_png output_subarea">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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==
"/>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Validation accuracy stalls in the low 50s. So in our case, pre-trained word embeddings does outperform jointly learned embeddings. If you 
increase the number of training samples, this will quickly stop being the case -- try it as an exercise.</p>
<p>Finally, let's evaluate the model on the test data. First, we will need to tokenize the test data:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [24]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">test_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">imdb_dir</span><span class="p">,</span> <span class="s1">'test'</span><span class="p">)</span>

<span class="n">labels</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">texts</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">label_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">'neg'</span><span class="p">,</span> <span class="s1">'pos'</span><span class="p">]:</span>
    <span class="n">dir_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">test_dir</span><span class="p">,</span> <span class="n">label_type</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">fname</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">dir_name</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">fname</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">:]</span> <span class="o">==</span> <span class="s1">'.txt'</span><span class="p">:</span>
            <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dir_name</span><span class="p">,</span> <span class="n">fname</span><span class="p">))</span>
            <span class="n">texts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
            <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">label_type</span> <span class="o">==</span> <span class="s1">'neg'</span><span class="p">:</span>
                <span class="n">labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">maxlen</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And let's load and evaluate the first model:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [25]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s1">'pre_trained_glove_model.h5'</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>24736/25000 [============================&gt;.] - ETA: 0s</pre>
</div>
</div>
<div class="output_area">
<div class="prompt output_prompt">Out[25]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>[0.93747248332977295, 0.53659999999999997]</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We get an appalling test accuracy of 54%. Working with just a handful of training samples is hard!</p>
</div>
</div>
</div>
</body>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['$','$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        " linebreaks: { automatic: true, width: '95% container' }, " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
</div>


            <footer>
<span class="byline author vcard">
    Posted by
    <span class="fn">
            niult
    </span>
</span><time datetime="2019-01-16T00:00:00+08:00" pubdate>2019-01-16 00:00</time><span class="categories">
        <a href="../../../category/deep-learning-with-python.html">deep-learning-with-python</a>
</span>

    <span class="categories">
            <a class="category" href="../../../tag/python.html">python</a>, 
            <a class="category" href="../../../tag/numpy.html">numpy</a>, 
            <a class="category" href="../../../tag/deep-learning.html">deep-learning</a>
    </span>

<div class="sharing">
</div>            </footer>
        </article>

    </div>
<aside class="sidebar">
    <section>
        <h1>Recent Posts</h1>
        <ul id="recent_posts">
                <li class="post">
                    <a href="../../../pages/2019/01/21-a-first-look-at-a-neural-network.html">2.1-a-first-look-at-a-neural-network</a>
                </li>
                <li class="post">
                    <a href="../../../pages/2019/01/35-classifying-movie-reviews.html">3.5-classifying-movie-reviews</a>
                </li>
                <li class="post">
                    <a href="../../../pages/2019/01/36-classifying-newswires.html">3.6-classifying-newswires</a>
                </li>
                <li class="post">
                    <a href="../../../pages/2019/01/37-predicting-house-prices.html">3.7-predicting-house-prices</a>
                </li>
                <li class="post">
                    <a href="../../../pages/2019/01/44-overfitting-and-underfitting.html">4.4-overfitting-and-underfitting</a>
                </li>
        </ul>
    </section>
        <section>

            <h1>Categories</h1>
            <ul id="recent_posts">
                    <li><a href="../../../category/01chang yong gong ju.html">01常用工具</a></li>
                    <li><a href="../../../category/02.wo ai du shu.html">02.我爱读书</a></li>
                    <li><a href="../../../category/algorithms.html">algorithms</a></li>
                    <li><a href="../../../category/book.html">book</a></li>
                    <li><a href="../../../category/book-pydata.html">book-pydata</a></li>
                    <li><a href="../../../category/deep-learning-with-python.html">deep-learning-with-python</a></li>
                    <li><a href="../../../category/ji qi xue xi shi zhan.html">机器学习实战</a></li>
                    <li><a href="../../../category/ke wai du wu.html">课外读物</a></li>
                    <li><a href="../../../category/ling ji chu ru men shen du xue xi.html">零基础入门深度学习</a></li>
                    <li><a href="../../../category/shen du xue xi.html">深度学习</a></li>
                    <li><a href="../../../category/shen jing wang luo.html">神经网络</a></li>
                    <li><a href="../../../category/shu ju wa jue.html">数据挖掘</a></li>
                    <li><a href="../../../category/shu xue ji chu.html">数学基础</a></li>
                    <li><a href="../../../category/tf-example.html">tf-example</a></li>
                    <li><a href="../../../category/tool1.html">tool1</a></li>
                    <li><a href="../../../category/tool2.html">tool2</a></li>
                    <li><a href="../../../category/tools.html">tools</a></li>
                    <li><a href="../../../category/tui jian xi tong.html">推荐系统</a></li>
                    <li><a href="../../../category/wen ben wa jue.html">文本挖掘</a></li>
            </ul>
        </section>


    <section>
        <h1>Tags</h1>
            <a href="../../../tag/python.html">python</a>, 
            <a href="../../../tag/numpy.html">numpy</a>, 
            <a href="../../../tag/deep-learning.html">deep-learning</a>, 
            <a href="../../../tag/algorithms.html">algorithms</a>, 
            <a href="../../../tag/wen-ben-wa-jue.html">文本挖掘</a>, 
            <a href="../../../tag/shen-jing-wang-luo.html">神经网络</a>, 
            <a href="../../../tag/shu-xue-ji-chu.html">数学基础</a>, 
            <a href="../../../tag/nlp.html">nlp</a>, 
            <a href="../../../tag/tf-example.html">tf-example</a>, 
            <a href="../../../tag/tui-jian-xi-tong.html">推荐系统</a>, 
            <a href="../../../tag/tf.html">tf</a>, 
            <a href="../../../tag/ji-huo-han-shu.html">激活函数</a>, 
            <a href="../../../tag/mapreduce.html">mapreduce</a>, 
            <a href="../../../tag/spark.html">spark</a>, 
            <a href="../../../tag/handbook.html">handbook</a>, 
            <a href="../../../tag/matplotlib.html">matplotlib</a>, 
            <a href="../../../tag/scikit-learn.html">scikit-learn</a>, 
            <a href="../../../tag/latex.html">latex</a>, 
            <a href="../../../tag/pandas.html">pandas</a>, 
            <a href="../../../tag/jupyter.html">jupyter</a>, 
            <a href="../../../tag/plot.html">plot</a>, 
            <a href="../../../tag/pip.html">pip</a>, 
            <a href="../../../tag/geng-xin-suo-you-mo-kuai.html">更新所有模块</a>, 
            <a href="../../../tag/shen-du-xue-xi.html">深度学习</a>, 
            <a href="../../../tag/xun-huan-shen-jing-wang-luo.html">循环神经网络</a>, 
            <a href="../../../tag/pangrank.html">PangRank</a>, 
            <a href="../../../tag/book.html">book</a>, 
            <a href="../../../tag/pydata.html">pydata</a>, 
            <a href="../../../tag/shell.html">shell</a>, 
            <a href="../../../tag/pyhton.html">pyhton</a>
    </section>



    <section>
        <h1>GitHub Repos</h1>
            <a href="https://github.com/1007530194">@1007530194</a> on GitHub
    </section>

</aside>    </div>
</div>
<footer role="contentinfo"><p>
        活到老，学到老，玩到老
    <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>

    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-132396898-1', 'auto');

    ga('require', 'displayfeatures');
    ga('send', 'pageview');
    </script>
</body>
</html>