<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!-->
<html class="no-js" lang="en"><!--<![endif]-->
    <head>
<meta charset="utf-8">
<title>10.k-means聚类 &mdash; 魑魅魍魉</title>

<meta name="author" content="niult">






<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">



<link href="../../../theme/css/main.css" media="screen, projection"
      rel="stylesheet" type="text/css">

<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
      rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
      rel="stylesheet" type="text/css">
</head>

<body>

<script src="../../../theme/js/modernizr-2.0.js"></script>
<script src="../../../theme/js/ender.js"></script>
<script src="../../../theme/js/octopress.js" type="text/javascript"></script>
<script src="../../../theme/js/echarts.min.js" type="text/javascript"></script>
<script src="../../../theme/js/require.min.js" type="text/javascript"></script>

<header role="banner"><hgroup>
  <h1><a href="../../../">魑魅魍魉</a></h1>
</hgroup></header>
<nav role="navigation">    <ul class="subscription" data-subscription="rss">
    </ul>


<ul class="main-navigation">
            <li >
                <a href="../../../category/01chang yong gong ju.html">01常用工具</a>
            </li>
            <li >
                <a href="../../../category/02.wo ai du shu.html">02.我爱读书</a>
            </li>
            <li >
                <a href="../../../category/algorithms.html">Algorithms</a>
            </li>
            <li >
                <a href="../../../category/book.html">Book</a>
            </li>
            <li >
                <a href="../../../category/book-pydata.html">Book-pydata</a>
            </li>
            <li >
                <a href="../../../category/deep-learning-with-python.html">Deep-learning-with-python</a>
            </li>
            <li class="active">
                <a href="../../../category/ji qi xue xi shi zhan.html">机器学习实战</a>
            </li>
            <li >
                <a href="../../../category/ke wai du wu.html">课外读物</a>
            </li>
            <li >
                <a href="../../../category/ling ji chu ru men shen du xue xi.html">零基础入门深度学习</a>
            </li>
            <li >
                <a href="../../../category/shen du xue xi.html">深度学习</a>
            </li>
            <li >
                <a href="../../../category/shen jing wang luo.html">神经网络</a>
            </li>
            <li >
                <a href="../../../category/shu ju wa jue.html">数据挖掘</a>
            </li>
            <li >
                <a href="../../../category/shu xue ji chu.html">数学基础</a>
            </li>
            <li >
                <a href="../../../category/tf-example.html">Tf-example</a>
            </li>
            <li >
                <a href="../../../category/tool1.html">Tool1</a>
            </li>
            <li >
                <a href="../../../category/tool2.html">Tool2</a>
            </li>
            <li >
                <a href="../../../category/tools.html">Tools</a>
            </li>
            <li >
                <a href="../../../category/tui jian xi tong.html">推荐系统</a>
            </li>
            <li >
                <a href="../../../category/wen ben wa jue.html">文本挖掘</a>
            </li>
</ul>
</nav>
<div id="main">
    <div id="content">
    <div>

        <h4>Contents</h4>
        

        <article class="hentry" role="article">
<header>
        <h1 class="entry-title">10.k-means聚类</h1>
    <p class="meta">
<time datetime="2017-01-01T00:00:00+08:00" pubdate>2017-01-01 00:00</time>    </p>
</header>

    <div class="entry-content"><h1>第 10 章 K-Means（K-均值）聚类算法</h1>
<h2>K-Means 算法</h2>
<p>聚类是一种无监督的学习, 它将相似的对象归到一个簇中, 将不相似对象归到不同簇中.<br/>
相似这一概念取决于所选择的相似度计算方法.<br/>
K-Means 是发现给定数据集的 K 个簇的聚类算法, 之所以称之为 <code>K-均值</code> 是因为它可以发现 K 个不同的簇, 且每个簇的中心采用簇中所含值的均值计算而成.<br/>
簇个数 K 是用户指定的, 每一个簇通过其质心（centroid）, 即簇中所有点的中心来描述.<br/>
聚类与分类算法的最大区别在于, 分类的目标类别已知, 而聚类的目标类别是未知的.  </p>
<div class="highlight"><pre><span></span><span class="err">优点</span><span class="o">:</span> <span class="err">容易实现</span>
<span class="err">缺点</span><span class="o">:</span><span class="err">可能收敛到局部最小值</span><span class="o">,</span> <span class="err">在大规模数据集上收敛较慢</span>
<span class="err">使用数据类型</span> <span class="o">:</span> <span class="err">数值型数据</span>
</pre></div>
<h3>K-Means 场景</h3>
<p>主要用来聚类, 但是类别是未知的.<br/>
例如: 对地图上的点进行聚类.</p>
<h3>K-Means 术语</h3>
<ul>
<li>簇: 所有数据点点集合，簇中的对象是相似的。</li>
<li>质心: 簇中所有点的中心（计算所有点的均值而来）.</li>
<li>SSE: Sum of Sqared Error（平方误差和）, SSE 值越小，表示越接近它们的质心. 由于对误差取了平方，因此更加注重那么远离中心的点.</li>
</ul>
<p>有关 <code>簇</code> 和 <code>质心</code> 术语更形象的介绍, 请参考下图:</p>
<p><img alt="K-Means 术语图" src="https://github.com/1007530194/datas/blob/master/images/blog/books/deeplearningdo/10.KMeans/apachecn-k-means-term-1.jpg?raw=true"/></p>
<h3>K-Means 工作流程</h3>
<ol>
<li>首先, 随机确定 K 个初始点作为质心（不是数据中的点）.</li>
<li>然后将数据集中的每个点分配到一个簇中, 具体来讲, 就是为每个点找到距其最近的质心, 并将其分配该质心所对应的簇. 这一步完成之后, 每个簇的质心更新为该簇所有点的平均值.</li>
</ol>
<p>上述过程的 <code>伪代码</code> 如下:</p>
<ul>
<li>创建 k 个点作为起始质心（通常是随机选择）</li>
<li>当任意一个点的簇分配结果发生改变时<ul>
<li>对数据集中的每个数据点<ul>
<li>对每个质心<ul>
<li>计算质心与数据点之间的距离</li>
</ul>
</li>
<li>将数据点分配到距其最近的簇</li>
</ul>
</li>
<li>对每一个簇, 计算簇中所有点的均值并将均值作为质心</li>
</ul>
</li>
</ul>
<h3>K-Means 开发流程</h3>
<div class="highlight"><pre><span></span>收集数据：使用任意方法
准备数据：需要数值型数据类计算距离, 也可以将标称型数据映射为二值型数据再用于距离计算
分析数据：使用任意方法
训练算法：此步骤不适用于 K-Means 算法
测试算法：应用聚类算法、观察结果.可以使用量化的误差指标如误差平方和（后面会介绍）来评价算法的结果.
使用算法：可以用于所希望的任何应用.通常情况下, 簇质心可以代表整个簇的数据来做出决策.
</pre></div>
<h3>K-Means 聚类算法函数</h3>
<h4>从文件加载数据集</h4>
<div class="highlight"><pre><span></span><span class="c1"># 从文本中构建矩阵，加载文本文件，然后处理</span>
<span class="k">def</span> <span class="nf">loadDataSet</span><span class="p">(</span><span class="n">fileName</span><span class="p">):</span>    <span class="c1"># 通用函数，用来解析以 tab 键分隔的 floats（浮点数），例如: 1.658985    4.285136</span>
    <span class="n">dataMat</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">fr</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">fileName</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">fr</span><span class="o">.</span><span class="n">readlines</span><span class="p">():</span>
        <span class="n">curLine</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">'</span><span class="se">\t</span><span class="s1">'</span><span class="p">)</span>
        <span class="n">fltLine</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="nb">float</span><span class="p">,</span><span class="n">curLine</span><span class="p">)</span>    <span class="c1"># 映射所有的元素为 float（浮点数）类型</span>
        <span class="n">dataMat</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fltLine</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dataMat</span>
</pre></div>
<h4>计算两个向量的欧氏距离</h4>
<div class="highlight"><pre><span></span><span class="c1"># 计算两个向量的欧式距离（可根据场景选择）</span>
<span class="k">def</span> <span class="nf">distEclud</span><span class="p">(</span><span class="n">vecA</span><span class="p">,</span> <span class="n">vecB</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sqrt</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">power</span><span class="p">(</span><span class="n">vecA</span> <span class="o">-</span> <span class="n">vecB</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span> <span class="c1"># la.norm(vecA-vecB)</span>
</pre></div>
<h4>构建一个包含 K 个随机质心的集合</h4>
<div class="highlight"><pre><span></span><span class="c1"># 为给定数据集构建一个包含 k 个随机质心的集合。随机质心必须要在整个数据集的边界之内，这可以通过找到数据集每一维的最小和最大值来完成。然后生成 0~1.0 之间的随机数并通过取值范围和最小值，以便确保随机点在数据的边界之内。</span>
<span class="k">def</span> <span class="nf">randCent</span><span class="p">(</span><span class="n">dataSet</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="n">dataSet</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># 列的数量</span>
    <span class="n">centroids</span> <span class="o">=</span> <span class="n">mat</span><span class="p">(</span><span class="n">zeros</span><span class="p">((</span><span class="n">k</span><span class="p">,</span><span class="n">n</span><span class="p">)))</span> <span class="c1"># 创建k个质心矩阵</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span> <span class="c1"># 创建随机簇质心，并且在每一维的边界内</span>
        <span class="n">minJ</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">dataSet</span><span class="p">[:,</span><span class="n">j</span><span class="p">])</span>    <span class="c1"># 最小值</span>
        <span class="n">rangeJ</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">dataSet</span><span class="p">[:,</span><span class="n">j</span><span class="p">])</span> <span class="o">-</span> <span class="n">minJ</span><span class="p">)</span>    <span class="c1"># 范围 = 最大值 - 最小值</span>
        <span class="n">centroids</span><span class="p">[:,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">mat</span><span class="p">(</span><span class="n">minJ</span> <span class="o">+</span> <span class="n">rangeJ</span> <span class="o">*</span> <span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>    <span class="c1"># 随机生成</span>
    <span class="k">return</span> <span class="n">centroids</span>
</pre></div>
<h4>K-Means 聚类算法</h4>
<div class="highlight"><pre><span></span><span class="c1"># k-means 聚类算法</span>
<span class="c1"># 该算法会创建k个质心，然后将每个点分配到最近的质心，再重新计算质心。</span>
<span class="c1"># 这个过程重复数次，直到数据点的簇分配结果不再改变位置。</span>
<span class="c1"># 运行结果（多次运行结果可能会不一样，可以试试，原因为随机质心的影响，但总的结果是对的， 因为数据足够相似，也可能会陷入局部最小值）</span>
<span class="k">def</span> <span class="nf">kMeans</span><span class="p">(</span><span class="n">dataSet</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">distMeas</span><span class="o">=</span><span class="n">distEclud</span><span class="p">,</span> <span class="n">createCent</span><span class="o">=</span><span class="n">randCent</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="n">dataSet</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>    <span class="c1"># 行数</span>
    <span class="n">clusterAssment</span> <span class="o">=</span> <span class="n">mat</span><span class="p">(</span><span class="n">zeros</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>    <span class="c1"># 创建一个与 dataSet 行数一样，但是有两列的矩阵，用来保存簇分配结果</span>
    <span class="n">centroids</span> <span class="o">=</span> <span class="n">createCent</span><span class="p">(</span><span class="n">dataSet</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>    <span class="c1"># 创建质心，随机k个质心</span>
    <span class="n">clusterChanged</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="k">while</span> <span class="n">clusterChanged</span><span class="p">:</span>
        <span class="n">clusterChanged</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>    <span class="c1"># 循环每一个数据点并分配到最近的质心中去</span>
            <span class="n">minDist</span> <span class="o">=</span> <span class="n">inf</span><span class="p">;</span> <span class="n">minIndex</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
                <span class="n">distJI</span> <span class="o">=</span> <span class="n">distMeas</span><span class="p">(</span><span class="n">centroids</span><span class="p">[</span><span class="n">j</span><span class="p">,:],</span><span class="n">dataSet</span><span class="p">[</span><span class="n">i</span><span class="p">,:])</span>    <span class="c1"># 计算数据点到质心的距离</span>
                <span class="k">if</span> <span class="n">distJI</span> <span class="o">&lt;</span> <span class="n">minDist</span><span class="p">:</span>    <span class="c1"># 如果距离比 minDist（最小距离）还小，更新 minDist（最小距离）和最小质心的 index（索引）</span>
                    <span class="n">minDist</span> <span class="o">=</span> <span class="n">distJI</span><span class="p">;</span> <span class="n">minIndex</span> <span class="o">=</span> <span class="n">j</span>
            <span class="k">if</span> <span class="n">clusterAssment</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">minIndex</span><span class="p">:</span>    <span class="c1"># 簇分配结果改变</span>
                <span class="n">clusterChanged</span> <span class="o">=</span> <span class="bp">True</span>    <span class="c1"># 簇改变</span>
                <span class="n">clusterAssment</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">minIndex</span><span class="p">,</span><span class="n">minDist</span><span class="o">**</span><span class="mi">2</span>    <span class="c1"># 更新簇分配结果为最小质心的 index（索引），minDist（最小距离）的平方</span>
        <span class="k">print</span> <span class="n">centroids</span>
        <span class="k">for</span> <span class="n">cent</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span> <span class="c1"># 更新质心</span>
            <span class="n">ptsInClust</span> <span class="o">=</span> <span class="n">dataSet</span><span class="p">[</span><span class="n">nonzero</span><span class="p">(</span><span class="n">clusterAssment</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">A</span><span class="o">==</span><span class="n">cent</span><span class="p">)[</span><span class="mi">0</span><span class="p">]]</span> <span class="c1"># 获取该簇中的所有点</span>
            <span class="n">centroids</span><span class="p">[</span><span class="n">cent</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">ptsInClust</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># 将质心修改为簇中所有点的平均值，mean 就是求平均值的</span>
    <span class="k">return</span> <span class="n">centroids</span><span class="p">,</span> <span class="n">clusterAssment</span>
</pre></div>
<h4>测试函数</h4>
<ol>
<li>测试一下以上的基础函数是否可以如预期运行, 请看: <a href="https://github.com/apachecn/MachineLearning/blob/master/src/python/10.kmeans/kMeans.py">https://github.com/apachecn/MachineLearning/blob/master/src/python/10.kmeans/kMeans.py</a></li>
<li>测试一下 kMeans 函数是否可以如预期运行, 请看: <a href="https://github.com/apachecn/MachineLearning/blob/master/src/python/10.kmeans/kMeans.py">https://github.com/apachecn/MachineLearning/blob/master/src/python/10.kmeans/kMeans.py</a> </li>
</ol>
<p>参考运行结果如下:<br/>
<img alt="K-Means 运行结果1" src="https://github.com/1007530194/datas/blob/master/images/blog/books/deeplearningdo/10.KMeans/apachecn-k-means-run-result-1.jpg?raw=true"/></p>
<blockquote>
<p>在 kMeans 的函数测试中，可能偶尔会陷入局部最小值（局部最优的结果，但不是全局最优的结果）.</p>
</blockquote>
<h3>K-Means 聚类算法的缺陷</h3>
<p>在 kMeans 的函数测试中，可能偶尔会陷入局部最小值（局部最优的结果，但不是全局最优的结果）. <br/>
局部最小值的的情况如下:<br/>
<img alt="K-Means 局部最小值1" src="https://github.com/1007530194/datas/blob/master/images/blog/books/deeplearningdo/10.KMeans/apachecn-kmeans-partial-best-result-1.jpg?raw=true"/></p>
<p>所以为了克服 KMeans 算法收敛于局部最小值的问题，有更厉害的大佬提出了另一个称之为二分K-均值（bisecting K-Means）的算法.   </p>
<h3>二分 K-Means 聚类算法</h3>
<p>该算法首先将所有点作为一个簇，然后将该簇一分为二。<br/>
之后选择其中一个簇继续进行划分，选择哪一个簇进行划分取决于对其划分时候可以最大程度降低 SSE（平方和误差）的值。<br/>
上述基于 SSE 的划分过程不断重复，直到得到用户指定的簇数目为止。  </p>
<h4>二分 K-Means 聚类算法伪代码</h4>
<ul>
<li>将所有点看成一个簇</li>
<li>当簇数目小雨 k 时</li>
<li>对于每一个簇<ul>
<li>计算总误差</li>
<li>在给定的簇上面进行 KMeans 聚类（k=2）</li>
<li>计算将该簇一分为二之后的总误差</li>
</ul>
</li>
<li>选择使得误差最小的那个簇进行划分操作</li>
</ul>
<p>另一种做法是选择 SSE 最大的簇进行划分，直到簇数目达到用户指定的数目位置。
接下来主要介绍该做法。</p>
<h4>二分 K-Means 聚类算法代码</h4>
<div class="highlight"><pre><span></span><span class="c1"># 二分 KMeans 聚类算法, 基于 kMeans 基础之上的优化，以避免陷入局部最小值</span>
<span class="k">def</span> <span class="nf">biKMeans</span><span class="p">(</span><span class="n">dataSet</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">distMeas</span><span class="o">=</span><span class="n">distEclud</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="n">dataSet</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">clusterAssment</span> <span class="o">=</span> <span class="n">mat</span><span class="p">(</span><span class="n">zeros</span><span class="p">((</span><span class="n">m</span><span class="p">,</span><span class="mi">2</span><span class="p">)))</span> <span class="c1"># 保存每个数据点的簇分配结果和平方误差</span>
    <span class="n">centroid0</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">dataSet</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># 质心初始化为所有数据点的均值</span>
    <span class="n">centList</span> <span class="o">=</span><span class="p">[</span><span class="n">centroid0</span><span class="p">]</span> <span class="c1"># 初始化只有 1 个质心的 list</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span> <span class="c1"># 计算所有数据点到初始质心的距离平方误差</span>
        <span class="n">clusterAssment</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">distMeas</span><span class="p">(</span><span class="n">mat</span><span class="p">(</span><span class="n">centroid0</span><span class="p">),</span> <span class="n">dataSet</span><span class="p">[</span><span class="n">j</span><span class="p">,:])</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">while</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">centList</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">k</span><span class="p">):</span> <span class="c1"># 当质心数量小于 k 时</span>
        <span class="n">lowestSSE</span> <span class="o">=</span> <span class="n">inf</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">centList</span><span class="p">)):</span> <span class="c1"># 对每一个质心</span>
            <span class="n">ptsInCurrCluster</span> <span class="o">=</span> <span class="n">dataSet</span><span class="p">[</span><span class="n">nonzero</span><span class="p">(</span><span class="n">clusterAssment</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">A</span><span class="o">==</span><span class="n">i</span><span class="p">)[</span><span class="mi">0</span><span class="p">],:]</span> <span class="c1"># 获取当前簇 i 下的所有数据点</span>
            <span class="n">centroidMat</span><span class="p">,</span> <span class="n">splitClustAss</span> <span class="o">=</span> <span class="n">kMeans</span><span class="p">(</span><span class="n">ptsInCurrCluster</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">distMeas</span><span class="p">)</span> <span class="c1"># 将当前簇 i 进行二分 kMeans 处理</span>
            <span class="n">sseSplit</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">splitClustAss</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># 将二分 kMeans 结果中的平方和的距离进行求和</span>
            <span class="n">sseNotSplit</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">clusterAssment</span><span class="p">[</span><span class="n">nonzero</span><span class="p">(</span><span class="n">clusterAssment</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">A</span><span class="o">!=</span><span class="n">i</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># 将未参与二分 kMeans 分配结果中的平方和的距离进行求和</span>
            <span class="k">print</span> <span class="s2">"sseSplit, and notSplit: "</span><span class="p">,</span><span class="n">sseSplit</span><span class="p">,</span><span class="n">sseNotSplit</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">sseSplit</span> <span class="o">+</span> <span class="n">sseNotSplit</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">lowestSSE</span><span class="p">:</span> <span class="c1"># 总的（未拆分和已拆分）误差和越小，越相似，效果越优化，划分的结果更好（注意：这里的理解很重要，不明白的地方可以和我们一起讨论）</span>
                <span class="n">bestCentToSplit</span> <span class="o">=</span> <span class="n">i</span>
                <span class="n">bestNewCents</span> <span class="o">=</span> <span class="n">centroidMat</span>
                <span class="n">bestClustAss</span> <span class="o">=</span> <span class="n">splitClustAss</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
                <span class="n">lowestSSE</span> <span class="o">=</span> <span class="n">sseSplit</span> <span class="o">+</span> <span class="n">sseNotSplit</span>
        <span class="c1"># 找出最好的簇分配结果    </span>
        <span class="n">bestClustAss</span><span class="p">[</span><span class="n">nonzero</span><span class="p">(</span><span class="n">bestClustAss</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">A</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">centList</span><span class="p">)</span> <span class="c1"># 调用二分 kMeans 的结果，默认簇是 0,1. 当然也可以改成其它的数字</span>
        <span class="n">bestClustAss</span><span class="p">[</span><span class="n">nonzero</span><span class="p">(</span><span class="n">bestClustAss</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">A</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">bestCentToSplit</span> <span class="c1"># 更新为最佳质心</span>
        <span class="k">print</span> <span class="s1">'the bestCentToSplit is: '</span><span class="p">,</span><span class="n">bestCentToSplit</span>
        <span class="k">print</span> <span class="s1">'the len of bestClustAss is: '</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">bestClustAss</span><span class="p">)</span>
        <span class="c1"># 更新质心列表</span>
        <span class="n">centList</span><span class="p">[</span><span class="n">bestCentToSplit</span><span class="p">]</span> <span class="o">=</span> <span class="n">bestNewCents</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># 更新原质心 list 中的第 i 个质心为使用二分 kMeans 后 bestNewCents 的第一个质心</span>
        <span class="n">centList</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bestNewCents</span><span class="p">[</span><span class="mi">1</span><span class="p">,:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># 添加 bestNewCents 的第二个质心</span>
        <span class="n">clusterAssment</span><span class="p">[</span><span class="n">nonzero</span><span class="p">(</span><span class="n">clusterAssment</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">A</span> <span class="o">==</span> <span class="n">bestCentToSplit</span><span class="p">)[</span><span class="mi">0</span><span class="p">],:]</span><span class="o">=</span> <span class="n">bestClustAss</span> <span class="c1"># 重新分配最好簇下的数据（质心）以及SSE</span>
    <span class="k">return</span> <span class="n">mat</span><span class="p">(</span><span class="n">centList</span><span class="p">),</span> <span class="n">clusterAssment</span>
</pre></div>
<h4>测试二分 KMeans 聚类算法</h4>
<ul>
<li>测试一下二分 KMeans 聚类算法，请看: <a href="https://github.com/apachecn/MachineLearning/blob/master/src/python/10.kmeans/kMeans.py">https://github.com/apachecn/MachineLearning/blob/master/src/python/10.kmeans/kMeans.py</a></li>
</ul>
<p>上述函数可以运行多次，聚类会收敛到全局最小值，而原始的 kMeans() 函数偶尔会陷入局部最小值。<br/>
运行参考结果如下:<br/>
<img alt="二分 K-Means 运行结果1" src="https://github.com/1007530194/datas/blob/master/images/blog/books/deeplearningdo/10.KMeans/apachecn-bikmeans-run-result-1.jpg?raw=true"/></p>
<ul>
<li><strong>作者：<a href="http://www.apache.wiki/display/~xuxin">那伊抹微笑</a></strong></li>
<li><a href="https://github.com/apachecn/MachineLearning">GitHub地址</a>: <a href="https://github.com/apachecn/MachineLearning">https://github.com/apachecn/MachineLearning</a></li>
<li><strong>版权声明：欢迎转载学习 =&gt; 请标注信息来源于 <a href="http://www.apachecn.org/">ApacheCN</a></strong></li>
</ul></div>


            <footer>
<span class="byline author vcard">
    Posted by
    <span class="fn">
            niult
    </span>
</span><time datetime="2017-01-01T00:00:00+08:00" pubdate>2017-01-01 00:00</time><span class="categories">
        <a href="../../../category/ji qi xue xi shi zhan.html">机器学习实战</a>
</span>


<div class="sharing">
</div>            </footer>
        </article>

    </div>
<aside class="sidebar">
    <section>
        <h1>Recent Posts</h1>
        <ul id="recent_posts">
                <li class="post">
                    <a href="../../../pages/2019/01/21-a-first-look-at-a-neural-network.html">2.1-a-first-look-at-a-neural-network</a>
                </li>
                <li class="post">
                    <a href="../../../pages/2019/01/35-classifying-movie-reviews.html">3.5-classifying-movie-reviews</a>
                </li>
                <li class="post">
                    <a href="../../../pages/2019/01/36-classifying-newswires.html">3.6-classifying-newswires</a>
                </li>
                <li class="post">
                    <a href="../../../pages/2019/01/37-predicting-house-prices.html">3.7-predicting-house-prices</a>
                </li>
                <li class="post">
                    <a href="../../../pages/2019/01/44-overfitting-and-underfitting.html">4.4-overfitting-and-underfitting</a>
                </li>
        </ul>
    </section>
        <section>

            <h1>Categories</h1>
            <ul id="recent_posts">
                    <li><a href="../../../category/01chang yong gong ju.html">01常用工具</a></li>
                    <li><a href="../../../category/02.wo ai du shu.html">02.我爱读书</a></li>
                    <li><a href="../../../category/algorithms.html">algorithms</a></li>
                    <li><a href="../../../category/book.html">book</a></li>
                    <li><a href="../../../category/book-pydata.html">book-pydata</a></li>
                    <li><a href="../../../category/deep-learning-with-python.html">deep-learning-with-python</a></li>
                    <li><a href="../../../category/ji qi xue xi shi zhan.html">机器学习实战</a></li>
                    <li><a href="../../../category/ke wai du wu.html">课外读物</a></li>
                    <li><a href="../../../category/ling ji chu ru men shen du xue xi.html">零基础入门深度学习</a></li>
                    <li><a href="../../../category/shen du xue xi.html">深度学习</a></li>
                    <li><a href="../../../category/shen jing wang luo.html">神经网络</a></li>
                    <li><a href="../../../category/shu ju wa jue.html">数据挖掘</a></li>
                    <li><a href="../../../category/shu xue ji chu.html">数学基础</a></li>
                    <li><a href="../../../category/tf-example.html">tf-example</a></li>
                    <li><a href="../../../category/tool1.html">tool1</a></li>
                    <li><a href="../../../category/tool2.html">tool2</a></li>
                    <li><a href="../../../category/tools.html">tools</a></li>
                    <li><a href="../../../category/tui jian xi tong.html">推荐系统</a></li>
                    <li><a href="../../../category/wen ben wa jue.html">文本挖掘</a></li>
            </ul>
        </section>


    <section>
        <h1>Tags</h1>
            <a href="../../../tag/python.html">python</a>, 
            <a href="../../../tag/numpy.html">numpy</a>, 
            <a href="../../../tag/deep-learning.html">deep-learning</a>, 
            <a href="../../../tag/algorithms.html">algorithms</a>, 
            <a href="../../../tag/wen-ben-wa-jue.html">文本挖掘</a>, 
            <a href="../../../tag/shen-jing-wang-luo.html">神经网络</a>, 
            <a href="../../../tag/shu-xue-ji-chu.html">数学基础</a>, 
            <a href="../../../tag/nlp.html">nlp</a>, 
            <a href="../../../tag/tf-example.html">tf-example</a>, 
            <a href="../../../tag/tui-jian-xi-tong.html">推荐系统</a>, 
            <a href="../../../tag/tf.html">tf</a>, 
            <a href="../../../tag/ji-huo-han-shu.html">激活函数</a>, 
            <a href="../../../tag/mapreduce.html">mapreduce</a>, 
            <a href="../../../tag/spark.html">spark</a>, 
            <a href="../../../tag/handbook.html">handbook</a>, 
            <a href="../../../tag/matplotlib.html">matplotlib</a>, 
            <a href="../../../tag/scikit-learn.html">scikit-learn</a>, 
            <a href="../../../tag/latex.html">latex</a>, 
            <a href="../../../tag/pandas.html">pandas</a>, 
            <a href="../../../tag/jupyter.html">jupyter</a>, 
            <a href="../../../tag/plot.html">plot</a>, 
            <a href="../../../tag/pip.html">pip</a>, 
            <a href="../../../tag/geng-xin-suo-you-mo-kuai.html">更新所有模块</a>, 
            <a href="../../../tag/shen-du-xue-xi.html">深度学习</a>, 
            <a href="../../../tag/xun-huan-shen-jing-wang-luo.html">循环神经网络</a>, 
            <a href="../../../tag/pangrank.html">PangRank</a>, 
            <a href="../../../tag/book.html">book</a>, 
            <a href="../../../tag/pydata.html">pydata</a>, 
            <a href="../../../tag/shell.html">shell</a>, 
            <a href="../../../tag/pyhton.html">pyhton</a>
    </section>



    <section>
        <h1>GitHub Repos</h1>
            <a href="https://github.com/1007530194">@1007530194</a> on GitHub
    </section>

</aside>    </div>
</div>
<footer role="contentinfo"><p>
        活到老，学到老，玩到老
    <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>

    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-132396898-1', 'auto');

    ga('require', 'displayfeatures');
    ga('send', 'pageview');
    </script>
</body>
</html>