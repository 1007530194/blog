<!DOCTYPE html>
<html lang="en">


<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width,minimum-scale=1">

  <title>merge_5</title>
  <meta name="description" content="">

  <link rel="canonical" href="https://jupyter.org/blog/blog2/11ReadBook/03handbook-en/merge_5.html">
  <link rel="alternate" type="application/rss+xml" title="MyBook" href="https://jupyter.org/blog/blog2/feed.xml">

  <meta property="og:url"         content="https://jupyter.org/blog/blog2/11ReadBook/03handbook-en/merge_5.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="merge_5" />
<meta property="og:description" content="" />
<meta property="og:image"       content="" />


  <script type="application/ld+json">
  {
  "@context": "http://schema.org",
  "@type": "NewsArticle",
  "mainEntityOfPage":
    "https://jupyter.org/blog/blog2/11ReadBook/03handbook-en/merge_5.html",
  "headline":
    "merge_5",
  "datePublished":
    "2019-06-12T16:24:42+08:00",
  "dateModified":
    "2019-06-12T16:24:42+08:00",
  "description":
    "",
  "author": {
    "@type": "Person",
    "name": "niult"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Data 100 at UC Berkeley",
    "logo": {
      "@type": "ImageObject",
      "url": "https://jupyter.org/blog/blog2",
      "width": 60,
      "height": 60
    }
  },
  "image": {
    "@type": "ImageObject",
    "url": "https://jupyter.org/blog/blog2",
    "height": 60,
    "width": 60
  }
}

  </script>
  <link rel="stylesheet" href="/blog/blog2/assets/css/styles.css">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css ">
  <link rel="apple-touch-icon" sizes="57x57" href="/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon-180x180.png">

  <!-- <link rel="manifest" href="/manifest.json"> -->
  <!-- <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#efae0a"> -->
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/mstile-144x144.png">
  <meta name="theme-color" content="#233947">

  <!-- Favicon -->
  <link rel="shortcut icon" type="image/x-icon" href="/blog/blog2/assets/images/logo/favicon.ico">

  <!-- MathJax Config -->
  <!-- Allow inline math using $ and automatically break long math lines -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true,
        processEnvironments: true
    },
    CommonHTML: {
        linebreaks: {
            automatic: true,
        },
    },
});
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML' async></script>

  <!-- DOM updating function -->
  <script>
const runWhenDOMLoaded = cb => {
  if (document.readyState != 'loading') {
    cb()
  } else if (document.addEventListener) {
    document.addEventListener('DOMContentLoaded', cb)
  } else {
    document.attachEvent('onreadystatechange', function() {
      if (document.readyState == 'complete') cb()
    })
  }
}

// Helper function to init things quickly
initFunction = function(myfunc) {
  runWhenDOMLoaded(myfunc);
  document.addEventListener('turbolinks:load', myfunc);
};
</script>

  <!-- Define some javascript variables that will be useful in other javascript -->
  <script>
    const site_basename = '/blog/blog2';
  </script>

  <!-- Add AnchorJS to let headers be linked -->
  <script src="/blog/blog2/assets/js/anchor.min.js"  type="text/javascript"></script>
  <script>

initFunction(function () {
    anchors.add("main h1, main h2, main h3, main h4")
});

</script>

  <!-- Include Turbolinks to make page loads fast -->
  <!-- https://github.com/turbolinks/turbolinks -->
  <script src="/blog/blog2/assets/js/turbolinks.js" async></script>
  <meta name="turbolinks-cache-control" content="no-cache">

  <!-- Load nbinteract for widgets -->
  

  <!-- Load Thebelab for interactive widgets -->
  <!-- Include Thebelab for interactive code if it's enabled -->


<!-- Display Thebelab button in each code cell -->
<script>
    /**
     * Set up thebelab button for code blocks
     */

    const thebelabCellButton = id =
    >
    `<a id="thebelab-cell-button-${id}" class="btn thebebtn o-tooltip--left" data-tooltip="Interactive Mode">
    <img src="https://raw.githubusercontent.com/1007530194/images/master/picgo/blog/logo/edit-button.svg" alt="Start interactive mode">
  </a>`


    const addThebelabButtonToCodeCells = () =
    >
    {

        const codeCells = document.querySelectorAll('div.input_area > div.highlighter-rouge:not(.output) pre')
        codeCells.forEach((codeCell, index) = > {
            const id = codeCellId(index)
            codeCell.setAttribute('id', id)
        if (document.getElementById("thebelab-cell-button-" + id) == null) {
            codeCell.insertAdjacentHTML('afterend', thebelabCellButton(id));
        }
    })
    }

    initFunction(addThebelabButtonToCodeCells);
</script>



<script type="text/x-thebe-config">
    {
      requestKernel: true,
      binderOptions: {
        repo: 'jupyter/jupyter-book',
        ref: 'gh-pages',
      },
      codeMirrorConfig: {
        theme: "abcdef"
      },
      kernelOptions: {
        name: 'python3',
      }
    }
</script>
<script src="https://unpkg.com/thebelab@0.4.0/lib/index.js"></script>
<script>
    /**
     * Add attributes to Thebelab blocks
     */

    const initThebelab = () => {
        const addThebelabToCodeCells = () => {
            console.log("Adding thebelab to code cells...");
            // If Thebelab hasn't loaded, wait a bit and try again. This
            // happens because we load ClipboardJS asynchronously.
            if (window.thebelab === undefined) {
                setTimeout(addThebelabToCodeCells, 250)
            return
            }

            // If we already detect a Thebelab cell, don't re-run
            if (document.querySelectorAll('div.thebelab-cell').length > 0) {
                return;
            }

            // Find all code cells, replace with Thebelab interactive code cells
            const codeCells = document.querySelectorAll('.input_area pre')
            codeCells.forEach((codeCell, index) => {
                const id = codeCellId(index)
                codeCell.setAttribute('data-executable', 'true')

                // Figure out the language it uses and add this too
                var parentDiv = codeCell.parentElement.parentElement;
                var arrayLength = parentDiv.classList.length;
                for (var ii = 0; ii < arrayLength; ii++) {
                    var parts = parentDiv.classList[ii].split('language-');
                    if (parts.length === 2) {
                        // If found, assign dataLanguage and break the loop
                        var dataLanguage = parts[1];
                        break;
                    }
                }
                codeCell.setAttribute('data-language', dataLanguage)

                // If the code cell is hidden, show it
                var inputCheckbox = document.querySelector(`input#hidebtn${codeCell.id}`);
                if (inputCheckbox !== null) {
                    setCodeCellVisibility(inputCheckbox, 'visible');
                }
            });

            // Remove the event listener from the page so keyboard press doesn't
            // Change page
            document.removeEventListener('keydown', initPageNav)
            keyboardListener = false;

            // Init thebelab
            thebelab.bootstrap();

            // Remove copy buttons since they won't work anymore
            const copyAndThebeButtons = document.querySelectorAll('.copybtn, .thebebtn')
            copyAndThebeButtons.forEach((button, index) => {
                button.remove();
            });

            // Remove outputs since they'll be stale
            const outputs = document.querySelectorAll('.output *, .output')
            outputs.forEach((output, index) => {
                output.remove();
            });
        }

        // Add event listener for the function to modify code cells
        const thebelabButtons = document.querySelectorAll('[id^=thebelab], [id$=thebelab]')
        thebelabButtons.forEach((thebelabButton,index) => {
            if (thebelabButton === null) {
                setTimeout(initThebelab, 250)
                return
            };
            thebelabButton.addEventListener('click', addThebelabToCodeCells);
        });
    }

    // Initialize Thebelab
    initFunction(initThebelab);
</script>



  <!-- Load the auto-generating TOC -->
  <script src="/blog/blog2/assets/js/tocbot.min.js"  type="text/javascript"></script>
  <script>
var initToc = function () {
  tocbot.init({
    tocSelector: 'nav.onthispage',
    contentSelector: '.c-textbook__content',
    headingSelector: 'h2, h3',
    orderedList: false,
    collapseDepth: 6,
    listClass: 'toc__menu',
    activeListItemClass: "",  // Not using
    activeLinkClass: "", // Not using
  });
  tocbot.refresh();
}
initFunction(initToc);
</script>

  <!-- Google analytics -->
  <script src="/blog/blog2/assets/js/ga.js" async></script>

  <!-- Clipboard copy button -->
  <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" async></script>

  <!-- Load JS that depends on site variables -->
  <script>
/**
 * Set up copy/paste for code blocks
 */
const codeCellId = index => `codecell${index}`

const clipboardButton = id =>
  `<a id="copy-button-${id}" class="btn copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#${id}">
    <img src="/blog/blog2/assets/images/copy-button.svg" alt="Copy to clipboard">
  </a>`

// Clears selected text since ClipboardJS will select the text when copying
const clearSelection = () => {
  if (window.getSelection) {
    window.getSelection().removeAllRanges()
  } else if (document.selection) {
    document.selection.empty()
  }
}

// Changes tooltip text for two seconds, then changes it back
const temporarilyChangeTooltip = (el, newText) => {
  const oldText = el.getAttribute('data-tooltip')
  el.setAttribute('data-tooltip', newText)
  setTimeout(() => el.setAttribute('data-tooltip', oldText), 2000)
}

const addCopyButtonToCodeCells = () => {
  // If ClipboardJS hasn't loaded, wait a bit and try again. This
  // happens because we load ClipboardJS asynchronously.
  if (window.ClipboardJS === undefined) {
    setTimeout(addCopyButtonToCodeCells, 250)
    return
  }

  const codeCells = document.querySelectorAll('div.c-textbook__content > div.highlighter-rouge > div.highlight > pre, div.input_area pre')
  codeCells.forEach((codeCell, index) => {
    const id = codeCellId(index)
    codeCell.setAttribute('id', id)
    if (document.getElementById("copy-button" + id) == null) {
      codeCell.insertAdjacentHTML('afterend', clipboardButton(id));
    }
  })

  const clipboard = new ClipboardJS('.copybtn')
  clipboard.on('success', event => {
    clearSelection()
    temporarilyChangeTooltip(event.trigger, 'Copied!')
  })

  clipboard.on('error', event => {
    temporarilyChangeTooltip(event.trigger, 'Failed to copy')
  })

  // Get rid of clipboard before the next page visit to avoid memory leak
  document.addEventListener('turbolinks:before-visit', () =>
    clipboard.destroy()
  )
}

initFunction(addCopyButtonToCodeCells);
</script>


  <!-- Hide cell code -->
  
<script>
/**
Add buttons to hide code cells
*/


var setCodeCellVisibility = function(inputField, kind) {
    // Update the image and class for hidden
    var id = inputField.getAttribute('data-id');
    var codeCell = document.querySelector(`#${id} div.highlight`);

    if (kind === "visible") {
        codeCell.classList.remove('hidden');
        inputField.checked = true;
    } else {
        codeCell.classList.add('hidden');
        inputField.checked = false;
    }
}

var toggleCodeCellVisibility = function (event) {
    // The label is clicked, and now we decide what to do based on the input field's clicked status
    if (event.target.tagName === "LABEL") {
        var inputField = event.target.previousElementSibling;
    } else {
        // It is the span inside the target
        var inputField = event.target.parentElement.previousElementSibling;
    }

    if (inputField.checked === true) {
        setCodeCellVisibility(inputField, "visible");
    } else {
        setCodeCellVisibility(inputField, "hidden");
    }
}


// Button constructor
const hideCodeButton = id => `<input class="hidebtn" type="checkbox" id="hidebtn${id}" data-id="${id}"><label title="Toggle cell" for="hidebtn${id}" class="plusminus"><span class="pm_h"></span><span class="pm_v"></span></label>`

var addHideButton = function () {
  // If a hide button is already added, don't add another
  if (document.querySelector('div.hidecode input') !== null) {
      return;
  }

  // Find the input cells and add a hide button
  document.querySelectorAll('div.input_area').forEach(function (item, index) {
    if (!item.classList.contains("hidecode")) {
        // Skip the cell if it doesn't have a hidecode class
        return;
    }

    const id = codeCellId(index)
    item.setAttribute('id', id);
    // Insert the button just inside the end of the next div
    item.querySelector('div').insertAdjacentHTML('beforeend', hideCodeButton(id))

    // Set up the visibility toggle
    hideLink = document.querySelector(`#${id} div.highlight + input + label`);
    hideLink.addEventListener('click', toggleCodeCellVisibility)
  });
}


// Initialize the hide buttos
var initHiddenCells = function () {
    // Add hide buttons to the cells
    addHideButton();

    // Toggle the code cells that should be hidden
    document.querySelectorAll('div.hidecode input').forEach(function (item) {
        setCodeCellVisibility(item, 'hidden');
        item.checked = true;
    })
}

initFunction(initHiddenCells);

</script>


  <!-- Load custom website scripts -->
  <script src="/blog/blog2/assets/js/scripts.js" async></script>

  <!-- Load custom user CSS and JS  -->
  <script src="/blog/blog2/assets/custom/custom.js" async></script>
  <link rel="stylesheet" href="/blog/blog2/assets/custom/custom.css">

  <!-- Update interact links w/ REST param, is defined in includes so we can use templates -->
  
<script>
/**
  * To auto-embed hub URLs in interact links if given in a RESTful fashion
 */

function getJsonFromUrl(url) {
  var query = url.split('?');
  if (query.length < 2) {
    // No queries so just return false
    return false;
  }
  query = query[1];
  // Collect REST params into a dictionary
  var result = {};
  query.split("&").forEach(function(part) {
    var item = part.split("=");
    result[item[0]] = decodeURIComponent(item[1]);
  });
  return result;
}
    
function dict2param(dict) {
    params = Object.keys(dict).map(function(k) {
        return encodeURIComponent(k) + '=' + encodeURIComponent(dict[k])
    });
    return params.join('&')
}

// Parse a Binder URL, converting it to the string needed for JupyterHub
function binder2Jupyterhub(url) {
  newUrl = {};
  parts = url.split('v2/gh/')[1];
  // Grab the base repo information
  repoinfo = parts.split('?')[0];
  var [org, repo, ref] = repoinfo.split('/');
  newUrl['repo'] = ['https://github.com', org, repo].join('/');
  newUrl['branch'] = ref
  // Grab extra parameters passed
  params = getJsonFromUrl(url);
  if (params['filepath'] !== undefined) {
    newUrl['subPath'] = params['filepath']
  }
  return dict2param(newUrl);
}

// Filter out potentially unsafe characters to prevent xss
function safeUrl(url)
{
   return String(encodeURIComponent(url))
            .replace(/&/g, '&amp;')
            .replace(/"/g, '&quot;')
            .replace(/'/g, '&#39;')
            .replace(/</g, '&lt;')
            .replace(/>/g, '&gt;');
}

function addParamToInternalLinks(hub) {
  var links = document.querySelectorAll("a").forEach(function(link) {
    var href = link.href;
    // If the link is an internal link...
    if (href.search("https://jupyter.org") !== -1 || href.startsWith('/') || href.search("127.0.0.1:") !== -1) {
      // Assume we're an internal link, add the hub param to it
      var params = getJsonFromUrl(href);
      if (params !== false) {
        // We have REST params, so append a new one
        params['jupyterhub'] = hub;
      } else {
        // Create the REST params
        params = {'jupyterhub': hub};
      }
      // Update the link
      var newHref = href.split('?')[0] + '?' + dict2param(params);
      link.setAttribute('href', decodeURIComponent(newHref));
    }
  });
  return false;
}


// Update interact links
function updateInteractLink() {
    // hack to make this work since it expects a ? in the URL
    rest = getJsonFromUrl("?" + location.search.substr(1));
    jupyterHubUrl = rest['jupyterhub'];
    var hubType = null;
    var hubUrl = null;
    if (jupyterHubUrl !== undefined) {
      hubType = 'jupyterhub';
      hubUrl = jupyterHubUrl;
    }

    if (hubType !== null) {
      // Sanitize the hubUrl
      hubUrl = safeUrl(hubUrl);

      // Add HTTP text if omitted
      if (hubUrl.indexOf('http') < 0) {hubUrl = 'http://' + hubUrl;}
      var interactButtons = document.querySelectorAll("button.interact-button")
      var lastButton = interactButtons[interactButtons.length-1];
      var link = lastButton.parentElement;

      // If we've already run this, skip the link updating
      if (link.nextElementSibling !== null) {
        return;
      }

      // Update the link and add context div
      var href = link.getAttribute('href');
      if (lastButton.id === 'interact-button-binder') {
        // If binder links exist, we need to re-work them for jupyterhub
        if (hubUrl.indexOf('http%3A%2F%2Flocalhost') > -1) {
          // If localhost, assume we're working from a local Jupyter server and remove `/hub`
          first = [hubUrl, 'git-sync'].join('/')
        } else {
          first = [hubUrl, 'hub', 'user-redirect', 'git-sync'].join('/')
        }
        href = first + '?' + binder2Jupyterhub(href);
      } else {
        // If interact button isn't binderhub, assume it's jupyterhub
        // If JupyterHub links, we only need to replace the hub url
        href = href.replace("", hubUrl);
        if (hubUrl.indexOf('http%3A%2F%2Flocalhost') > -1) {
          // Assume we're working from a local Jupyter server and remove `/hub`
          href = href.replace("/hub/user-redirect", "");
        }
      }
      link.setAttribute('href', decodeURIComponent(href));

      // Add text after interact link saying where we're launching
      hubUrlNoHttp = decodeURIComponent(hubUrl).replace('http://', '').replace('https://', '');
      link.insertAdjacentHTML('afterend', '<div class="interact-context">on ' + hubUrlNoHttp + '</div>');

      // Update internal links so we retain the hub url
      addParamToInternalLinks(hubUrl);
    }
}

runWhenDOMLoaded(updateInteractLink)
document.addEventListener('turbolinks:load', updateInteractLink)
</script>


  <!-- Lunr search code - will only be executed on the /search page -->
  <script src="/blog/blog2/assets/js/lunr/lunr.min.js" type="text/javascript"></script>
  <script>var initQuery = function() {
  // See if we have a search box
  var searchInput = document.querySelector('input#lunr_search');
  if (searchInput === null) {
    return;
  }

  // Function to parse our lunr cache
  var idx = lunr(function () {
    this.field('title')
    this.field('excerpt')
    this.field('categories')
    this.field('tags')
    this.ref('id')

    this.pipeline.remove(lunr.trimmer)

    for (var item in store) {
      this.add({
        title: store[item].title,
        excerpt: store[item].excerpt,
        categories: store[item].categories,
        tags: store[item].tags,
        id: item
      })
    }
  });

  // Run search upon keyup
  searchInput.addEventListener('keyup', function () {
    var resultdiv = document.querySelector('#results');
    var query = document.querySelector("input#lunr_search").value.toLowerCase();
    var result =
      idx.query(function (q) {
        query.split(lunr.tokenizer.separator).forEach(function (term) {
          q.term(term, { boost: 100 })
          if(query.lastIndexOf(" ") != query.length-1){
            q.term(term, {  usePipeline: false, wildcard: lunr.Query.wildcard.TRAILING, boost: 10 })
          }
          if (term != ""){
            q.term(term, {  usePipeline: false, editDistance: 1, boost: 1 })
          }
        })
      });

      // Empty the results div
      while (resultdiv.firstChild) {
        resultdiv.removeChild(resultdiv.firstChild);
      }

    resultdiv.insertAdjacentHTML('afterbegin', '<p class="results__found">'+result.length+' Result(s) found</p>');
    for (var item in result) {
      var ref = result[item].ref;
      if(store[ref].teaser){
        var searchitem =
          '<div class="list__item">'+
            '<article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">'+
              '<h2 class="archive__item-title" itemprop="headline">'+
                '<a href="'+store[ref].url+'" rel="permalink">'+store[ref].title+'</a>'+
              '</h2>'+
              '<div class="archive__item-teaser">'+
                '<img src="'+store[ref].teaser+'" alt="">'+
              '</div>'+
              '<p class="archive__item-excerpt" itemprop="description">'+store[ref].excerpt.split(" ").splice(0,20).join(" ")+'...</p>'+
            '</article>'+
          '</div>';
      }
      else{
    	  var searchitem =
          '<div class="list__item">'+
            '<article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">'+
              '<h2 class="archive__item-title" itemprop="headline">'+
                '<a href="'+store[ref].url+'" rel="permalink">'+store[ref].title+'</a>'+
              '</h2>'+
              '<p class="archive__item-excerpt" itemprop="description">'+store[ref].excerpt.split(" ").splice(0,20).join(" ")+'...</p>'+
            '</article>'+
          '</div>';
      }
      resultdiv.insertAdjacentHTML('beforeend', searchitem);
    }
  });
};

initFunction(initQuery);
</script>
</head>

<body>
<!-- .js-show-sidebar shows sidebar by default -->
<div id="js-textbook" class="c-textbook js-show-sidebar">
    



<nav id="js-sidebar" class="c-textbook__sidebar">
  <a href="https://jupyter.org/jupyter-book/intro.html"><img src="/blog/blog2/assets/images/logo/logo.png" class="textbook_logo" id="sidebar-logo" data-turbolinks-permanent/></a>
  <h2 class="c-sidebar__title">MyBook</h2>
  <ul class="c-sidebar__chapters">
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/blog/blog2/00Todo/index.html"
        >
          
            1.
          
          Todo
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/00Todo/00Todo.html"
                >
                  
                    1.1
                  
                  Todo
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/00Todo/Example.html"
                >
                  
                    1.2
                  
                  Example
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/00Todo/jupyter-book-start.html"
                >
                  
                    1.3
                  
                  jupyter-book-start
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/blog/blog2/01TensorFlow/index.html"
        >
          
            2.
          
          TensorFlow
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/01TensorFlow/DNN%E5%AE%9E%E4%BE%8B%E4%BB%A3%E7%A0%81%E5%9D%97.html"
                >
                  
                    2.1
                  
                  DNN实例代码块
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/01TensorFlow/DSSM%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B.html"
                >
                  
                    2.2
                  
                  DSSM双塔模型
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/01TensorFlow/keras-RNN%E5%AE%9E%E4%BE%8B.html"
                >
                  
                    2.3
                  
                  keras-RNN实例
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/01TensorFlow/KerasExample/index.html"
                >
                  
                    2.4
                  
                  KerasExample
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/KerasExample/addition_rnn.html"
                    >
                      
                        2.4.1
                        
                      
                      addition_rnn
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/KerasExample/antirectifier.html"
                    >
                      
                        2.4.2
                        
                      
                      antirectifier
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/KerasExample/babi_memnn.html"
                    >
                      
                        2.4.3
                        
                      
                      babi_memnn
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/KerasExample/babi_rnn.html"
                    >
                      
                        2.4.4
                        
                      
                      babi_rnn
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/KerasExample/image_ocr.html"
                    >
                      
                        2.4.5
                        
                      
                      image_ocr
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/KerasExample/imdb_bidirectional_lstm.html"
                    >
                      
                        2.4.6
                        
                      
                      imdb_bidirectional_lstm
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/KerasExample/imdb_cnn.html"
                    >
                      
                        2.4.7
                        
                      
                      imdb_cnn
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/KerasExample/imdb_cnn_lstm.html"
                    >
                      
                        2.4.8
                        
                      
                      imdb_cnn_lstm
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/KerasExample/imdb_lstm.html"
                    >
                      
                        2.4.9
                        
                      
                      imdb_lstm
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/KerasExample/lstm-seq2seq.html"
                    >
                      
                        2.4.10
                        
                      
                      lstm-seq2seq
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/KerasExample/mnist_acgan.html"
                    >
                      
                        2.4.11
                        
                      
                      mnist_acgan
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/KerasExample/mnist_mlp.html"
                    >
                      
                        2.4.12
                        
                      
                      mnist_mlp
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/KerasExample/README.html"
                    >
                      
                        2.4.13
                        
                      
                      README
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/KerasExample/reuters_mlp.html"
                    >
                      
                        2.4.14
                        
                      
                      reuters_mlp
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/KerasExample/tensorboard_embeddings_mnist.html"
                    >
                      
                        2.4.15
                        
                      
                      tensorboard_embeddings_mnist
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/01TensorFlow/keras%E6%97%A5%E5%B8%B8%E5%B0%8F%E5%B7%A5%E5%85%B7.html"
                >
                  
                    2.5
                  
                  keras日常小工具
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/01TensorFlow/kera%E5%AD%A6%E4%B9%A0-%E6%9B%B4%E6%96%B0.html"
                >
                  
                    2.6
                  
                  kera学习-更新
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/01TensorFlow/kera%E5%AE%9E%E4%BE%8B%E4%BA%8C.html"
                >
                  
                    2.7
                  
                  kera实例二
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/01TensorFlow/recommendation/index.html"
                >
                  
                    2.8
                  
                  recommendation
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/recommendation/DeepFM%E6%A8%A1%E5%9E%8B%E7%90%86%E8%AE%BA%E5%92%8C%E5%AE%9E%E8%B7%B5.html"
                    >
                      
                        2.8.1
                        
                      
                      DeepFM模型理论和实践
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/recommendation/DIN%E6%A8%A1%E5%9E%8B%E7%90%86%E8%AE%BA%E5%92%8C%E5%AE%9E%E8%B7%B5.html"
                    >
                      
                        2.8.2
                        
                      
                      DIN模型理论和实践
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/recommendation/LR.html"
                    >
                      
                        2.8.3
                        
                      
                      LR
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/recommendation/MLR%E6%A8%A1%E5%9E%8B%E7%90%86%E8%AE%BA%E5%92%8C%E5%AE%9E%E8%B7%B5.html"
                    >
                      
                        2.8.4
                        
                      
                      MLR模型理论和实践
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/01TensorFlow/tensorflowExample/index.html"
                >
                  
                    2.9
                  
                  tensorflowExample
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0001-ml_introduction.html"
                    >
                      
                        2.9.1
                        
                      
                      ml_introduction
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0002-mnist_dataset_intro.html"
                    >
                      
                        2.9.2
                        
                      
                      mnist_dataset_intro
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0101-helloworld.html"
                    >
                      
                        2.9.3
                        
                      
                      helloworld
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0102-basic_eager_api.html"
                    >
                      
                        2.9.4
                        
                      
                      basic_eager_api
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0103-basic_operations.html"
                    >
                      
                        2.9.5
                        
                      
                      basic_operations
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0201-linear_regression_eager_api.html"
                    >
                      
                        2.9.6
                        
                      
                      linear_regression_eager_api
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0202-linear_regression.html"
                    >
                      
                        2.9.7
                        
                      
                      linear_regression
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0203-logistic_regression_eager_api.html"
                    >
                      
                        2.9.8
                        
                      
                      logistic_regression_eager_api
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0204-nearest_neighbor.html"
                    >
                      
                        2.9.9
                        
                      
                      nearest_neighbor
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0205-kmeans.html"
                    >
                      
                        2.9.10
                        
                      
                      kmeans
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0206-LR.html"
                    >
                      
                        2.9.11
                        
                      
                      LR
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0207-GBDT.html"
                    >
                      
                        2.9.12
                        
                      
                      GBDT
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0208-word2vec.html"
                    >
                      
                        2.9.13
                        
                      
                      word2vec
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0209-random_forest.html"
                    >
                      
                        2.9.14
                        
                      
                      random_forest
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0301-autoencoder.html"
                    >
                      
                        2.9.15
                        
                      
                      autoencoder
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0302-bidirectional_rnn.html"
                    >
                      
                        2.9.16
                        
                      
                      bidirectional_rnn
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0303-convolutional_network_raw.html"
                    >
                      
                        2.9.17
                        
                      
                      convolutional_network_raw
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0304-convolutional_network.html"
                    >
                      
                        2.9.18
                        
                      
                      convolutional_network
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0305-dcgan.html"
                    >
                      
                        2.9.19
                        
                      
                      dcgan
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0306-dynamic_rnn.html"
                    >
                      
                        2.9.20
                        
                      
                      dynamic_rnn
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0307-gan.html"
                    >
                      
                        2.9.21
                        
                      
                      gan
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0308-neural_network_eager_api.html"
                    >
                      
                        2.9.22
                        
                      
                      neural_network_eager_api
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0309-neural_network_raw.html"
                    >
                      
                        2.9.23
                        
                      
                      neural_network_raw
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0310-neural_network.html"
                    >
                      
                        2.9.24
                        
                      
                      neural_network
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0311-recurrent_network.html"
                    >
                      
                        2.9.25
                        
                      
                      recurrent_network
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0312-variational_autoencoder.html"
                    >
                      
                        2.9.26
                        
                      
                      variational_autoencoder
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0401-save_restore_model.html"
                    >
                      
                        2.9.27
                        
                      
                      save_restore_model
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0402-tensorboard_advanced.html"
                    >
                      
                        2.9.28
                        
                      
                      tensorboard_advanced
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0403-tensorboard_basic.html"
                    >
                      
                        2.9.29
                        
                      
                      tensorboard_basic
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0501-build_an_image_dataset.html"
                    >
                      
                        2.9.30
                        
                      
                      build_an_image_dataset
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0502-tensorflow_dataset_api.html"
                    >
                      
                        2.9.31
                        
                      
                      tensorflow_dataset_api
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0601-multigpu_basics.html"
                    >
                      
                        2.9.32
                        
                      
                      multigpu_basics
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/01TensorFlow/tensorflowExample/0602-multigpu_cnn.html"
                    >
                      
                        2.9.33
                        
                      
                      multigpu_cnn
                    </a>
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/blog/blog2/11ReadBook/index.html"
        >
          
            3.
          
          ReadBook
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections ">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/11ReadBook/00%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/index.html"
                >
                  
                    3.1
                  
                  统计学习方法
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/00%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/00%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%AE%BA.html"
                    >
                      
                        3.1.1
                        
                      
                      统计学习方法概论
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/00%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/02%E6%84%9F%E7%9F%A5%E6%9C%BA.html"
                    >
                      
                        3.1.2
                        
                      
                      感知机
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/00%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/03k%E8%BF%91%E9%82%BB%E6%B3%95.html"
                    >
                      
                        3.1.3
                        
                      
                      k近邻法
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/00%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/04%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF.html"
                    >
                      
                        3.1.4
                        
                      
                      朴素贝叶斯
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/00%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/05%E5%86%B3%E7%AD%96%E6%A0%91.html"
                    >
                      
                        3.1.5
                        
                      
                      决策树
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/00%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/06%E9%80%BB%E8%BE%91%E6%96%AF%E8%B0%9B%E5%9B%9E%E5%BD%92.html"
                    >
                      
                        3.1.6
                        
                      
                      逻辑斯谛回归
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/00%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/07%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.html"
                    >
                      
                        3.1.7
                        
                      
                      支持向量机
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/00%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/08%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95.html"
                    >
                      
                        3.1.8
                        
                      
                      提升方法
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/00%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/09%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6%E6%8E%A8%E5%B9%BF.html"
                    >
                      
                        3.1.9
                        
                      
                      算法及其推广
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/00%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/10%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B.html"
                    >
                      
                        3.1.10
                        
                      
                      隐马尔可夫模型
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/00%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/11%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA.html"
                    >
                      
                        3.1.11
                        
                      
                      条件随机场
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/11ReadBook/02deep-learning-with-python/index.html"
                >
                  
                    3.2
                  
                  deep-learning-with-python
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/02deep-learning-with-python/21-a-first-look-at-a-neural-network.html"
                    >
                      
                        3.2.1
                        
                      
                      a-first-look-at-a-neural-network
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/02deep-learning-with-python/35-classifying-movie-reviews.html"
                    >
                      
                        3.2.2
                        
                      
                      classifying-movie-reviews
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/02deep-learning-with-python/36-classifying-newswires.html"
                    >
                      
                        3.2.3
                        
                      
                      classifying-newswires
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/02deep-learning-with-python/37-predicting-house-prices.html"
                    >
                      
                        3.2.4
                        
                      
                      predicting-house-prices
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/02deep-learning-with-python/44-overfitting-and-underfitting.html"
                    >
                      
                        3.2.5
                        
                      
                      overfitting-and-underfitting
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/02deep-learning-with-python/51-introduction-to-convnets.html"
                    >
                      
                        3.2.6
                        
                      
                      introduction-to-convnets
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/02deep-learning-with-python/52-using-convnets-with-small-datasets.html"
                    >
                      
                        3.2.7
                        
                      
                      using-convnets-with-small-datasets
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/02deep-learning-with-python/53-using-a-pretrained-convnet.html"
                    >
                      
                        3.2.8
                        
                      
                      using-a-pretrained-convnet
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/02deep-learning-with-python/54-visualizing-what-convnets-learn.html"
                    >
                      
                        3.2.9
                        
                      
                      visualizing-what-convnets-learn
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/02deep-learning-with-python/61-one-hot-encoding-of-words-or-characters.html"
                    >
                      
                        3.2.10
                        
                      
                      one-hot-encoding-of-words-or-characters
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/02deep-learning-with-python/61-using-word-embeddings.html"
                    >
                      
                        3.2.11
                        
                      
                      using-word-embeddings
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/02deep-learning-with-python/62-understanding-recurrent-neural-networks.html"
                    >
                      
                        3.2.12
                        
                      
                      understanding-recurrent-neural-networks
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/02deep-learning-with-python/63-advanced-usage-of-recurrent-neural-networks.html"
                    >
                      
                        3.2.13
                        
                      
                      advanced-usage-of-recurrent-neural-networks
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/02deep-learning-with-python/64-sequence-processing-with-convnets.html"
                    >
                      
                        3.2.14
                        
                      
                      sequence-processing-with-convnets
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/02deep-learning-with-python/81-text-generation-with-lstm.html"
                    >
                      
                        3.2.15
                        
                      
                      text-generation-with-lstm
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/02deep-learning-with-python/82-deep-dream.html"
                    >
                      
                        3.2.16
                        
                      
                      deep-dream
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/02deep-learning-with-python/83-neural-style-transfer.html"
                    >
                      
                        3.2.17
                        
                      
                      neural-style-transfer
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/02deep-learning-with-python/84-generating-images-with-vaes.html"
                    >
                      
                        3.2.18
                        
                      
                      generating-images-with-vaes
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/02deep-learning-with-python/85-introduction-to-gans.html"
                    >
                      
                        3.2.19
                        
                      
                      introduction-to-gans
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/11ReadBook/03handbook-en/index.html"
                >
                  
                    3.3
                  
                  handbook-en
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/000-Preface.html"
                    >
                      
                        3.3.1
                        
                      
                      Preface
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/200-Introduction-to-NumPy.html"
                    >
                      
                        3.3.2
                        
                      
                      Introduction-to-NumPy
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/201-Understanding-Data-Types.html"
                    >
                      
                        3.3.3
                        
                      
                      Understanding-Data-Types
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/202-The-Basics-Of-NumPy-Arrays.html"
                    >
                      
                        3.3.4
                        
                      
                      The-Basics-Of-NumPy-Arrays
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/203-Computation-on-arrays-ufuncs.html"
                    >
                      
                        3.3.5
                        
                      
                      Computation-on-arrays-ufuncs
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/204-Computation-on-arrays-aggregates.html"
                    >
                      
                        3.3.6
                        
                      
                      Computation-on-arrays-aggregates
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/205-Computation-on-arrays-broadcasting.html"
                    >
                      
                        3.3.7
                        
                      
                      Computation-on-arrays-broadcasting
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/206-Boolean-Arrays-and-Masks.html"
                    >
                      
                        3.3.8
                        
                      
                      Boolean-Arrays-and-Masks
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/207-Fancy-Indexing.html"
                    >
                      
                        3.3.9
                        
                      
                      Fancy-Indexing
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/208-Sorting.html"
                    >
                      
                        3.3.10
                        
                      
                      Sorting
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/209-Structured-Data-NumPy.html"
                    >
                      
                        3.3.11
                        
                      
                      Structured-Data-NumPy
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/300-Introduction-to-Pandas.html"
                    >
                      
                        3.3.12
                        
                      
                      Introduction-to-Pandas
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/301-Introducing-Pandas-Objects.html"
                    >
                      
                        3.3.13
                        
                      
                      Introducing-Pandas-Objects
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/302-Data-Indexing-and-Selection.html"
                    >
                      
                        3.3.14
                        
                      
                      Data-Indexing-and-Selection
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/303-Operations-in-Pandas.html"
                    >
                      
                        3.3.15
                        
                      
                      Operations-in-Pandas
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/304-Missing-Values.html"
                    >
                      
                        3.3.16
                        
                      
                      Missing-Values
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/305-Hierarchical-Indexing.html"
                    >
                      
                        3.3.17
                        
                      
                      Hierarchical-Indexing
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/306-Concat-And-Append.html"
                    >
                      
                        3.3.18
                        
                      
                      Concat-And-Append
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/307-Merge-and-Join.html"
                    >
                      
                        3.3.19
                        
                      
                      Merge-and-Join
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/308-Aggregation-and-Grouping.html"
                    >
                      
                        3.3.20
                        
                      
                      Aggregation-and-Grouping
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/309-Pivot-Tables.html"
                    >
                      
                        3.3.21
                        
                      
                      Pivot-Tables
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/310-Working-With-Strings.html"
                    >
                      
                        3.3.22
                        
                      
                      Working-With-Strings
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/311-Working-with-Time-Series.html"
                    >
                      
                        3.3.23
                        
                      
                      Working-with-Time-Series
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/312-Performance-Eval-and-Query.html"
                    >
                      
                        3.3.24
                        
                      
                      Performance-Eval-and-Query
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/313-Further-Resources.html"
                    >
                      
                        3.3.25
                        
                      
                      Further-Resources
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/400-Introduction-To-Matplotlib.html"
                    >
                      
                        3.3.26
                        
                      
                      Introduction-To-Matplotlib
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/401-Simple-Line-Plots.html"
                    >
                      
                        3.3.27
                        
                      
                      Simple-Line-Plots
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/402-Simple-Scatter-Plots.html"
                    >
                      
                        3.3.28
                        
                      
                      Simple-Scatter-Plots
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/403-Errorbars.html"
                    >
                      
                        3.3.29
                        
                      
                      Errorbars
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/404-Density-and-Contour-Plots.html"
                    >
                      
                        3.3.30
                        
                      
                      Density-and-Contour-Plots
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/405-Histograms-and-Binnings.html"
                    >
                      
                        3.3.31
                        
                      
                      Histograms-and-Binnings
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/406-Customizing-Legends.html"
                    >
                      
                        3.3.32
                        
                      
                      Customizing-Legends
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/407-Customizing-Colorbars.html"
                    >
                      
                        3.3.33
                        
                      
                      Customizing-Colorbars
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/408-Multiple-Subplots.html"
                    >
                      
                        3.3.34
                        
                      
                      Multiple-Subplots
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/409-Text-and-Annotation.html"
                    >
                      
                        3.3.35
                        
                      
                      Text-and-Annotation
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/410-Customizing-Ticks.html"
                    >
                      
                        3.3.36
                        
                      
                      Customizing-Ticks
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/411-Settings-and-Stylesheets.html"
                    >
                      
                        3.3.37
                        
                      
                      Settings-and-Stylesheets
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/412-Three-Dimensional-Plotting.html"
                    >
                      
                        3.3.38
                        
                      
                      Three-Dimensional-Plotting
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/413-Geographic-Data-With-Basemap.html"
                    >
                      
                        3.3.39
                        
                      
                      Geographic-Data-With-Basemap
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/414-Visualization-With-Seaborn.html"
                    >
                      
                        3.3.40
                        
                      
                      Visualization-With-Seaborn
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/415-Further-Resources.html"
                    >
                      
                        3.3.41
                        
                      
                      Further-Resources
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/500-Machine-Learning.html"
                    >
                      
                        3.3.42
                        
                      
                      Machine-Learning
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/501-What-Is-Machine-Learning.html"
                    >
                      
                        3.3.43
                        
                      
                      What-Is-Machine-Learning
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/502-Introducing-Scikit-Learn.html"
                    >
                      
                        3.3.44
                        
                      
                      Introducing-Scikit-Learn
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/503-Hyperparameters-and-Model-Validation.html"
                    >
                      
                        3.3.45
                        
                      
                      Hyperparameters-and-Model-Validation
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/504-Feature-Engineering.html"
                    >
                      
                        3.3.46
                        
                      
                      Feature-Engineering
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/505-Naive-Bayes.html"
                    >
                      
                        3.3.47
                        
                      
                      Naive-Bayes
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/506-Linear-Regression.html"
                    >
                      
                        3.3.48
                        
                      
                      Linear-Regression
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/507-Support-Vector-Machines.html"
                    >
                      
                        3.3.49
                        
                      
                      Support-Vector-Machines
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/508-Random-Forests.html"
                    >
                      
                        3.3.50
                        
                      
                      Random-Forests
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/509-Principal-Component-Analysis.html"
                    >
                      
                        3.3.51
                        
                      
                      Principal-Component-Analysis
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/510-Manifold-Learning.html"
                    >
                      
                        3.3.52
                        
                      
                      Manifold-Learning
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/511-K-Means.html"
                    >
                      
                        3.3.53
                        
                      
                      K-Means
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/512-Gaussian-Mixtures.html"
                    >
                      
                        3.3.54
                        
                      
                      Gaussian-Mixtures
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/513-Kernel-Density-Estimation.html"
                    >
                      
                        3.3.55
                        
                      
                      Kernel-Density-Estimation
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/514-Image-Features.html"
                    >
                      
                        3.3.56
                        
                      
                      Image-Features
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/515-Learning-More.html"
                    >
                      
                        3.3.57
                        
                      
                      Learning-More
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/600-Figure-Code.html"
                    >
                      
                        3.3.58
                        
                      
                      Figure-Code
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/handbook-en-01.html"
                    >
                      
                        3.3.59
                        
                      
                      handbook-en-01
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/handbook-en-02.html"
                    >
                      
                        3.3.60
                        
                      
                      handbook-en-02
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/Index.html"
                    >
                      
                        3.3.61
                        
                      
                      Index
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/merge_3.html"
                    >
                      
                        3.3.62
                        
                      
                      merge_3
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/03handbook-en/merge_4.html"
                    >
                      
                        3.3.63
                        
                      
                      merge_4
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry c-sidebar__entry--active"
                      href="/blog/blog2/11ReadBook/03handbook-en/merge_5.html"
                    >
                      
                        3.3.64
                        
                      
                      merge_5
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/11ReadBook/04MachineLearningPractice/index.html"
                >
                  
                    3.4
                  
                  MachineLearningPractice
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/04MachineLearningPractice/00naive-bayes-discuss.html"
                    >
                      
                        3.4.1
                        
                      
                      naive-bayes-discuss
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/04MachineLearningPractice/01%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.html"
                    >
                      
                        3.4.2
                        
                      
                      机器学习基础
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/04MachineLearningPractice/02k-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95.html"
                    >
                      
                        3.4.3
                        
                      
                      k-近邻算法
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/04MachineLearningPractice/03%E5%86%B3%E7%AD%96%E6%A0%91.html"
                    >
                      
                        3.4.4
                        
                      
                      决策树
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/04MachineLearningPractice/04%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF.html"
                    >
                      
                        3.4.5
                        
                      
                      朴素贝叶斯
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/04MachineLearningPractice/05Logistic%E5%9B%9E%E5%BD%92.html"
                    >
                      
                        3.4.6
                        
                      
                      Logistic回归
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/04MachineLearningPractice/06%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.html"
                    >
                      
                        3.4.7
                        
                      
                      支持向量机
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/04MachineLearningPractice/07%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%92%8CAdaBoost.html"
                    >
                      
                        3.4.8
                        
                      
                      集成方法-随机森林和AdaBoost
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/04MachineLearningPractice/08%E9%A2%84%E6%B5%8B%E6%95%B0%E5%80%BC%E5%9E%8B%E6%95%B0%E6%8D%AE-%E5%9B%9E%E5%BD%92.html"
                    >
                      
                        3.4.9
                        
                      
                      预测数值型数据-回归
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/04MachineLearningPractice/09%E6%A0%91%E5%9B%9E%E5%BD%92.html"
                    >
                      
                        3.4.10
                        
                      
                      树回归
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/04MachineLearningPractice/10k-means%E8%81%9A%E7%B1%BB.html"
                    >
                      
                        3.4.11
                        
                      
                      k-means聚类
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/04MachineLearningPractice/11Apriori-%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90.html"
                    >
                      
                        3.4.12
                        
                      
                      Apriori-关联分析
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/04MachineLearningPractice/12FP-growth-%E9%A2%91%E7%B9%81%E9%A1%B9%E9%9B%86.html"
                    >
                      
                        3.4.13
                        
                      
                      FP-growth-频繁项集
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/04MachineLearningPractice/13%E5%88%A9%E7%94%A8PCA%E6%9D%A5%E7%AE%80%E5%8C%96%E6%95%B0%E6%8D%AE.html"
                    >
                      
                        3.4.14
                        
                      
                      利用PCA来简化数据
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/04MachineLearningPractice/14%E5%88%A9%E7%94%A8SVD%E7%AE%80%E5%8C%96%E6%95%B0%E6%8D%AE.html"
                    >
                      
                        3.4.15
                        
                      
                      利用SVD简化数据
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/04MachineLearningPractice/15%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8EMapReduce.html"
                    >
                      
                        3.4.16
                        
                      
                      大数据与MapReduce
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/04MachineLearningPractice/16%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.html"
                    >
                      
                        3.4.17
                        
                      
                      推荐系统
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/11ReadBook/05pydata-zh/index.html"
                >
                  
                    3.5
                  
                  pydata-zh
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/05pydata-zh/041%20%E5%A4%9A%E7%BB%B4%E6%95%B0%E7%BB%84%E5%AF%B9%E8%B1%A1.html"
                    >
                      
                        3.5.1
                        
                      
                      多维数组对象
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/05pydata-zh/042%20%E9%80%9A%E7%94%A8%E5%87%BD%E6%95%B0.html"
                    >
                      
                        3.5.2
                        
                      
                      通用函数
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/05pydata-zh/043%20%E6%95%B0%E7%BB%84%E5%AF%BC%E5%90%91%E7%BC%96%E7%A8%8B.html"
                    >
                      
                        3.5.3
                        
                      
                      数组导向编程
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/05pydata-zh/051%20pandas%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.html"
                    >
                      
                        3.5.4
                        
                      
                      pandas的数据结构
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/05pydata-zh/052%20pandas%E4%B8%BB%E8%A6%81%E5%8A%9F%E8%83%BD.html"
                    >
                      
                        3.5.5
                        
                      
                      pandas主要功能
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/05pydata-zh/053%20%E6%80%BB%E7%BB%93%E5%92%8C%E6%8F%8F%E8%BF%B0%E6%80%A7%E7%BB%9F%E8%AE%A1.html"
                    >
                      
                        3.5.6
                        
                      
                      总结和描述性统计
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/05pydata-zh/071%20%E5%A4%84%E7%90%86%E7%BC%BA%E5%A4%B1%E6%95%B0%E6%8D%AE.html"
                    >
                      
                        3.5.7
                        
                      
                      处理缺失数据
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/05pydata-zh/072%20%E6%95%B0%E6%8D%AE%E5%8F%98%E6%8D%A2.html"
                    >
                      
                        3.5.8
                        
                      
                      数据变换
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/05pydata-zh/073%20%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%A4%84%E7%90%86.html"
                    >
                      
                        3.5.9
                        
                      
                      字符串处理
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/05pydata-zh/111%20%E6%97%A5%E6%9C%9F%E5%92%8C%E6%97%B6%E9%97%B4%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%8F%8A%E5%85%B6%E5%B7%A5%E5%85%B7.html"
                    >
                      
                        3.5.10
                        
                      
                      日期和时间数据类型及其工具
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/05pydata-zh/112%20%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E5%9F%BA%E7%A1%80.html"
                    >
                      
                        3.5.11
                        
                      
                      时间序列基础
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/05pydata-zh/113%20%E6%97%A5%E6%9C%9F%E8%8C%83%E5%9B%B4,%E9%A2%91%E5%BA%A6,%E5%92%8C%E4%BD%8D%E7%A7%BB.html"
                    >
                      
                        3.5.12
                        
                      
                      日期范围，频度，和位移
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/05pydata-zh/121%20%E7%B1%BB%E5%88%AB%E6%95%B0%E6%8D%AE.html"
                    >
                      
                        3.5.13
                        
                      
                      类别数据
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/05pydata-zh/142%20MovieLens%201M%E6%95%B0%E6%8D%AE%E9%9B%86.html"
                    >
                      
                        3.5.14
                        
                      
                      MovieLens 1M数据集
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/11ReadBook/05pydata-zh/143%20%E4%BB%8E1880%E5%B9%B4%E8%87%B32010%E5%B9%B4%E7%BE%8E%E5%9B%BD%E5%A9%B4%E5%84%BF%E5%A7%93%E5%90%8D.html"
                    >
                      
                        3.5.15
                        
                      
                      从1880年至2010年美国婴儿姓名
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/11ReadBook/11%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E6%96%87%E7%89%88.html"
                >
                  
                    3.6
                  
                  深度学习中文版
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/11ReadBook/12spark-en%E6%B1%87%E6%80%BB.html"
                >
                  
                    3.7
                  
                  spark-en汇总
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/11ReadBook/51%E6%89%80%E8%B0%93%E6%83%85%E5%95%86%E9%AB%98,%E5%B0%B1%E6%98%AF%E4%BC%9A%E8%AF%B4%E8%AF%9D.html"
                >
                  
                    3.8
                  
                  所谓情商高，就是会说话
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/index.html"
        >
          
            4.
          
          常见算法
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/adaBoost/index.html"
                >
                  
                    4.1
                  
                  adaBoost
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/adaBoost/AdaBoost.html"
                    >
                      
                        4.1.1
                        
                      
                      AdaBoost
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/adaBoost/AdaBoost%E5%85%83%E7%AE%97%E6%B3%95%E6%8F%90%E9%AB%98%E5%88%86%E7%B1%BB%E6%80%A7%E8%83%BD.html"
                    >
                      
                        4.1.2
                        
                      
                      AdaBoost元算法提高分类性能
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/apriori/index.html"
                >
                  
                    4.2
                  
                  apriori
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/apriori/apriori.html"
                    >
                      
                        4.2.1
                        
                      
                      apriori
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/bayes/index.html"
                >
                  
                    4.3
                  
                  bayes
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/bayes/naiveBayes.html"
                    >
                      
                        4.3.1
                        
                      
                      naiveBayes
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/boost/index.html"
                >
                  
                    4.4
                  
                  boost
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/boost/Boost%E7%AE%80%E4%BB%8B.html"
                    >
                      
                        4.4.1
                        
                      
                      Boost简介
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/ES%E6%9F%A5%E8%AF%A2.html"
                >
                  
                    4.5
                  
                  ES查询
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/fp-growth/index.html"
                >
                  
                    4.6
                  
                  fp-growth
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/fp-growth/fp-growth.html"
                    >
                      
                        4.6.1
                        
                      
                      fp-growth
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/k-Nearest%20Neighbor/index.html"
                >
                  
                    4.7
                  
                  k-Nearest Neighbor
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/k-Nearest%20Neighbor/knn.html"
                    >
                      
                        4.7.1
                        
                      
                      knn
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/k-Nearest%20Neighbor/knn_%E5%8E%9F%E7%89%88.html"
                    >
                      
                        4.7.2
                        
                      
                      knn_原版
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/kmeans/index.html"
                >
                  
                    4.8
                  
                  kmeans
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/kmeans/kmeans.html"
                    >
                      
                        4.8.1
                        
                      
                      kmeans
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/line-regression/index.html"
                >
                  
                    4.9
                  
                  line-regression
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/line-regression/line_regression.html"
                    >
                      
                        4.9.1
                        
                      
                      line_regression
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/line-regression/LR.html"
                    >
                      
                        4.9.2
                        
                      
                      LR
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/logistic/index.html"
                >
                  
                    4.10
                  
                  logistic
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/logistic/logistic-code.html"
                    >
                      
                        4.10.1
                        
                      
                      logistic-code
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/logistic/logistic-%E7%90%86%E8%AE%BA.html"
                    >
                      
                        4.10.2
                        
                      
                      logistic-理论
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/logistic/temo.html"
                    >
                      
                        4.10.3
                        
                      
                      temo
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/pca/index.html"
                >
                  
                    4.11
                  
                  pca
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/pca/pca.html"
                    >
                      
                        4.11.1
                        
                      
                      pca
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/pca%E4%B8%8Esvd%E7%9A%84%E6%AF%94%E8%BE%83/index.html"
                >
                  
                    4.12
                  
                  pca与svd的比较
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/pca%E4%B8%8Esvd%E7%9A%84%E6%AF%94%E8%BE%83/pca.html"
                    >
                      
                        4.12.1
                        
                      
                      pca
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/README.html"
                >
                  
                    4.13
                  
                  README
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/regressionTree/index.html"
                >
                  
                    4.14
                  
                  regressionTree
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/regressionTree/CART.html"
                    >
                      
                        4.14.1
                        
                      
                      CART
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/svd/index.html"
                >
                  
                    4.15
                  
                  svd
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/svd/collaborative_filter.html"
                    >
                      
                        4.15.1
                        
                      
                      collaborative_filter
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/svd/svd.html"
                    >
                      
                        4.15.2
                        
                      
                      svd
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/svm/index.html"
                >
                  
                    4.16
                  
                  svm
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/svm/smo.html"
                    >
                      
                        4.16.1
                        
                      
                      smo
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/Untitled.html"
                >
                  
                    4.17
                  
                  Untitled
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/xgboost/index.html"
                >
                  
                    4.18
                  
                  xgboost
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/xgboost/xgboost.html"
                    >
                      
                        4.18.1
                        
                      
                      xgboost
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/xgboost/%E5%8D%81%E5%88%86%E9%92%9F%E4%B8%8A%E6%89%8Bxgboost.html"
                    >
                      
                        4.18.2
                        
                      
                      十分钟上手xgboost
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E5%86%B3%E7%AD%96%E6%A0%91(DecisionTree)/index.html"
                >
                  
                    4.19
                  
                  决策树(DecisionTree)
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E5%86%B3%E7%AD%96%E6%A0%91(DecisionTree)/decisionTree.html"
                    >
                      
                        4.19.1
                        
                      
                      decisionTree
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E5%86%B3%E7%AD%96%E6%A0%91(DecisionTree)/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%80%E4%BB%8B.html"
                    >
                      
                        4.19.2
                        
                      
                      决策树简介
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/14%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95/%E5%BB%BA%E6%A8%A1%E8%BF%87%E7%A8%8B.html"
                >
                  
                    4.20
                  
                  建模过程
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/index.html"
        >
          
            5.
          
          算法集锦
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/math/index.html"
                >
                  
                    5.1
                  
                  math
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/math/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90.html"
                    >
                      
                        5.1.1
                        
                      
                      主成分分析
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/math/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95.html"
                    >
                      
                        5.1.2
                        
                      
                      最小二乘法
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/math/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC.html"
                    >
                      
                        5.1.3
                        
                      
                      矩阵求导
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/index.html"
                >
                  
                    5.2
                  
                  推荐算法
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93.html"
                    >
                      
                        5.2.1
                        
                      
                      协同过滤推荐算法总结
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3.html"
                    >
                      
                        5.2.2
                        
                      
                      矩阵分解
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/index.html"
                >
                  
                    5.3
                  
                  推荐系统
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/test.html"
                    >
                      
                        5.3.1
                        
                      
                      test
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98/index.html"
                >
                  
                    5.4
                  
                  文本挖掘
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98/LDA%E6%A8%A1%E5%9E%8B.html"
                    >
                      
                        5.4.1
                        
                      
                      LDA模型
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98/Word2Vec.html"
                    >
                      
                        5.4.2
                        
                      
                      Word2Vec
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98/%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%E4%B9%8B%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E7%B4%A2%E5%BC%95(LSI).html"
                    >
                      
                        5.4.3
                        
                      
                      文本主题模型之潜在语义索引(LSI)
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E9%A2%84%E5%A4%84%E7%90%86%E4%B9%8BTF-IDF.html"
                    >
                      
                        5.4.4
                        
                      
                      文本挖掘预处理之TF-IDF
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8BHMM.html"
                    >
                      
                        5.4.5
                        
                      
                      隐马尔科夫模型HMM
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.html"
                >
                  
                    5.5
                  
                  深度学习
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/LSTM.html"
                    >
                      
                        5.5.1
                        
                      
                      LSTM
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/RNN.html"
                    >
                      
                        5.5.2
                        
                      
                      RNN
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Seq2Seq.html"
                    >
                      
                        5.5.3
                        
                      
                      Seq2Seq
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/TensorFlow-%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%A4%84%E7%90%86.html"
                    >
                      
                        5.5.4
                        
                      
                      TensorFlow-字符串处理
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/tensorflow%E5%AD%A6%E4%B9%A0.html"
                    >
                      
                        5.5.5
                        
                      
                      tensorflow学习
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/TensorFlow%E5%B8%B8%E8%A7%81%E4%BB%A3%E7%A0%81%E5%9D%97.html"
                    >
                      
                        5.5.6
                        
                      
                      TensorFlow常见代码块
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/tf%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95.html"
                    >
                      
                        5.5.7
                        
                      
                      tf优化方法
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/tf%E5%87%BD%E6%95%B0.html"
                    >
                      
                        5.5.8
                        
                      
                      tf函数
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html"
                    >
                      
                        5.5.9
                        
                      
                      人工神经网络
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html"
                >
                  
                    5.6
                  
                  神经网络
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/AutoEncoder-%E8%87%AA%E7%BC%96%E7%A0%81%E5%AE%9E%E8%B7%B5.html"
                    >
                      
                        5.6.1
                        
                      
                      AutoEncoder-自编码实践
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/AutoEncoder-%E8%87%AA%E7%BC%96%E7%A0%81%E7%90%86%E8%AE%BA.html"
                    >
                      
                        5.6.2
                        
                      
                      AutoEncoder-自编码理论
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%90%86%E8%AE%BA.html"
                    >
                      
                        5.6.3
                        
                      
                      CNN-卷积神经网络理论
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/DNN-%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html"
                    >
                      
                        5.6.4
                        
                      
                      DNN-深度神经网络
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/LSTM-%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C%E7%90%86%E8%AE%BA.html"
                    >
                      
                        5.6.5
                        
                      
                      LSTM-长短期记忆网络理论
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/PCA-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90.html"
                    >
                      
                        5.6.6
                        
                      
                      PCA-主成分分析
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/RBM-%E5%8F%97%E9%99%90%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E6%9C%BA%E5%AE%9E%E8%B7%B5.html"
                    >
                      
                        5.6.7
                        
                      
                      RBM-受限玻尔兹曼机实践
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/RBM-%E5%8F%97%E9%99%90%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E6%9C%BA%E7%90%86%E8%AE%BA.html"
                    >
                      
                        5.6.8
                        
                      
                      RBM-受限玻尔兹曼机理论
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/RNN-%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%90%86%E8%AE%BA.html"
                    >
                      
                        5.6.9
                        
                      
                      RNN-递归神经网络理论
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%90%91%E9%87%8F%E9%99%8D%E7%BB%B4.html"
                    >
                      
                        5.6.10
                        
                      
                      向量降维
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%B8%B8%E8%A7%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%B1%87%E6%80%BB.html"
                    >
                      
                        5.6.11
                        
                      
                      常见损失函数汇总
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/14%E7%AE%97%E6%B3%95%E9%9B%86%E9%94%A6/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%B8%B8%E8%A7%81%E6%BF%80%E5%8A%B1%E5%87%BD%E6%95%B0%E6%B1%87%E6%80%BB.html"
                    >
                      
                        5.6.12
                        
                      
                      常见激励函数汇总
                    </a>
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/blog/blog2/15tools/index.html"
        >
          
            6.
          
          tools
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/15tools/colabHub/index.html"
                >
                  
                    6.1
                  
                  colabHub
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/15tools/colabHub/README.html"
                    >
                      
                        6.1.1
                        
                      
                      README
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/15tools/colabHub/%E5%88%9D%E5%A7%8B%E5%8C%96.html"
                    >
                      
                        6.1.2
                        
                      
                      初始化
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/15tools/DownLoadPicture.html"
                >
                  
                    6.2
                  
                  DownLoadPicture
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/15tools/latex/index.html"
                >
                  
                    6.3
                  
                  latex
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/15tools/latex/LaTeX%E5%85%AC%E5%BC%8F%E5%AE%9E%E4%BE%8B.html"
                    >
                      
                        6.3.1
                        
                      
                      LaTeX公式实例
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/15tools/latex/latex%E5%85%AC%E5%BC%8F%E7%BC%96%E5%8F%B7.html"
                    >
                      
                        6.3.2
                        
                      
                      latex公式编号
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/15tools/latex/%E5%8D%81%E5%88%86%E9%92%9F%E4%B8%8A%E6%89%8BLaTeX.html"
                    >
                      
                        6.3.3
                        
                      
                      十分钟上手LaTeX
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/15tools/numpy/index.html"
                >
                  
                    6.4
                  
                  numpy
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/15tools/numpy/numpy%E6%95%99%E7%A8%8B00.html"
                    >
                      
                        6.4.1
                        
                      
                      numpy教程00
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/15tools/numpy/numpy%E6%95%99%E7%A8%8B01.html"
                    >
                      
                        6.4.2
                        
                      
                      numpy教程01
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/15tools/numpy/numpy%E6%95%99%E7%A8%8B02.html"
                    >
                      
                        6.4.3
                        
                      
                      numpy教程02
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/15tools/numpy/numpy%E6%95%99%E7%A8%8B03.html"
                    >
                      
                        6.4.4
                        
                      
                      numpy教程03
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/15tools/pandas/index.html"
                >
                  
                    6.5
                  
                  pandas
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/15tools/pandas/pandas.html"
                    >
                      
                        6.5.1
                        
                      
                      pandas
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/15tools/pandas/pandas%E5%87%BD%E6%95%B0%E6%B7%B1%E5%85%A5.html"
                    >
                      
                        6.5.2
                        
                      
                      pandas函数深入
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/15tools/pandas/pandas%E6%95%99%E7%A8%8B01.html"
                    >
                      
                        6.5.3
                        
                      
                      pandas教程01
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/15tools/pandas/pandas%E6%95%99%E7%A8%8B02.html"
                    >
                      
                        6.5.4
                        
                      
                      pandas教程02
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/15tools/pandas/pandas%E6%95%99%E7%A8%8B03.html"
                    >
                      
                        6.5.5
                        
                      
                      pandas教程03
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/15tools/pandas/%E5%8D%81%E5%88%86%E9%92%9F%E4%B8%8A%E6%89%8BPandas.html"
                    >
                      
                        6.5.6
                        
                      
                      十分钟上手Pandas
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/15tools/pandas/%E5%8D%81%E5%88%86%E9%92%9F%E4%B8%8A%E6%89%8BPandas%E5%87%BD%E6%95%B0.html"
                    >
                      
                        6.5.7
                        
                      
                      十分钟上手Pandas函数
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/15tools/%E7%94%BB%E5%9B%BE/index.html"
                >
                  
                    6.6
                  
                  画图
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/15tools/%E7%94%BB%E5%9B%BE/python%E7%94%BB%E5%9B%BE%E4%B9%8Bfolium%E5%9C%B0%E5%9B%BE.html"
                    >
                      
                        6.6.1
                        
                      
                      python画图之folium地图
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/15tools/%E7%94%BB%E5%9B%BE/python%E7%94%BB%E5%9B%BE%E4%B9%8Bpycharts-3D%E5%9B%BE.html"
                    >
                      
                        6.6.2
                        
                      
                      python画图之pycharts-3D图
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/15tools/%E7%94%BB%E5%9B%BE/python%E7%94%BB%E5%9B%BE%E4%B9%8Bpycharts-kline.html"
                    >
                      
                        6.6.3
                        
                      
                      python画图之pycharts-kline
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/15tools/%E7%94%BB%E5%9B%BE/python%E7%94%BB%E5%9B%BE%E4%B9%8Bpycharts-%E5%85%B6%E4%BB%96.html"
                    >
                      
                        6.6.4
                        
                      
                      python画图之pycharts-其他
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/15tools/%E7%94%BB%E5%9B%BE/python%E7%94%BB%E5%9B%BE%E4%B9%8Bpycharts-%E5%85%B6%E4%BB%96%E5%9D%90%E6%A0%87%E7%B3%BB.html"
                    >
                      
                        6.6.5
                        
                      
                      python画图之pycharts-其他坐标系
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/15tools/%E7%94%BB%E5%9B%BE/python%E7%94%BB%E5%9B%BE%E4%B9%8Bpycharts-%E5%9C%B0%E5%9B%BE.html"
                    >
                      
                        6.6.6
                        
                      
                      python画图之pycharts-地图
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/15tools/%E7%94%BB%E5%9B%BE/python%E7%94%BB%E5%9B%BE%E4%B9%8Bpycharts-%E7%9B%B4%E8%A7%92%E5%9D%90%E6%A0%87%E7%B3%BB.html"
                    >
                      
                        6.6.7
                        
                      
                      python画图之pycharts-直角坐标系
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/15tools/%E7%94%BB%E5%9B%BE/python%E7%94%BB%E5%9B%BE%E4%B9%8Bpycharts-%E7%AE%80%E4%BB%8B.html"
                    >
                      
                        6.6.8
                        
                      
                      python画图之pycharts-简介
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/15tools/%E7%94%BB%E5%9B%BE/Untitled.html"
                    >
                      
                        6.6.9
                        
                      
                      Untitled
                    </a>
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/blog/blog2/99Example/index.html"
        >
          
            7.
          
          Example
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/99Example/4/index.html"
                >
                  
                    7.1
                  
                  
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/99Example/4/computational-tools.html"
                    >
                      
                        7.1.1
                        
                      
                      computational-tools
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/99Example/4/endnote.html"
                    >
                      
                        7.1.2
                        
                      
                      endnote
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/99Example/4/establishing-causality.html"
                    >
                      
                        7.1.3
                        
                      
                      establishing-causality
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/99Example/4/intro.html"
                    >
                      
                        7.1.4
                        
                      
                      intro
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/99Example/4/observation-and-visualization-john-snow-and-the-broad-street-pump.html"
                    >
                      
                        7.1.5
                        
                      
                      observation-and-visualization-john-snow-and-the-broad-street-pump
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/99Example/4/Plotting_the_Classics.html"
                    >
                      
                        7.1.6
                        
                      
                      Plotting_the_Classics
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/99Example/4/randomization.html"
                    >
                      
                        7.1.7
                        
                      
                      randomization
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/99Example/4/snow-s-grand-experiment.html"
                    >
                      
                        7.1.8
                        
                      
                      snow-s-grand-experiment
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/99Example/4/subsections.html"
                    >
                      
                        7.1.9
                        
                      
                      subsections
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/99Example/4/subsectiontwo.html"
                    >
                      
                        7.1.10
                        
                      
                      subsectiontwo
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/99Example/4/why-data-science.html"
                    >
                      
                        7.1.11
                        
                      
                      why-data-science
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/99Example/Calls.html"
                >
                  
                    7.2
                  
                  Calls
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/99Example/causality-and-experiments.html"
                >
                  
                    7.3
                  
                  causality-and-experiments
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/99Example/Expressions.html"
                >
                  
                    7.4
                  
                  Expressions
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/99Example/features/index.html"
                >
                  
                    7.5
                  
                  features
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/99Example/features/citations.html"
                    >
                      
                        7.5.1
                        
                      
                      citations
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/99Example/features/features.html"
                    >
                      
                        7.5.2
                        
                      
                      features
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/99Example/features/hiding.html"
                    >
                      
                        7.5.3
                        
                      
                      hiding
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/99Example/features/interact.html"
                    >
                      
                        7.5.4
                        
                      
                      interact
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/99Example/features/interactive_cells.html"
                    >
                      
                        7.5.5
                        
                      
                      interactive_cells
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/99Example/features/limits.html"
                    >
                      
                        7.5.6
                        
                      
                      limits
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/99Example/features/markdown.html"
                    >
                      
                        7.5.7
                        
                      
                      markdown
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/99Example/features/notebooks.html"
                    >
                      
                        7.5.8
                        
                      
                      notebooks
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/99Example/features/search.html"
                    >
                      
                        7.5.9
                        
                      
                      search
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/99Example/Growth.html"
                >
                  
                    7.6
                  
                  Growth
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/99Example/guide/index.html"
                >
                  
                    7.7
                  
                  guide
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/99Example/guide/01_overview.html"
                    >
                      
                        7.7.1
                        
                      
                      _overview
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/99Example/guide/02_create.html"
                    >
                      
                        7.7.2
                        
                      
                      _create
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/99Example/guide/03_build.html"
                    >
                      
                        7.7.3
                        
                      
                      _build
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/99Example/guide/04_faq.html"
                    >
                      
                        7.7.4
                        
                      
                      _faq
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/99Example/guide/05_advanced.html"
                    >
                      
                        7.7.5
                        
                      
                      _advanced
                    </a>
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/99Example/Introduction_to_Tables.html"
                >
                  
                    7.8
                  
                  Introduction_to_Tables
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/99Example/Names.html"
                >
                  
                    7.9
                  
                  Names
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/99Example/Numbers.html"
                >
                  
                    7.10
                  
                  Numbers
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/99Example/programming-in-python.html"
                >
                  
                    7.11
                  
                  programming-in-python
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/99Example/statistical-techniques.html"
                >
                  
                    7.12
                  
                  statistical-techniques
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/99Example/Strings.html"
                >
                  
                    7.13
                  
                  Strings
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/99Example/Types.html"
                >
                  
                    7.14
                  
                  Types
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/99Example/what-is-data-science.html"
                >
                  
                    7.15
                  
                  what-is-data-science
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/blog/blog2/notebooks/index.html"
        >
          
            8.
          
          notebooks
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/notebooks/JupyterNotebookTips.html"
                >
                  
                    8.1
                  
                  JupyterNotebookTips
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/notebooks/LinearRegression.html"
                >
                  
                    8.2
                  
                  LinearRegression
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/blog/blog2/notechats/index.html"
        >
          
            9.
          
          notechats
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/notechats/notedata%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE.html"
                >
                  
                    9.1
                  
                  notedata测试数据
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/blog/blog2/package/index.html"
        >
          
            10.
          
          package
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/package/folium.html"
                >
                  
                    10.1
                  
                  folium
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/package/graphviz%E4%B9%8B%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8.html"
                >
                  
                    10.2
                  
                  graphviz之安装使用
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/package/pygithub.html"
                >
                  
                    10.3
                  
                  pygithub
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/package/word2vec.html"
                >
                  
                    10.4
                  
                  word2vec
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/package/%E5%90%91%E9%87%8F%E5%BC%95%E6%93%8Efaiss.html"
                >
                  
                    10.5
                  
                  向量引擎faiss
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/package/%E8%AF%8D%E5%90%91%E9%87%8F%E8%AE%AD%E7%BB%83.html"
                >
                  
                    10.6
                  
                  词向量训练
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/blog/blog2/papers/index.html"
        >
          
            11.
          
          papers
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/blog/blog2/papers/atrank/index.html"
                >
                  
                    11.1
                  
                  atrank
                </a>

                
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/papers/atrank/Atrank.html"
                    >
                      
                        11.1.1
                        
                      
                      Atrank
                    </a>
                
                  
                  

                  <li class="c-sidebar__subsection">
                    <a class="c-sidebar__entry "
                      href="/blog/blog2/papers/atrank/Attention.html"
                    >
                      
                        11.1.2
                        
                      
                      Attention
                    </a>
                

              </li>
              
            
          </ul>
        
      </li>

      
    
  </ul>
  <p class="sidebar_footer">Powered by <a href="https://github.com/jupyter/jupyter-book">Jupyter Book</a></p>
</nav>

    
    <!-- Empty sidebar placeholder that we'll auto-fill with javascript -->
    <aside class="sidebar__right">
        <header><h4 class="nav__title"><i class="fa fa-list"></i>   On this page</h4></header>
        <nav class="onthispage">
        </nav>
    </aside>
    
    <main class="c-textbook__page" tabindex="-1">
        <div class="o-wrapper">
            <div class="c-sidebar-toggle">
  <!-- We show the sidebar by default so we use .is-active -->
  <button
    id="js-sidebar-toggle"
    class="hamburger hamburger--arrowalt is-active"
  >
    <span class="hamburger-box">
      <span class="hamburger-inner"></span>
    </span>
    <span class="c-sidebar-toggle__label">Toggle Sidebar</span>
  </button>
</div>

            
<div class="buttons">
<a href="/blog/content/11ReadBook/03handbook-en/merge_5.ipynb" download>
<button id="interact-button-download" class="interact-button">Download</button>
</a>

<button id="interact-button-thebelab" class="interact-button">Thebelab</button>









<a href="https://mybinder.org/v2/gh/jupyter/jupyter-book/gh-pages?filepath=..%2Fcontent%2F11ReadBook%2F03handbook-en%2Fmerge_5.ipynb"><button class="interact-button" id="interact-button-binder"><img class="interact-button-logo" src="/blog/blog2/assets/images/logo_binder.svg" alt="Interact" />Interact</button></a>


</div>


            <div class="c-textbook__content">
                <!--BOOK_INFORMATION-->
<p><img align="left" style="padding-right:10px;" src="figures/PDSH-cover-small.png" /></p>

<p><em>This notebook contains an excerpt from the <a href="http://shop.oreilly.com/product/0636920034919.do">Python Data Science Handbook</a> by Jake VanderPlas; the content is available <a href="https://github.com/jakevdp/PythonDataScienceHandbook">on GitHub</a>.</em></p>

<p><em>The text is released under the <a href="https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode">CC-BY-NC-ND license</a>, and code is released under the <a href="https://opensource.org/licenses/MIT">MIT license</a>. If you find this content useful, please consider supporting the work by <a href="http://shop.oreilly.com/product/0636920034919.do">buying the book</a>!</em></p>

<!--NAVIGATION-->
<p>&lt; <a href="04.15-Further-Resources.ipynb">Further Resources</a> | <a href="Index.ipynb">Contents</a> | <a href="05.01-What-Is-Machine-Learning.ipynb">What Is Machine Learning?</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.00-Machine-Learning.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<h1 id="machine-learning">Machine Learning</h1>

<p>In many ways, machine learning is the primary means by which data science manifests itself to the broader world.
Machine learning is where these computational and algorithmic skills of data science meet the statistical thinking of data science, and the result is a collection of approaches to inference and data exploration that are not about effective theory so much as effective computation.</p>

<p>The term “machine learning” is sometimes thrown around as if it is some kind of magic pill: <em>apply machine learning to your data, and all your problems will be solved!</em>
As you might expect, the reality is rarely this simple.
While these methods can be incredibly powerful, to be effective they must be approached with a firm grasp of the strengths and weaknesses of each method, as well as a grasp of general concepts such as bias and variance, overfitting and underfitting, and more.</p>

<p>This chapter will dive into practical aspects of machine learning, primarily using Python’s <a href="http://scikit-learn.org">Scikit-Learn</a> package.
This is not meant to be a comprehensive introduction to the field of machine learning; that is a large subject and necessitates a more technical approach than we take here.
Nor is it meant to be a comprehensive manual for the use of the Scikit-Learn package (for this, you can refer to the resources listed in <a href="05.15-Learning-More.ipynb">Further Machine Learning Resources</a>).
Rather, the goals of this chapter are:</p>

<ul>
  <li>To introduce the fundamental vocabulary and concepts of machine learning.</li>
  <li>To introduce the Scikit-Learn API and show some examples of its use.</li>
  <li>To take a deeper dive into the details of several of the most important machine learning approaches, and develop an intuition into how they work and when and where they are applicable.</li>
</ul>

<p>Much of this material is drawn from the Scikit-Learn tutorials and workshops I have given on several occasions at PyCon, SciPy, PyData, and other conferences.
Any clarity in the following pages is likely due to the many workshop participants and co-instructors who have given me valuable feedback on this material over the years!</p>

<p>Finally, if you are seeking a more comprehensive or technical treatment of any of these subjects, I’ve listed several resources and references in <a href="05.15-Learning-More.ipynb">Further Machine Learning Resources</a>.</p>

<!--NAVIGATION-->
<p>&lt; <a href="04.15-Further-Resources.ipynb">Further Resources</a> | <a href="Index.ipynb">Contents</a> | <a href="05.01-What-Is-Machine-Learning.ipynb">What Is Machine Learning?</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.00-Machine-Learning.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<!--BOOK_INFORMATION-->
<p><img align="left" style="padding-right:10px;" src="figures/PDSH-cover-small.png" /></p>

<p><em>This notebook contains an excerpt from the <a href="http://shop.oreilly.com/product/0636920034919.do">Python Data Science Handbook</a> by Jake VanderPlas; the content is available <a href="https://github.com/jakevdp/PythonDataScienceHandbook">on GitHub</a>.</em></p>

<p><em>The text is released under the <a href="https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode">CC-BY-NC-ND license</a>, and code is released under the <a href="https://opensource.org/licenses/MIT">MIT license</a>. If you find this content useful, please consider supporting the work by <a href="http://shop.oreilly.com/product/0636920034919.do">buying the book</a>!</em></p>

<!--NAVIGATION-->
<p>&lt; <a href="05.00-Machine-Learning.ipynb">Machine Learning</a> | <a href="Index.ipynb">Contents</a> | <a href="05.02-Introducing-Scikit-Learn.ipynb">Introducing Scikit-Learn</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.01-What-Is-Machine-Learning.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<h1 id="what-is-machine-learning">What Is Machine Learning?</h1>

<p>Before we take a look at the details of various machine learning methods, let’s start by looking at what machine learning is, and what it isn’t.
Machine learning is often categorized as a subfield of artificial intelligence, but I find that categorization can often be misleading at first brush.
The study of machine learning certainly arose from research in this context, but in the data science application of machine learning methods, it’s more helpful to think of machine learning as a means of <em>building models of data</em>.</p>

<p>Fundamentally, machine learning involves building mathematical models to help understand data.
“Learning” enters the fray when we give these models <em>tunable parameters</em> that can be adapted to observed data; in this way the program can be considered to be “learning” from the data.
Once these models have been fit to previously seen data, they can be used to predict and understand aspects of newly observed data.
I’ll leave to the reader the more philosophical digression regarding the extent to which this type of mathematical, model-based “learning” is similar to the “learning” exhibited by the human brain.</p>

<p>Understanding the problem setting in machine learning is essential to using these tools effectively, and so we will start with some broad categorizations of the types of approaches we’ll discuss here.</p>

<h2 id="categories-of-machine-learning">Categories of Machine Learning</h2>

<p>At the most fundamental level, machine learning can be categorized into two main types: supervised learning and unsupervised learning.</p>

<p><em>Supervised learning</em> involves somehow modeling the relationship between measured features of data and some label associated with the data; once this model is determined, it can be used to apply labels to new, unknown data.
This is further subdivided into <em>classification</em> tasks and <em>regression</em> tasks: in classification, the labels are discrete categories, while in regression, the labels are continuous quantities.
We will see examples of both types of supervised learning in the following section.</p>

<p><em>Unsupervised learning</em> involves modeling the features of a dataset without reference to any label, and is often described as “letting the dataset speak for itself.”
These models include tasks such as <em>clustering</em> and <em>dimensionality reduction.</em>
Clustering algorithms identify distinct groups of data, while dimensionality reduction algorithms search for more succinct representations of the data.
We will see examples of both types of unsupervised learning in the following section.</p>

<p>In addition, there are so-called <em>semi-supervised learning</em> methods, which falls somewhere between supervised learning and unsupervised learning.
Semi-supervised learning methods are often useful when only incomplete labels are available.</p>

<h2 id="qualitative-examples-of-machine-learning-applications">Qualitative Examples of Machine Learning Applications</h2>

<p>To make these ideas more concrete, let’s take a look at a few very simple examples of a machine learning task.
These examples are meant to give an intuitive, non-quantitative overview of the types of machine learning tasks we will be looking at in this chapter.
In later sections, we will go into more depth regarding the particular models and how they are used.
For a preview of these more technical aspects, you can find the Python source that generates the following figures in the <a href="06.00-Figure-Code.ipynb">Appendix: Figure Code</a>.</p>

<h3 id="classification-predicting-discrete-labels">Classification: Predicting discrete labels</h3>

<p>We will first take a look at a simple <em>classification</em> task, in which you are given a set of labeled points and want to use these to classify some unlabeled points.</p>

<p>Imagine that we have the data shown in this figure:</p>

<p><img src="figures/05.01-classification-1.png" alt="" />
<a href="06.00-Figure-Code.ipynb#Classification-Example-Figure-1">figure source in Appendix</a></p>

<p>Here we have two-dimensional data: that is, we have two <em>features</em> for each point, represented by the <em>(x,y)</em> positions of the points on the plane.
In addition, we have one of two <em>class labels</em> for each point, here represented by the colors of the points.
From these features and labels, we would like to create a model that will let us decide whether a new point should be labeled “blue” or “red.”</p>

<p>There are a number of possible models for such a classification task, but here we will use an extremely simple one. We will make the assumption that the two groups can be separated by drawing a straight line through the plane between them, such that points on each side of the line fall in the same group.
Here the <em>model</em> is a quantitative version of the statement “a straight line separates the classes”, while the <em>model parameters</em> are the particular numbers describing the location and orientation of that line for our data.
The optimal values for these model parameters are learned from the data (this is the “learning” in machine learning), which is often called <em>training the model</em>.</p>

<p>The following figure shows a visual representation of what the trained model looks like for this data:</p>

<p><img src="figures/05.01-classification-2.png" alt="" />
<a href="06.00-Figure-Code.ipynb#Classification-Example-Figure-2">figure source in Appendix</a></p>

<p>Now that this model has been trained, it can be generalized to new, unlabeled data.
In other words, we can take a new set of data, draw this model line through it, and assign labels to the new points based on this model.
This stage is usually called <em>prediction</em>. See the following figure:</p>

<p><img src="figures/05.01-classification-3.png" alt="" />
<a href="06.00-Figure-Code.ipynb#Classification-Example-Figure-3">figure source in Appendix</a></p>

<p>This is the basic idea of a classification task in machine learning, where “classification” indicates that the data has discrete class labels.
At first glance this may look fairly trivial: it would be relatively easy to simply look at this data and draw such a discriminatory line to accomplish this classification.
A benefit of the machine learning approach, however, is that it can generalize to much larger datasets in many more dimensions.</p>

<p>For example, this is similar to the task of automated spam detection for email; in this case, we might use the following features and labels:</p>

<ul>
  <li><em>feature 1</em>, <em>feature 2</em>, etc. $\to$ normalized counts of important words or phrases (“Viagra”, “Nigerian prince”, etc.)</li>
  <li><em>label</em> $\to$ “spam” or “not spam”</li>
</ul>

<p>For the training set, these labels might be determined by individual inspection of a small representative sample of emails; for the remaining emails, the label would be determined using the model.
For a suitably trained classification algorithm with enough well-constructed features (typically thousands or millions of words or phrases), this type of approach can be very effective.
We will see an example of such text-based classification in <a href="05.05-Naive-Bayes.ipynb">In Depth: Naive Bayes Classification</a>.</p>

<p>Some important classification algorithms that we will discuss in more detail are Gaussian naive Bayes (see <a href="05.05-Naive-Bayes.ipynb">In Depth: Naive Bayes Classification</a>), support vector machines (see <a href="05.07-Support-Vector-Machines.ipynb">In-Depth: Support Vector Machines</a>), and random forest classification (see <a href="05.08-Random-Forests.ipynb">In-Depth: Decision Trees and Random Forests</a>).</p>

<h3 id="regression-predicting-continuous-labels">Regression: Predicting continuous labels</h3>

<p>In contrast with the discrete labels of a classification algorithm, we will next look at a simple <em>regression</em> task in which the labels are continuous quantities.</p>

<p>Consider the data shown in the following figure, which consists of a set of points each with a continuous label:</p>

<p><img src="figures/05.01-regression-1.png" alt="" />
<a href="06.00-Figure-Code.ipynb#Regression-Example-Figure-1">figure source in Appendix</a></p>

<p>As with the classification example, we have two-dimensional data: that is, there are two features describing each data point.
The color of each point represents the continuous label for that point.</p>

<p>There are a number of possible regression models we might use for this type of data, but here we will use a simple linear regression to predict the points.
This simple linear regression model assumes that if we treat the label as a third spatial dimension, we can fit a plane to the data.
This is a higher-level generalization of the well-known problem of fitting a line to data with two coordinates.</p>

<p>We can visualize this setup as shown in the following figure:</p>

<p><img src="figures/05.01-regression-2.png" alt="" />
<a href="06.00-Figure-Code.ipynb#Regression-Example-Figure-2">figure source in Appendix</a></p>

<p>Notice that the <em>feature 1-feature 2</em> plane here is the same as in the two-dimensional plot from before; in this case, however, we have represented the labels by both color and three-dimensional axis position.
From this view, it seems reasonable that fitting a plane through this three-dimensional data would allow us to predict the expected label for any set of input parameters.
Returning to the two-dimensional projection, when we fit such a plane we get the result shown in the following figure:</p>

<p><img src="figures/05.01-regression-3.png" alt="" />
<a href="06.00-Figure-Code.ipynb#Regression-Example-Figure-3">figure source in Appendix</a></p>

<p>This plane of fit gives us what we need to predict labels for new points.
Visually, we find the results shown in the following figure:</p>

<p><img src="figures/05.01-regression-4.png" alt="" />
<a href="06.00-Figure-Code.ipynb#Regression-Example-Figure-4">figure source in Appendix</a></p>

<p>As with the classification example, this may seem rather trivial in a low number of dimensions.
But the power of these methods is that they can be straightforwardly applied and evaluated in the case of data with many, many features.</p>

<p>For example, this is similar to the task of computing the distance to galaxies observed through a telescope—in this case, we might use the following features and labels:</p>

<ul>
  <li><em>feature 1</em>, <em>feature 2</em>, etc. $\to$ brightness of each galaxy at one of several wave lengths or colors</li>
  <li><em>label</em> $\to$ distance or redshift of the galaxy</li>
</ul>

<p>The distances for a small number of these galaxies might be determined through an independent set of (typically more expensive) observations.
Distances to remaining galaxies could then be estimated using a suitable regression model, without the need to employ the more expensive observation across the entire set.
In astronomy circles, this is known as the “photometric redshift” problem.</p>

<p>Some important regression algorithms that we will discuss are linear regression (see <a href="05.06-Linear-Regression.ipynb">In Depth: Linear Regression</a>), support vector machines (see <a href="05.07-Support-Vector-Machines.ipynb">In-Depth: Support Vector Machines</a>), and random forest regression (see <a href="05.08-Random-Forests.ipynb">In-Depth: Decision Trees and Random Forests</a>).</p>

<h3 id="clustering-inferring-labels-on-unlabeled-data">Clustering: Inferring labels on unlabeled data</h3>

<p>The classification and regression illustrations we just looked at are examples of supervised learning algorithms, in which we are trying to build a model that will predict labels for new data.
Unsupervised learning involves models that describe data without reference to any known labels.</p>

<p>One common case of unsupervised learning is “clustering,” in which data is automatically assigned to some number of discrete groups.
For example, we might have some two-dimensional data like that shown in the following figure:</p>

<p><img src="figures/05.01-clustering-1.png" alt="" />
<a href="06.00-Figure-Code.ipynb#Clustering-Example-Figure-2">figure source in Appendix</a></p>

<p>By eye, it is clear that each of these points is part of a distinct group.
Given this input, a clustering model will use the intrinsic structure of the data to determine which points are related.
Using the very fast and intuitive <em>k</em>-means algorithm (see <a href="05.11-K-Means.ipynb">In Depth: K-Means Clustering</a>), we find the clusters shown in the following figure:</p>

<p><img src="figures/05.01-clustering-2.png" alt="" />
<a href="06.00-Figure-Code.ipynb#Clustering-Example-Figure-2">figure source in Appendix</a></p>

<p><em>k</em>-means fits a model consisting of <em>k</em> cluster centers; the optimal centers are assumed to be those that minimize the distance of each point from its assigned center.
Again, this might seem like a trivial exercise in two dimensions, but as our data becomes larger and more complex, such clustering algorithms can be employed to extract useful information from the dataset.</p>

<p>We will discuss the <em>k</em>-means algorithm in more depth in <a href="05.11-K-Means.ipynb">In Depth: K-Means Clustering</a>.
Other important clustering algorithms include Gaussian mixture models (See <a href="05.12-Gaussian-Mixtures.ipynb">In Depth: Gaussian Mixture Models</a>) and spectral clustering (See <a href="http://scikit-learn.org/stable/modules/clustering.html">Scikit-Learn’s clustering documentation</a>).</p>

<h3 id="dimensionality-reduction-inferring-structure-of-unlabeled-data">Dimensionality reduction: Inferring structure of unlabeled data</h3>

<p>Dimensionality reduction is another example of an unsupervised algorithm, in which labels or other information are inferred from the structure of the dataset itself.
Dimensionality reduction is a bit more abstract than the examples we looked at before, but generally it seeks to pull out some low-dimensional representation of data that in some way preserves relevant qualities of the full dataset.
Different dimensionality reduction routines measure these relevant qualities in different ways, as we will see in <a href="05.10-Manifold-Learning.ipynb">In-Depth: Manifold Learning</a>.</p>

<p>As an example of this, consider the data shown in the following figure:</p>

<p><img src="figures/05.01-dimesionality-1.png" alt="" />
<a href="06.00-Figure-Code.ipynb#Dimensionality-Reduction-Example-Figure-1">figure source in Appendix</a></p>

<p>Visually, it is clear that there is some structure in this data: it is drawn from a one-dimensional line that is arranged in a spiral within this two-dimensional space.
In a sense, you could say that this data is “intrinsically” only one dimensional, though this one-dimensional data is embedded in higher-dimensional space.
A suitable dimensionality reduction model in this case would be sensitive to this nonlinear embedded structure, and be able to pull out this lower-dimensionality representation.</p>

<p>The following figure shows a visualization of the results of the Isomap algorithm, a manifold learning algorithm that does exactly this:</p>

<p><img src="figures/05.01-dimesionality-2.png" alt="" />
<a href="06.00-Figure-Code.ipynb#Dimensionality-Reduction-Example-Figure-2">figure source in Appendix</a></p>

<p>Notice that the colors (which represent the extracted one-dimensional latent variable) change uniformly along the spiral, which indicates that the algorithm did in fact detect the structure we saw by eye.
As with the previous examples, the power of dimensionality reduction algorithms becomes clearer in higher-dimensional cases.
For example, we might wish to visualize important relationships within a dataset that has 100 or 1,000 features.
Visualizing 1,000-dimensional data is a challenge, and one way we can make this more manageable is to use a dimensionality reduction technique to reduce the data to two or three dimensions.</p>

<p>Some important dimensionality reduction algorithms that we will discuss are principal component analysis (see <a href="05.09-Principal-Component-Analysis.ipynb">In Depth: Principal Component Analysis</a>) and various manifold learning algorithms, including Isomap and locally linear embedding (See <a href="05.10-Manifold-Learning.ipynb">In-Depth: Manifold Learning</a>).</p>

<h2 id="summary">Summary</h2>

<p>Here we have seen a few simple examples of some of the basic types of machine learning approaches.
Needless to say, there are a number of important practical details that we have glossed over, but I hope this section was enough to give you a basic idea of what types of problems machine learning approaches can solve.</p>

<p>In short, we saw the following:</p>

<ul>
  <li>
    <p><em>Supervised learning</em>: Models that can predict labels based on labeled training data</p>

    <ul>
      <li><em>Classification</em>: Models that predict labels as two or more discrete categories</li>
      <li><em>Regression</em>: Models that predict continuous labels</li>
    </ul>
  </li>
  <li>
    <p><em>Unsupervised learning</em>: Models that identify structure in unlabeled data</p>

    <ul>
      <li><em>Clustering</em>: Models that detect and identify distinct groups in the data</li>
      <li><em>Dimensionality reduction</em>: Models that detect and identify lower-dimensional structure in higher-dimensional data</li>
    </ul>
  </li>
</ul>

<p>In the following sections we will go into much greater depth within these categories, and see some more interesting examples of where these concepts can be useful.</p>

<p>All of the figures in the preceding discussion are generated based on actual machine learning computations; the code behind them can be found in <a href="06.00-Figure-Code.ipynb">Appendix: Figure Code</a>.</p>

<!--NAVIGATION-->
<p>&lt; <a href="05.00-Machine-Learning.ipynb">Machine Learning</a> | <a href="Index.ipynb">Contents</a> | <a href="05.02-Introducing-Scikit-Learn.ipynb">Introducing Scikit-Learn</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.01-What-Is-Machine-Learning.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<!--BOOK_INFORMATION-->
<p><img align="left" style="padding-right:10px;" src="figures/PDSH-cover-small.png" /></p>

<p><em>This notebook contains an excerpt from the <a href="http://shop.oreilly.com/product/0636920034919.do">Python Data Science Handbook</a> by Jake VanderPlas; the content is available <a href="https://github.com/jakevdp/PythonDataScienceHandbook">on GitHub</a>.</em></p>

<p><em>The text is released under the <a href="https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode">CC-BY-NC-ND license</a>, and code is released under the <a href="https://opensource.org/licenses/MIT">MIT license</a>. If you find this content useful, please consider supporting the work by <a href="http://shop.oreilly.com/product/0636920034919.do">buying the book</a>!</em></p>

<!--NAVIGATION-->
<p>&lt; <a href="05.01-What-Is-Machine-Learning.ipynb">What Is Machine Learning?</a> | <a href="Index.ipynb">Contents</a> | <a href="05.03-Hyperparameters-and-Model-Validation.ipynb">Hyperparameters and Model Validation</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.02-Introducing-Scikit-Learn.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<h1 id="introducing-scikit-learn">Introducing Scikit-Learn</h1>

<p>There are several Python libraries which provide solid implementations of a range of machine learning algorithms.
One of the best known is <a href="http://scikit-learn.org">Scikit-Learn</a>, a package that provides efficient versions of a large number of common algorithms.
Scikit-Learn is characterized by a clean, uniform, and streamlined API, as well as by very useful and complete online documentation.
A benefit of this uniformity is that once you understand the basic use and syntax of Scikit-Learn for one type of model, switching to a new model or algorithm is very straightforward.</p>

<p>This section provides an overview of the Scikit-Learn API; a solid understanding of these API elements will form the foundation for understanding the deeper practical discussion of machine learning algorithms and approaches in the following chapters.</p>

<p>We will start by covering <em>data representation</em> in Scikit-Learn, followed by covering the <em>Estimator</em> API, and finally go through a more interesting example of using these tools for exploring a set of images of hand-written digits.</p>

<h2 id="data-representation-in-scikit-learn">Data Representation in Scikit-Learn</h2>

<p>Machine learning is about creating models from data: for that reason, we’ll start by discussing how data can be represented in order to be understood by the computer.
The best way to think about data within Scikit-Learn is in terms of tables of data.</p>

<h3 id="data-as-table">Data as table</h3>

<p>A basic table is a two-dimensional grid of data, in which the rows represent individual elements of the dataset, and the columns represent quantities related to each of these elements.
For example, consider the <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">Iris dataset</a>, famously analyzed by Ronald Fisher in 1936.
We can download this dataset in the form of a Pandas <code class="highlighter-rouge">DataFrame</code> using the <a href="http://seaborn.pydata.org/">seaborn</a> library:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s">'iris'</span><span class="p">)</span>
<span class="n">iris</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output output_html">
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal_length</th>
      <th>sepal_width</th>
      <th>petal_length</th>
      <th>petal_width</th>
      <th>species</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
  </tbody>
</table>
</div>
</div>

    </div>
  </div>
</div>

<p>Here each row of the data refers to a single observed flower, and the number of rows is the total number of flowers in the dataset.
In general, we will refer to the rows of the matrix as <em>samples</em>, and the number of rows as <code class="highlighter-rouge">n_samples</code>.</p>

<p>Likewise, each column of the data refers to a particular quantitative piece of information that describes each sample.
In general, we will refer to the columns of the matrix as <em>features</em>, and the number of columns as <code class="highlighter-rouge">n_features</code>.</p>

<h4 id="features-matrix">Features matrix</h4>

<p>This table layout makes clear that the information can be thought of as a two-dimensional numerical array or matrix, which we will call the <em>features matrix</em>.
By convention, this features matrix is often stored in a variable named <code class="highlighter-rouge">X</code>.
The features matrix is assumed to be two-dimensional, with shape <code class="highlighter-rouge">[n_samples, n_features]</code>, and is most often contained in a NumPy array or a Pandas <code class="highlighter-rouge">DataFrame</code>, though some Scikit-Learn models also accept SciPy sparse matrices.</p>

<p>The samples (i.e., rows) always refer to the individual objects described by the dataset.
For example, the sample might be a flower, a person, a document, an image, a sound file, a video, an astronomical object, or anything else you can describe with a set of quantitative measurements.</p>

<p>The features (i.e., columns) always refer to the distinct observations that describe each sample in a quantitative manner.
Features are generally real-valued, but may be Boolean or discrete-valued in some cases.</p>

<h4 id="target-array">Target array</h4>

<p>In addition to the feature matrix <code class="highlighter-rouge">X</code>, we also generally work with a <em>label</em> or <em>target</em> array, which by convention we will usually call <code class="highlighter-rouge">y</code>.
The target array is usually one dimensional, with length <code class="highlighter-rouge">n_samples</code>, and is generally contained in a NumPy array or Pandas <code class="highlighter-rouge">Series</code>.
The target array may have continuous numerical values, or discrete classes/labels.
While some Scikit-Learn estimators do handle multiple target values in the form of a two-dimensional, <code class="highlighter-rouge">[n_samples, n_targets]</code> target array, we will primarily be working with the common case of a one-dimensional target array.</p>

<p>Often one point of confusion is how the target array differs from the other features columns. The distinguishing feature of the target array is that it is usually the quantity we want to <em>predict from the data</em>: in statistical terms, it is the dependent variable.
For example, in the preceding data we may wish to construct a model that can predict the species of flower based on the other measurements; in this case, the <code class="highlighter-rouge">species</code> column would be considered the target array.</p>

<p>With this target array in mind, we can use Seaborn (see <a href="04.14-Visualization-With-Seaborn.ipynb">Visualization With Seaborn</a>) to conveniently visualize the data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span><span class="p">;</span> <span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">iris</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">'species'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mf">1.5</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_50_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>For use in Scikit-Learn, we will extract the features matrix and target array from the <code class="highlighter-rouge">DataFrame</code>, which we can do using some of the Pandas <code class="highlighter-rouge">DataFrame</code> operations discussed in the <a href="03.00-Introduction-to-Pandas.ipynb">Chapter 3</a>:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_iris</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'species'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X_iris</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(150, 4)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_iris</span> <span class="o">=</span> <span class="n">iris</span><span class="p">[</span><span class="s">'species'</span><span class="p">]</span>
<span class="n">y_iris</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(150,)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>To summarize, the expected layout of features and target values is visualized in the following diagram:</p>

<p><img src="figures/05.02-samples-features.png" alt="" />
<a href="06.00-Figure-Code.ipynb#Features-and-Labels-Grid">figure source in Appendix</a></p>

<p>With this data properly formatted, we can move on to consider the <em>estimator</em> API of Scikit-Learn:</p>

<h2 id="scikit-learns-estimator-api">Scikit-Learn’s Estimator API</h2>

<p>The Scikit-Learn API is designed with the following guiding principles in mind, as outlined in the <a href="http://arxiv.org/abs/1309.0238">Scikit-Learn API paper</a>:</p>

<ul>
  <li>
    <p><em>Consistency</em>: All objects share a common interface drawn from a limited set of methods, with consistent documentation.</p>
  </li>
  <li>
    <p><em>Inspection</em>: All specified parameter values are exposed as public attributes.</p>
  </li>
  <li>
    <p><em>Limited object hierarchy</em>: Only algorithms are represented by Python classes; datasets are represented
in standard formats (NumPy arrays, Pandas <code class="highlighter-rouge">DataFrame</code>s, SciPy sparse matrices) and parameter
names use standard Python strings.</p>
  </li>
  <li>
    <p><em>Composition</em>: Many machine learning tasks can be expressed as sequences of more fundamental algorithms,
and Scikit-Learn makes use of this wherever possible.</p>
  </li>
  <li>
    <p><em>Sensible defaults</em>: When models require user-specified parameters, the library defines an appropriate default value.</p>
  </li>
</ul>

<p>In practice, these principles make Scikit-Learn very easy to use, once the basic principles are understood.
Every machine learning algorithm in Scikit-Learn is implemented via the Estimator API, which provides a consistent interface for a wide range of machine learning applications.</p>

<h3 id="basics-of-the-api">Basics of the API</h3>

<p>Most commonly, the steps in using the Scikit-Learn estimator API are as follows
(we will step through a handful of detailed examples in the sections that follow).</p>

<ol>
  <li>Choose a class of model by importing the appropriate estimator class from Scikit-Learn.</li>
  <li>Choose model hyperparameters by instantiating this class with desired values.</li>
  <li>Arrange data into a features matrix and target vector following the discussion above.</li>
  <li>Fit the model to your data by calling the <code class="highlighter-rouge">fit()</code> method of the model instance.</li>
  <li>Apply the Model to new data:
    <ul>
      <li>For supervised learning, often we predict labels for unknown data using the <code class="highlighter-rouge">predict()</code> method.</li>
      <li>For unsupervised learning, we often transform or infer properties of the data using the <code class="highlighter-rouge">transform()</code> or <code class="highlighter-rouge">predict()</code> method.</li>
    </ul>
  </li>
</ol>

<p>We will now step through several simple examples of applying supervised and unsupervised learning methods.</p>

<h3 id="supervised-learning-example-simple-linear-regression">Supervised learning example: Simple linear regression</h3>

<p>As an example of this process, let’s consider a simple linear regression—that is, the common case of fitting a line to $(x, y)$ data.
We will use the following simple data for our regression example:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_61_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>With this data in place, we can use the recipe outlined earlier. Let’s walk through the process:</p>

<h4 id="1-choose-a-class-of-model">1. Choose a class of model</h4>

<p>In Scikit-Learn, every class of model is represented by a Python class.
So, for example, if we would like to compute a simple linear regression model, we can import the linear regression class:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Note that other more general linear regression models exist as well; you can read more about them in the <a href="http://Scikit-Learn.org/stable/modules/linear_model.html"><code class="highlighter-rouge">sklearn.linear_model</code> module documentation</a>.</p>

<h4 id="2-choose-model-hyperparameters">2. Choose model hyperparameters</h4>

<p>An important point is that <em>a class of model is not the same as an instance of a model</em>.</p>

<p>Once we have decided on our model class, there are still some options open to us.
Depending on the model class we are working with, we might need to answer one or more questions like the following:</p>

<ul>
  <li>Would we like to fit for the offset (i.e., <em>y</em>-intercept)?</li>
  <li>Would we like the model to be normalized?</li>
  <li>Would we like to preprocess our features to add model flexibility?</li>
  <li>What degree of regularization would we like to use in our model?</li>
  <li>How many model components would we like to use?</li>
</ul>

<p>These are examples of the important choices that must be made <em>once the model class is selected</em>.
These choices are often represented as <em>hyperparameters</em>, or parameters that must be set before the model is fit to data.
In Scikit-Learn, hyperparameters are chosen by passing values at model instantiation.
We will explore how you can quantitatively motivate the choice of hyperparameters in <a href="05.03-Hyperparameters-and-Model-Validation.ipynb">Hyperparameters and Model Validation</a>.</p>

<p>For our linear regression example, we can instantiate the <code class="highlighter-rouge">LinearRegression</code> class and specify that we would like to fit the intercept using the <code class="highlighter-rouge">fit_intercept</code> hyperparameter:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">model</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>Keep in mind that when the model is instantiated, the only action is the storing of these hyperparameter values.
In particular, we have not yet applied the model to any data: the Scikit-Learn API makes very clear the distinction between <em>choice of model</em> and <em>application of model to data</em>.</p>

<h4 id="3-arrange-data-into-a-features-matrix-and-target-vector">3. Arrange data into a features matrix and target vector</h4>

<p>Previously we detailed the Scikit-Learn data representation, which requires a two-dimensional features matrix and a one-dimensional target array.
Here our target variable <code class="highlighter-rouge">y</code> is already in the correct form (a length-<code class="highlighter-rouge">n_samples</code> array), but we need to massage the data <code class="highlighter-rouge">x</code> to make it a matrix of size <code class="highlighter-rouge">[n_samples, n_features]</code>.
In this case, this amounts to a simple reshaping of the one-dimensional array:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
<span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(50, 1)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<h4 id="4-fit-the-model-to-your-data">4. Fit the model to your data</h4>

<p>Now it is time to apply our model to data.
This can be done with the <code class="highlighter-rouge">fit()</code> method of the model:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>This <code class="highlighter-rouge">fit()</code> command causes a number of model-dependent internal computations to take place, and the results of these computations are stored in model-specific attributes that the user can explore.
In Scikit-Learn, by convention all model parameters that were learned during the <code class="highlighter-rouge">fit()</code> process have trailing underscores; for example in this linear model, we have the following:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="n">coef_</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([ 1.9776566])
</code></pre></div>      </div>

    </div>
  </div>
</div>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="n">intercept_</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-0.90331072553111635
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>These two parameters represent the slope and intercept of the simple linear fit to the data.
Comparing to the data definition, we see that they are very close to the input slope of 2 and intercept of -1.</p>

<p>One question that frequently comes up regards the uncertainty in such internal model parameters.
In general, Scikit-Learn does not provide tools to draw conclusions from internal model parameters themselves: interpreting model parameters is much more a <em>statistical modeling</em> question than a <em>machine learning</em> question.
Machine learning rather focuses on what the model <em>predicts</em>.
If you would like to dive into the meaning of fit parameters within the model, other tools are available, including the <a href="http://statsmodels.sourceforge.net/">Statsmodels Python package</a>.</p>

<h4 id="5-predict-labels-for-unknown-data">5. Predict labels for unknown data</h4>

<p>Once the model is trained, the main task of supervised machine learning is to evaluate it based on what it says about new data that was not part of the training set.
In Scikit-Learn, this can be done using the <code class="highlighter-rouge">predict()</code> method.
For the sake of this example, our “new data” will be a grid of <em>x</em> values, and we will ask what <em>y</em> values the model predicts:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xfit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>As before, we need to coerce these <em>x</em> values into a <code class="highlighter-rouge">[n_samples, n_features]</code> features matrix, after which we can feed it to the model:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Xfit</span> <span class="o">=</span> <span class="n">xfit</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
<span class="n">yfit</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xfit</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Finally, let’s visualize the results by plotting first the raw data, and then this model fit:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xfit</span><span class="p">,</span> <span class="n">yfit</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_82_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Typically the efficacy of the model is evaluated by comparing its results to some known baseline, as we will see in the next example</p>

<h3 id="supervised-learning-example-iris-classification">Supervised learning example: Iris classification</h3>

<p>Let’s take a look at another example of this process, using the Iris dataset we discussed earlier.
Our question will be this: given a model trained on a portion of the Iris data, how well can we predict the remaining labels?</p>

<p>For this task, we will use an extremely simple generative model known as Gaussian naive Bayes, which proceeds by assuming each class is drawn from an axis-aligned Gaussian distribution (see <a href="05.05-Naive-Bayes.ipynb">In Depth: Naive Bayes Classification</a> for more details).
Because it is so fast and has no hyperparameters to choose, Gaussian naive Bayes is often a good model to use as a baseline classification, before exploring whether improvements can be found through more sophisticated models.</p>

<p>We would like to evaluate the model on data it has not seen before, and so we will split the data into a <em>training set</em> and a <em>testing set</em>.
This could be done by hand, but it is more convenient to use the <code class="highlighter-rouge">train_test_split</code> utility function:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">Xtrain</span><span class="p">,</span> <span class="n">Xtest</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">ytest</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_iris</span><span class="p">,</span> <span class="n">y_iris</span><span class="p">,</span>
                                                <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>With the data arranged, we can follow our recipe to predict the labels:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span> <span class="c"># 1. choose model class</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>                       <span class="c"># 2. instantiate model</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">)</span>                  <span class="c"># 3. fit model to data</span>
<span class="n">y_model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xtest</span><span class="p">)</span>             <span class="c"># 4. predict on new data</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Finally, we can use the <code class="highlighter-rouge">accuracy_score</code> utility to see the fraction of predicted labels that match their true value:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="n">accuracy_score</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">y_model</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.97368421052631582
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>With an accuracy topping 97%, we see that even this very naive classification algorithm is effective for this particular dataset!</p>

<h3 id="unsupervised-learning-example-iris-dimensionality">Unsupervised learning example: Iris dimensionality</h3>

<p>As an example of an unsupervised learning problem, let’s take a look at reducing the dimensionality of the Iris data so as to more easily visualize it.
Recall that the Iris data is four dimensional: there are four features recorded for each sample.</p>

<p>The task of dimensionality reduction is to ask whether there is a suitable lower-dimensional representation that retains the essential features of the data.
Often dimensionality reduction is used as an aid to visualizing data: after all, it is much easier to plot data in two dimensions than in four dimensions or higher!</p>

<p>Here we will use principal component analysis (PCA; see <a href="05.09-Principal-Component-Analysis.ipynb">In Depth: Principal Component Analysis</a>), which is a fast linear dimensionality reduction technique.
We will ask the model to return two components—that is, a two-dimensional representation of the data.</p>

<p>Following the sequence of steps outlined earlier, we have:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>  <span class="c"># 1. Choose the model class</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>            <span class="c"># 2. Instantiate the model with hyperparameters</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_iris</span><span class="p">)</span>                      <span class="c"># 3. Fit to data. Notice y is not specified!</span>
<span class="n">X_2D</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_iris</span><span class="p">)</span>         <span class="c"># 4. Transform the data to two dimensions</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Now let’s plot the results. A quick way to do this is to insert the results into the original Iris <code class="highlighter-rouge">DataFrame</code>, and use Seaborn’s <code class="highlighter-rouge">lmplot</code> to show the results:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">iris</span><span class="p">[</span><span class="s">'PCA1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_2D</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">iris</span><span class="p">[</span><span class="s">'PCA2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_2D</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lmplot</span><span class="p">(</span><span class="s">"PCA1"</span><span class="p">,</span> <span class="s">"PCA2"</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">'species'</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">iris</span><span class="p">,</span> <span class="n">fit_reg</span><span class="o">=</span><span class="bp">False</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_94_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>We see that in the two-dimensional representation, the species are fairly well separated, even though the PCA algorithm had no knowledge of the species labels!
This indicates to us that a relatively straightforward classification will probably be effective on the dataset, as we saw before.</p>

<h3 id="unsupervised-learning-iris-clustering">Unsupervised learning: Iris clustering</h3>

<p>Let’s next look at applying clustering to the Iris data.
A clustering algorithm attempts to find distinct groups of data without reference to any labels.
Here we will use a powerful clustering method called a Gaussian mixture model (GMM), discussed in more detail in <a href="05.12-Gaussian-Mixtures.ipynb">In Depth: Gaussian Mixture Models</a>.
A GMM attempts to model the data as a collection of Gaussian blobs.</p>

<p>We can fit the Gaussian mixture model as follows:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.mixture</span> <span class="kn">import</span> <span class="n">GMM</span>      <span class="c"># 1. Choose the model class</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GMM</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
            <span class="n">covariance_type</span><span class="o">=</span><span class="s">'full'</span><span class="p">)</span>  <span class="c"># 2. Instantiate the model with hyperparameters</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_iris</span><span class="p">)</span>                    <span class="c"># 3. Fit to data. Notice y is not specified!</span>
<span class="n">y_gmm</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_iris</span><span class="p">)</span>        <span class="c"># 4. Determine cluster labels</span>
</code></pre></div>    </div>
  </div>

</div>

<p>As before, we will add the cluster label to the Iris <code class="highlighter-rouge">DataFrame</code> and use Seaborn to plot the results:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">iris</span><span class="p">[</span><span class="s">'cluster'</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_gmm</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lmplot</span><span class="p">(</span><span class="s">"PCA1"</span><span class="p">,</span> <span class="s">"PCA2"</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">iris</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">'species'</span><span class="p">,</span>
           <span class="n">col</span><span class="o">=</span><span class="s">'cluster'</span><span class="p">,</span> <span class="n">fit_reg</span><span class="o">=</span><span class="bp">False</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_99_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>By splitting the data by cluster number, we see exactly how well the GMM algorithm has recovered the underlying label: the <em>setosa</em> species is separated perfectly within cluster 0, while there remains a small amount of mixing between <em>versicolor</em> and <em>virginica</em>.
This means that even without an expert to tell us the species labels of the individual flowers, the measurements of these flowers are distinct enough that we could <em>automatically</em> identify the presence of these different groups of species with a simple clustering algorithm!
This sort of algorithm might further give experts in the field clues as to the relationship between the samples they are observing.</p>

<h2 id="application-exploring-hand-written-digits">Application: Exploring Hand-written Digits</h2>

<p>To demonstrate these principles on a more interesting problem, let’s consider one piece of the optical character recognition problem: the identification of hand-written digits.
In the wild, this problem involves both locating and identifying characters in an image. Here we’ll take a shortcut and use Scikit-Learn’s set of pre-formatted digits, which is built into the library.</p>

<h3 id="loading-and-visualizing-the-digits-data">Loading and visualizing the digits data</h3>

<p>We’ll use Scikit-Learn’s data access interface and take a look at this data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="n">digits</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>
<span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(1797, 8, 8)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>The images data is a three-dimensional array: 1,797 samples each consisting of an 8 × 8 grid of pixels.
Let’s visualize the first hundred of these:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
                         <span class="n">subplot_kw</span><span class="o">=</span><span class="p">{</span><span class="s">'xticks'</span><span class="p">:[],</span> <span class="s">'yticks'</span><span class="p">:[]},</span>
                         <span class="n">gridspec_kw</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flat</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'binary'</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s">'nearest'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span>
            <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'green'</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_106_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>In order to work with this data within Scikit-Learn, we need a two-dimensional, <code class="highlighter-rouge">[n_samples, n_features]</code> representation.
We can accomplish this by treating each pixel in the image as a feature: that is, by flattening out the pixel arrays so that we have a length-64 array of pixel values representing each digit.
Additionally, we need the target array, which gives the previously determined label for each digit.
These two quantities are built into the digits dataset under the <code class="highlighter-rouge">data</code> and <code class="highlighter-rouge">target</code> attributes, respectively:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">data</span>
<span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(1797, 64)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span>
<span class="n">y</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(1797,)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>We see here that there are 1,797 samples and 64 features.</p>

<h3 id="unsupervised-learning-dimensionality-reduction">Unsupervised learning: Dimensionality reduction</h3>

<p>We’d like to visualize our points within the 64-dimensional parameter space, but it’s difficult to effectively visualize points in such a high-dimensional space.
Instead we’ll reduce the dimensions to 2, using an unsupervised method.
Here, we’ll make use of a manifold learning algorithm called <em>Isomap</em> (see <a href="05.10-Manifold-Learning.ipynb">In-Depth: Manifold Learning</a>), and transform the data to two dimensions:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">Isomap</span>
<span class="n">iso</span> <span class="o">=</span> <span class="n">Isomap</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">iso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">data_projected</span> <span class="o">=</span> <span class="n">iso</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">data_projected</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(1797, 2)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>We see that the projected data is now two-dimensional.
Let’s plot this data to see if we can learn anything from its structure:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_projected</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_projected</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">,</span>
            <span class="n">edgecolor</span><span class="o">=</span><span class="s">'none'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
            <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s">'spectral'</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s">'digit label'</span><span class="p">,</span> <span class="n">ticks</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">9.5</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_114_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>This plot gives us some good intuition into how well various numbers are separated in the larger 64-dimensional space. For example, zeros (in black) and ones (in purple) have very little overlap in parameter space.
Intuitively, this makes sense: a zero is empty in the middle of the image, while a one will generally have ink in the middle.
On the other hand, there seems to be a more or less continuous spectrum between ones and fours: we can understand this by realizing that some people draw ones with “hats” on them, which cause them to look similar to fours.</p>

<p>Overall, however, the different groups appear to be fairly well separated in the parameter space: this tells us that even a very straightforward supervised classification algorithm should perform suitably on this data.
Let’s give it a try.</p>

<h3 id="classification-on-digits">Classification on digits</h3>

<p>Let’s apply a classification algorithm to the digits.
As with the Iris data previously, we will split the data into a training and testing set, and fit a Gaussian naive Bayes model:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Xtrain</span><span class="p">,</span> <span class="n">Xtest</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">ytest</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">)</span>
<span class="n">y_model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xtest</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Now that we have predicted our model, we can gauge its accuracy by comparing the true values of the test set to the predictions:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="n">accuracy_score</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">y_model</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.83333333333333337
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>With even this extremely simple model, we find about 80% accuracy for classification of the digits!
However, this single number doesn’t tell us <em>where</em> we’ve gone wrong—one nice way to do this is to use the <em>confusion matrix</em>, which we can compute with Scikit-Learn and plot with Seaborn:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>

<span class="n">mat</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">y_model</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'predicted value'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'true value'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_122_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>This shows us where the mis-labeled points tend to be: for example, a large number of twos here are mis-classified as either ones or eights.
Another way to gain intuition into the characteristics of the model is to plot the inputs again, with their predicted labels.
We’ll use green for correct labels, and red for incorrect labels:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
                         <span class="n">subplot_kw</span><span class="o">=</span><span class="p">{</span><span class="s">'xticks'</span><span class="p">:[],</span> <span class="s">'yticks'</span><span class="p">:[]},</span>
                         <span class="n">gridspec_kw</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>

<span class="n">test_images</span> <span class="o">=</span> <span class="n">Xtest</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flat</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">test_images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'binary'</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s">'nearest'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">y_model</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span>
            <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span>
            <span class="n">color</span><span class="o">=</span><span class="s">'green'</span> <span class="k">if</span> <span class="p">(</span><span class="n">ytest</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">y_model</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">else</span> <span class="s">'red'</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_124_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Examining this subset of the data, we can gain insight regarding where the algorithm might be not performing optimally.
To go beyond our 80% classification rate, we might move to a more sophisticated algorithm such as support vector machines (see <a href="05.07-Support-Vector-Machines.ipynb">In-Depth: Support Vector Machines</a>), random forests (see <a href="05.08-Random-Forests.ipynb">In-Depth: Decision Trees and Random Forests</a>) or another classification approach.</p>

<h2 id="summary-1">Summary</h2>

<p>In this section we have covered the essential features of the Scikit-Learn data representation, and the estimator API.
Regardless of the type of estimator, the same import/instantiate/fit/predict pattern holds.
Armed with this information about the estimator API, you can explore the Scikit-Learn documentation and begin trying out various models on your data.</p>

<p>In the next section, we will explore perhaps the most important topic in machine learning: how to select and validate your model.</p>

<!--NAVIGATION-->
<p>&lt; <a href="05.01-What-Is-Machine-Learning.ipynb">What Is Machine Learning?</a> | <a href="Index.ipynb">Contents</a> | <a href="05.03-Hyperparameters-and-Model-Validation.ipynb">Hyperparameters and Model Validation</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.02-Introducing-Scikit-Learn.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<!--BOOK_INFORMATION-->
<p><img align="left" style="padding-right:10px;" src="figures/PDSH-cover-small.png" /></p>

<p><em>This notebook contains an excerpt from the <a href="http://shop.oreilly.com/product/0636920034919.do">Python Data Science Handbook</a> by Jake VanderPlas; the content is available <a href="https://github.com/jakevdp/PythonDataScienceHandbook">on GitHub</a>.</em></p>

<p><em>The text is released under the <a href="https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode">CC-BY-NC-ND license</a>, and code is released under the <a href="https://opensource.org/licenses/MIT">MIT license</a>. If you find this content useful, please consider supporting the work by <a href="http://shop.oreilly.com/product/0636920034919.do">buying the book</a>!</em></p>

<!--NAVIGATION-->
<p>&lt; <a href="05.02-Introducing-Scikit-Learn.ipynb">Introducing Scikit-Learn</a> | <a href="Index.ipynb">Contents</a> | <a href="05.04-Feature-Engineering.ipynb">Feature Engineering</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.03-Hyperparameters-and-Model-Validation.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<h1 id="hyperparameters-and-model-validation">Hyperparameters and Model Validation</h1>

<p>In the previous section, we saw the basic recipe for applying a supervised machine learning model:</p>

<ol>
  <li>Choose a class of model</li>
  <li>Choose model hyperparameters</li>
  <li>Fit the model to the training data</li>
  <li>Use the model to predict labels for new data</li>
</ol>

<p>The first two pieces of this—the choice of model and choice of hyperparameters—are perhaps the most important part of using these tools and techniques effectively.
In order to make an informed choice, we need a way to <em>validate</em> that our model and our hyperparameters are a good fit to the data.
While this may sound simple, there are some pitfalls that you must avoid to do this effectively.</p>

<h2 id="thinking-about-model-validation">Thinking about Model Validation</h2>

<p>In principle, model validation is very simple: after choosing a model and its hyperparameters, we can estimate how effective it is by applying it to some of the training data and comparing the prediction to the known value.</p>

<p>The following sections first show a naive approach to model validation and why it
fails, before exploring the use of holdout sets and cross-validation for more robust
model evaluation.</p>

<h3 id="model-validation-the-wrong-way">Model validation the wrong way</h3>

<p>Let’s demonstrate the naive approach to validation using the Iris data, which we saw in the previous section.
We will start by loading the data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Next we choose a model and hyperparameters. Here we’ll use a <em>k</em>-neighbors classifier with <code class="highlighter-rouge">n_neighbors=1</code>.
This is a very simple and intuitive model that says “the label of an unknown point is the same as the label of its closest training point:”</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Then we train the model, and use it to predict labels for data we already know:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Finally, we compute the fraction of correctly labeled points:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_model</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1.0
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>We see an accuracy score of 1.0, which indicates that 100% of points were correctly labeled by our model!
But is this truly measuring the expected accuracy? Have we really come upon a model that we expect to be correct 100% of the time?</p>

<p>As you may have gathered, the answer is no.
In fact, this approach contains a fundamental flaw: <em>it trains and evaluates the model on the same data</em>.
Furthermore, the nearest neighbor model is an <em>instance-based</em> estimator that simply stores the training data, and predicts labels by comparing new data to these stored points: except in contrived cases, it will get 100% accuracy <em>every time!</em></p>

<h3 id="model-validation-the-right-way-holdout-sets">Model validation the right way: Holdout sets</h3>

<p>So what can be done?
A better sense of a model’s performance can be found using what’s known as a <em>holdout set</em>: that is, we hold back some subset of the data from the training of the model, and then use this holdout set to check the model performance.
This splitting can be done using the <code class="highlighter-rouge">train_test_split</code> utility in Scikit-Learn:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="c"># split the data with 50% in each set</span>
<span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">y2</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                  <span class="n">train_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c"># fit the model on one set of data</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">y1</span><span class="p">)</span>

<span class="c"># evaluate the model on the second set of data</span>
<span class="n">y2_model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X2</span><span class="p">)</span>
<span class="n">accuracy_score</span><span class="p">(</span><span class="n">y2</span><span class="p">,</span> <span class="n">y2_model</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.90666666666666662
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>We see here a more reasonable result: the nearest-neighbor classifier is about 90% accurate on this hold-out set.
The hold-out set is similar to unknown data, because the model has not “seen” it before.</p>

<h3 id="model-validation-via-cross-validation">Model validation via cross-validation</h3>

<p>One disadvantage of using a holdout set for model validation is that we have lost a portion of our data to the model training.
In the above case, half the dataset does not contribute to the training of the model!
This is not optimal, and can cause problems – especially if the initial set of training data is small.</p>

<p>One way to address this is to use <em>cross-validation</em>; that is, to do a sequence of fits where each subset of the data is used both as a training set and as a validation set.
Visually, it might look something like this:</p>

<p><img src="figures/05.03-2-fold-CV.png" alt="" />
<a href="06.00-Figure-Code.ipynb#2-Fold-Cross-Validation">figure source in Appendix</a></p>

<p>Here we do two validation trials, alternately using each half of the data as a holdout set.
Using the split data from before, we could implement it like this:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y2_model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">y1</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X2</span><span class="p">)</span>
<span class="n">y1_model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span> <span class="n">y2</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X1</span><span class="p">)</span>
<span class="n">accuracy_score</span><span class="p">(</span><span class="n">y1</span><span class="p">,</span> <span class="n">y1_model</span><span class="p">),</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y2</span><span class="p">,</span> <span class="n">y2_model</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(0.95999999999999996, 0.90666666666666662)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>What comes out are two accuracy scores, which we could combine (by, say, taking the mean) to get a better measure of the global model performance.
This particular form of cross-validation is a <em>two-fold cross-validation</em>—that is, one in which we have split the data into two sets and used each in turn as a validation set.</p>

<p>We could expand on this idea to use even more trials, and more folds in the data—for example, here is a visual depiction of five-fold cross-validation:</p>

<p><img src="figures/05.03-5-fold-CV.png" alt="" />
<a href="06.00-Figure-Code.ipynb#5-Fold-Cross-Validation">figure source in Appendix</a></p>

<p>Here we split the data into five groups, and use each of them in turn to evaluate the model fit on the other 4/5 of the data.
This would be rather tedious to do by hand, and so we can use Scikit-Learn’s <code class="highlighter-rouge">cross_val_score</code> convenience routine to do it succinctly:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([ 0.96666667,  0.96666667,  0.93333333,  0.93333333,  1.        ])
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>Repeating the validation across different subsets of the data gives us an even better idea of the performance of the algorithm.</p>

<p>Scikit-Learn implements a number of useful cross-validation schemes that are useful in particular situations; these are implemented via iterators in the <code class="highlighter-rouge">cross_validation</code> module.
For example, we might wish to go to the extreme case in which our number of folds is equal to the number of data points: that is, we train on all points but one in each trial.
This type of cross-validation is known as <em>leave-one-out</em> cross validation, and can be used as follows:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">LeaveOneOut</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">LeaveOneOut</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)))</span>
<span class="n">scores</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.])
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>Because we have 150 samples, the leave one out cross-validation yields scores for 150 trials, and the score indicates either successful (1.0) or unsuccessful (0.0) prediction.
Taking the mean of these gives an estimate of the error rate:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.95999999999999996
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>Other cross-validation schemes can be used similarly.
For a description of what is available in Scikit-Learn, use IPython to explore the <code class="highlighter-rouge">sklearn.cross_validation</code> submodule, or take a look at Scikit-Learn’s online <a href="http://scikit-learn.org/stable/modules/cross_validation.html">cross-validation documentation</a>.</p>

<h2 id="selecting-the-best-model">Selecting the Best Model</h2>

<p>Now that we’ve seen the basics of validation and cross-validation, we will go into a litte more depth regarding model selection and selection of hyperparameters.
These issues are some of the most important aspects of the practice of machine learning, and I find that this information is often glossed over in introductory machine learning tutorials.</p>

<p>Of core importance is the following question: <em>if our estimator is underperforming, how should we move forward?</em>
There are several possible answers:</p>

<ul>
  <li>Use a more complicated/more flexible model</li>
  <li>Use a less complicated/less flexible model</li>
  <li>Gather more training samples</li>
  <li>Gather more data to add features to each sample</li>
</ul>

<p>The answer to this question is often counter-intuitive.
In particular, sometimes using a more complicated model will give worse results, and adding more training samples may not improve your results!
The ability to determine what steps will improve your model is what separates the successful machine learning practitioners from the unsuccessful.</p>

<h3 id="the-bias-variance-trade-off">The Bias-variance trade-off</h3>

<p>Fundamentally, the question of “the best model” is about finding a sweet spot in the tradeoff between <em>bias</em> and <em>variance</em>.
Consider the following figure, which presents two regression fits to the same dataset:</p>

<p><img src="figures/05.03-bias-variance.png" alt="" />
<a href="06.00-Figure-Code.ipynb#Bias-Variance-Tradeoff">figure source in Appendix</a></p>

<p>It is clear that neither of these models is a particularly good fit to the data, but they fail in different ways.</p>

<p>The model on the left attempts to find a straight-line fit through the data.
Because the data are intrinsically more complicated than a straight line, the straight-line model will never be able to describe this dataset well.
Such a model is said to <em>underfit</em> the data: that is, it does not have enough model flexibility to suitably account for all the features in the data; another way of saying this is that the model has high <em>bias</em>.</p>

<p>The model on the right attempts to fit a high-order polynomial through the data.
Here the model fit has enough flexibility to nearly perfectly account for the fine features in the data, but even though it very accurately describes the training data, its precise form seems to be more reflective of the particular noise properties of the data rather than the intrinsic properties of whatever process generated that data.
Such a model is said to <em>overfit</em> the data: that is, it has so much model flexibility that the model ends up accounting for random errors as well as the underlying data distribution; another way of saying this is that the model has high <em>variance</em>.</p>

<p>To look at this in another light, consider what happens if we use these two models to predict the y-value for some new data.
In the following diagrams, the red/lighter points indicate data that is omitted from the training set:</p>

<p><img src="figures/05.03-bias-variance-2.png" alt="" />
<a href="06.00-Figure-Code.ipynb#Bias-Variance-Tradeoff-Metrics">figure source in Appendix</a></p>

<p>The score here is the $R^2$ score, or <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination">coefficient of determination</a>, which measures how well a model performs relative to a simple mean of the target values. $R^2=1$ indicates a perfect match, $R^2=0$ indicates the model does no better than simply taking the mean of the data, and negative values mean even worse models.
From the scores associated with these two models, we can make an observation that holds more generally:</p>

<ul>
  <li>For high-bias models, the performance of the model on the validation set is similar to the performance on the training set.</li>
  <li>For high-variance models, the performance of the model on the validation set is far worse than the performance on the training set.</li>
</ul>

<p>If we imagine that we have some ability to tune the model complexity, we would expect the training score and validation score to behave as illustrated in the following figure:</p>

<p><img src="figures/05.03-validation-curve.png" alt="" />
<a href="06.00-Figure-Code.ipynb#Validation-Curve">figure source in Appendix</a></p>

<p>The diagram shown here is often called a <em>validation curve</em>, and we see the following essential features:</p>

<ul>
  <li>The training score is everywhere higher than the validation score. This is generally the case: the model will be a better fit to data it has seen than to data it has not seen.</li>
  <li>For very low model complexity (a high-bias model), the training data is under-fit, which means that the model is a poor predictor both for the training data and for any previously unseen data.</li>
  <li>For very high model complexity (a high-variance model), the training data is over-fit, which means that the model predicts the training data very well, but fails for any previously unseen data.</li>
  <li>For some intermediate value, the validation curve has a maximum. This level of complexity indicates a suitable trade-off between bias and variance.</li>
</ul>

<p>The means of tuning the model complexity varies from model to model; when we discuss individual models in depth in later sections, we will see how each model allows for such tuning.</p>

<h3 id="validation-curves-in-scikit-learn">Validation curves in Scikit-Learn</h3>

<p>Let’s look at an example of using cross-validation to compute the validation curve for a class of models.
Here we will use a <em>polynomial regression</em> model: this is a generalized linear model in which the degree of the polynomial is a tunable parameter.
For example, a degree-1 polynomial fits a straight line to the data; for model parameters $a$ and $b$:</p>

<script type="math/tex; mode=display">y = ax + b</script>

<p>A degree-3 polynomial fits a cubic curve to the data; for model parameters $a, b, c, d$:</p>

<script type="math/tex; mode=display">y = ax^3 + bx^2 + cx + d</script>

<p>We can generalize this to any number of polynomial features.
In Scikit-Learn, we can implement this with a simple linear regression combined with the polynomial preprocessor.
We will use a <em>pipeline</em> to string these operations together (we will discuss polynomial features and pipelines more fully in <a href="05.04-Feature-Engineering.ipynb">Feature Engineering</a>):</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>

<span class="k">def</span> <span class="nf">PolynomialRegression</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="p">),</span>
                         <span class="n">LinearRegression</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Now let’s create some data to which we will fit our model:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">make_data</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">err</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">rseed</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="c"># randomly sample the data</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">rseed</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">y</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">-</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="o">+</span> <span class="mf">0.1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">err</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">+=</span> <span class="n">err</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_data</span><span class="p">(</span><span class="mi">40</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>We can now visualize our data, along with polynomial fits of several degrees:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span><span class="p">;</span> <span class="n">seaborn</span><span class="o">.</span><span class="nb">set</span><span class="p">()</span>  <span class="c"># plot formatting</span>

<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="mi">500</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'black'</span><span class="p">)</span>
<span class="n">axis</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">()</span>
<span class="k">for</span> <span class="n">degree</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">]:</span>
    <span class="n">y_test</span> <span class="o">=</span> <span class="n">PolynomialRegression</span><span class="p">(</span><span class="n">degree</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'degree={0}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">degree</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'best'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_164_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The knob controlling model complexity in this case is the degree of the polynomial, which can be any non-negative integer.
A useful question to answer is this: what degree of polynomial provides a suitable trade-off between bias (under-fitting) and variance (over-fitting)?</p>

<p>We can make progress in this by visualizing the validation curve for this particular data and model; this can be done straightforwardly using the <code class="highlighter-rouge">validation_curve</code> convenience routine provided by Scikit-Learn.
Given a model, data, parameter name, and a range to explore, this function will automatically compute both the training score and validation score across the range:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.learning_curve</span> <span class="kn">import</span> <span class="n">validation_curve</span>
<span class="n">degree</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">21</span><span class="p">)</span>
<span class="n">train_score</span><span class="p">,</span> <span class="n">val_score</span> <span class="o">=</span> <span class="n">validation_curve</span><span class="p">(</span><span class="n">PolynomialRegression</span><span class="p">(),</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                          <span class="s">'polynomialfeatures__degree'</span><span class="p">,</span> <span class="n">degree</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">degree</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">train_score</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s">'blue'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'training score'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">degree</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">val_score</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'validation score'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'best'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'degree'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'score'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_166_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>This shows precisely the qualitative behavior we expect: the training score is everywhere higher than the validation score; the training score is monotonically improving with increased model complexity; and the validation score reaches a maximum before dropping off as the model becomes over-fit.</p>

<p>From the validation curve, we can read-off that the optimal trade-off between bias and variance is found for a third-order polynomial; we can compute and display this fit over the original data as follows:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">y</span><span class="p">)</span>
<span class="n">lim</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">()</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">PolynomialRegression</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">y_test</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="n">lim</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_168_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Notice that finding this optimal model did not actually require us to compute the training score, but examining the relationship between the training score and validation score can give us useful insight into the performance of the model.</p>

<h2 id="learning-curves">Learning Curves</h2>

<p>One important aspect of model complexity is that the optimal model will generally depend on the size of your training data.
For example, let’s generate a new dataset with a factor of five more points:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X2</span><span class="p">,</span> <span class="n">y2</span> <span class="o">=</span> <span class="n">make_data</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X2</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">y2</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_171_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>We will duplicate the preceding code to plot the validation curve for this larger dataset; for reference let’s over-plot the previous results as well:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">degree</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">21</span><span class="p">)</span>
<span class="n">train_score2</span><span class="p">,</span> <span class="n">val_score2</span> <span class="o">=</span> <span class="n">validation_curve</span><span class="p">(</span><span class="n">PolynomialRegression</span><span class="p">(),</span> <span class="n">X2</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span>
                                            <span class="s">'polynomialfeatures__degree'</span><span class="p">,</span> <span class="n">degree</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">degree</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">train_score2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s">'blue'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'training score'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">degree</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">val_score2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'validation score'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">degree</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">train_score</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s">'blue'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">degree</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">val_score</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'lower center'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'degree'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'score'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_173_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The solid lines show the new results, while the fainter dashed lines show the results of the previous smaller dataset.
It is clear from the validation curve that the larger dataset can support a much more complicated model: the peak here is probably around a degree of 6, but even a degree-20 model is not seriously over-fitting the data—the validation and training scores remain very close.</p>

<p>Thus we see that the behavior of the validation curve has not one but two important inputs: the model complexity and the number of training points.
It is often useful to to explore the behavior of the model as a function of the number of training points, which we can do by using increasingly larger subsets of the data to fit our model.
A plot of the training/validation score with respect to the size of the training set is known as a <em>learning curve.</em></p>

<p>The general behavior we would expect from a learning curve is this:</p>

<ul>
  <li>A model of a given complexity will <em>overfit</em> a small dataset: this means the training score will be relatively high, while the validation score will be relatively low.</li>
  <li>A model of a given complexity will <em>underfit</em> a large dataset: this means that the training score will decrease, but the validation score will increase.</li>
  <li>A model will never, except by chance, give a better score to the validation set than the training set: this means the curves should keep getting closer together but never cross.</li>
</ul>

<p>With these features in mind, we would expect a learning curve to look qualitatively like that shown in the following figure:</p>

<p><img src="figures/05.03-learning-curve.png" alt="" />
<a href="06.00-Figure-Code.ipynb#Learning-Curve">figure source in Appendix</a></p>

<p>The notable feature of the learning curve is the convergence to a particular score as the number of training samples grows.
In particular, once you have enough points that a particular model has converged, <em>adding more training data will not help you!</em>
The only way to increase model performance in this case is to use another (often more complex) model.</p>

<h3 id="learning-curves-in-scikit-learn">Learning curves in Scikit-Learn</h3>

<p>Scikit-Learn offers a convenient utility for computing such learning curves from your models; here we will compute a learning curve for our original dataset with a second-order polynomial model and a ninth-order polynomial:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.learning_curve</span> <span class="kn">import</span> <span class="n">learning_curve</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0625</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">degree</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">9</span><span class="p">]):</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">train_lc</span><span class="p">,</span> <span class="n">val_lc</span> <span class="o">=</span> <span class="n">learning_curve</span><span class="p">(</span><span class="n">PolynomialRegression</span><span class="p">(</span><span class="n">degree</span><span class="p">),</span>
                                         <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
                                         <span class="n">train_sizes</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">25</span><span class="p">))</span>

    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_lc</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s">'blue'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'training score'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">val_lc</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'validation score'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">train_lc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">val_lc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]),</span> <span class="n">N</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">N</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                 <span class="n">color</span><span class="o">=</span><span class="s">'gray'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">)</span>

    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">N</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">N</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'training size'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'score'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'degree = {0}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">degree</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'best'</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_178_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>This is a valuable diagnostic, because it gives us a visual depiction of how our model responds to increasing training data.
In particular, when your learning curve has already converged (i.e., when the training and validation curves are already close to each other) <em>adding more training data will not significantly improve the fit!</em>
This situation is seen in the left panel, with the learning curve for the degree-2 model.</p>

<p>The only way to increase the converged score is to use a different (usually more complicated) model.
We see this in the right panel: by moving to a much more complicated model, we increase the score of convergence (indicated by the dashed line), but at the expense of higher model variance (indicated by the difference between the training and validation scores).
If we were to add even more data points, the learning curve for the more complicated model would eventually converge.</p>

<p>Plotting a learning curve for your particular choice of model and dataset can help you to make this type of decision about how to move forward in improving your analysis.</p>

<h2 id="validation-in-practice-grid-search">Validation in Practice: Grid Search</h2>

<p>The preceding discussion is meant to give you some intuition into the trade-off between bias and variance, and its dependence on model complexity and training set size.
In practice, models generally have more than one knob to turn, and thus plots of validation and learning curves change from lines to multi-dimensional surfaces.
In these cases, such visualizations are difficult and we would rather simply find the particular model that maximizes the validation score.</p>

<p>Scikit-Learn provides automated tools to do this in the grid search module.
Here is an example of using grid search to find the optimal polynomial model.
We will explore a three-dimensional grid of model features; namely the polynomial degree, the flag telling us whether to fit the intercept, and the flag telling us whether to normalize the problem.
This can be set up using Scikit-Learn’s <code class="highlighter-rouge">GridSearchCV</code> meta-estimator:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.grid_search</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s">'polynomialfeatures__degree'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">21</span><span class="p">),</span>
              <span class="s">'linearregression__fit_intercept'</span><span class="p">:</span> <span class="p">[</span><span class="bp">True</span><span class="p">,</span> <span class="bp">False</span><span class="p">],</span>
              <span class="s">'linearregression__normalize'</span><span class="p">:</span> <span class="p">[</span><span class="bp">True</span><span class="p">,</span> <span class="bp">False</span><span class="p">]}</span>

<span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">PolynomialRegression</span><span class="p">(),</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Notice that like a normal estimator, this has not yet been applied to any data.
Calling the <code class="highlighter-rouge">fit()</code> method will fit the model at each grid point, keeping track of the scores along the way:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Now that this is fit, we can ask for the best parameters as follows:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">grid</span><span class="o">.</span><span class="n">best_params_</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'linearregression__fit_intercept': False,
 'linearregression__normalize': True,
 'polynomialfeatures__degree': 4}
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>Finally, if we wish, we can use the best model and show the fit to our data using code from before:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">best_estimator_</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">y</span><span class="p">)</span>
<span class="n">lim</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">()</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">hold</span><span class="o">=</span><span class="bp">True</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="n">lim</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_187_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The grid search provides many more options, including the ability to specify a custom scoring function, to parallelize the computations, to do randomized searches, and more.
For information, see the examples in <a href="05.13-Kernel-Density-Estimation.ipynb">In-Depth: Kernel Density Estimation</a> and <a href="05.14-Image-Features.ipynb">Feature Engineering: Working with Images</a>, or refer to Scikit-Learn’s <a href="http://Scikit-Learn.org/stable/modules/grid_search.html">grid search documentation</a>.</p>

<h2 id="summary-2">Summary</h2>

<p>In this section, we have begun to explore the concept of model validation and hyperparameter optimization, focusing on intuitive aspects of the bias–variance trade-off and how it comes into play when fitting models to data.
In particular, we found that the use of a validation set or cross-validation approach is <em>vital</em> when tuning parameters in order to avoid over-fitting for more complex/flexible models.</p>

<p>In later sections, we will discuss the details of particularly useful models, and throughout will talk about what tuning is available for these models and how these free parameters affect model complexity.
Keep the lessons of this section in mind as you read on and learn about these machine learning approaches!</p>

<!--NAVIGATION-->
<p>&lt; <a href="05.02-Introducing-Scikit-Learn.ipynb">Introducing Scikit-Learn</a> | <a href="Index.ipynb">Contents</a> | <a href="05.04-Feature-Engineering.ipynb">Feature Engineering</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.03-Hyperparameters-and-Model-Validation.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<!--BOOK_INFORMATION-->
<p><img align="left" style="padding-right:10px;" src="figures/PDSH-cover-small.png" /></p>

<p><em>This notebook contains an excerpt from the <a href="http://shop.oreilly.com/product/0636920034919.do">Python Data Science Handbook</a> by Jake VanderPlas; the content is available <a href="https://github.com/jakevdp/PythonDataScienceHandbook">on GitHub</a>.</em></p>

<p><em>The text is released under the <a href="https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode">CC-BY-NC-ND license</a>, and code is released under the <a href="https://opensource.org/licenses/MIT">MIT license</a>. If you find this content useful, please consider supporting the work by <a href="http://shop.oreilly.com/product/0636920034919.do">buying the book</a>!</em></p>

<!--NAVIGATION-->
<p>&lt; <a href="05.03-Hyperparameters-and-Model-Validation.ipynb">Hyperparameters and Model Validation</a> | <a href="Index.ipynb">Contents</a> | <a href="05.05-Naive-Bayes.ipynb">In Depth: Naive Bayes Classification</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.04-Feature-Engineering.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<h1 id="feature-engineering">Feature Engineering</h1>

<p>The previous sections outline the fundamental ideas of machine learning, but all of the examples assume that you have numerical data in a tidy, <code class="highlighter-rouge">[n_samples, n_features]</code> format.
In the real world, data rarely comes in such a form.
With this in mind, one of the more important steps in using machine learning in practice is <em>feature engineering</em>: that is, taking whatever information you have about your problem and turning it into numbers that you can use to build your feature matrix.</p>

<p>In this section, we will cover a few common examples of feature engineering tasks: features for representing <em>categorical data</em>, features for representing <em>text</em>, and features for representing <em>images</em>.
Additionally, we will discuss <em>derived features</em> for increasing model complexity and <em>imputation</em> of missing data.
Often this process is known as <em>vectorization</em>, as it involves converting arbitrary data into well-behaved vectors.</p>

<h2 id="categorical-features">Categorical Features</h2>

<p>One common type of non-numerical data is <em>categorical</em> data.
For example, imagine you are exploring some data on housing prices, and along with numerical features like “price” and “rooms”, you also have “neighborhood” information.
For example, your data might look something like this:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s">'price'</span><span class="p">:</span> <span class="mi">850000</span><span class="p">,</span> <span class="s">'rooms'</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s">'neighborhood'</span><span class="p">:</span> <span class="s">'Queen Anne'</span><span class="p">},</span>
    <span class="p">{</span><span class="s">'price'</span><span class="p">:</span> <span class="mi">700000</span><span class="p">,</span> <span class="s">'rooms'</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s">'neighborhood'</span><span class="p">:</span> <span class="s">'Fremont'</span><span class="p">},</span>
    <span class="p">{</span><span class="s">'price'</span><span class="p">:</span> <span class="mi">650000</span><span class="p">,</span> <span class="s">'rooms'</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s">'neighborhood'</span><span class="p">:</span> <span class="s">'Wallingford'</span><span class="p">},</span>
    <span class="p">{</span><span class="s">'price'</span><span class="p">:</span> <span class="mi">600000</span><span class="p">,</span> <span class="s">'rooms'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s">'neighborhood'</span><span class="p">:</span> <span class="s">'Fremont'</span><span class="p">}</span>
<span class="p">]</span>
</code></pre></div>    </div>
  </div>

</div>

<p>You might be tempted to encode this data with a straightforward numerical mapping:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="s">'Queen Anne'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">'Fremont'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s">'Wallingford'</span><span class="p">:</span> <span class="mi">3</span><span class="p">};</span>
</code></pre></div>    </div>
  </div>

</div>

<p>It turns out that this is not generally a useful approach in Scikit-Learn: the package’s models make the fundamental assumption that numerical features reflect algebraic quantities.
Thus such a mapping would imply, for example, that <em>Queen Anne &lt; Fremont &lt; Wallingford</em>, or even that <em>Wallingford - Queen Anne = Fremont</em>, which (niche demographic jokes aside) does not make much sense.</p>

<p>In this case, one proven technique is to use <em>one-hot encoding</em>, which effectively creates extra columns indicating the presence or absence of a category with a value of 1 or 0, respectively.
When your data comes as a list of dictionaries, Scikit-Learn’s <code class="highlighter-rouge">DictVectorizer</code> will do this for you:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.feature_extraction</span> <span class="kn">import</span> <span class="n">DictVectorizer</span>
<span class="n">vec</span> <span class="o">=</span> <span class="n">DictVectorizer</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
<span class="n">vec</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[     0,      1,      0, 850000,      4],
       [     1,      0,      0, 700000,      3],
       [     0,      0,      1, 650000,      3],
       [     1,      0,      0, 600000,      2]], dtype=int64)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>Notice that the ‘neighborhood’ column has been expanded into three separate columns, representing the three neighborhood labels, and that each row has a 1 in the column associated with its neighborhood.
With these categorical features thus encoded, you can proceed as normal with fitting a Scikit-Learn model.</p>

<p>To see the meaning of each column, you can inspect the feature names:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">vec</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['neighborhood=Fremont',
 'neighborhood=Queen Anne',
 'neighborhood=Wallingford',
 'price',
 'rooms']
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>There is one clear disadvantage of this approach: if your category has many possible values, this can <em>greatly</em> increase the size of your dataset.
However, because the encoded data contains mostly zeros, a sparse output can be a very efficient solution:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">vec</span> <span class="o">=</span> <span class="n">DictVectorizer</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
<span class="n">vec</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;4x5 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
	with 12 stored elements in Compressed Sparse Row format&gt;
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>Many (though not yet all) of the Scikit-Learn estimators accept such sparse inputs when fitting and evaluating models. <code class="highlighter-rouge">sklearn.preprocessing.OneHotEncoder</code> and <code class="highlighter-rouge">sklearn.feature_extraction.FeatureHasher</code> are two additional tools that Scikit-Learn includes to support this type of encoding.</p>

<h2 id="text-features">Text Features</h2>

<p>Another common need in feature engineering is to convert text to a set of representative numerical values.
For example, most automatic mining of social media data relies on some form of encoding the text as numbers.
One of the simplest methods of encoding data is by <em>word counts</em>: you take each snippet of text, count the occurrences of each word within it, and put the results in a table.</p>

<p>For example, consider the following set of three phrases:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sample</span> <span class="o">=</span> <span class="p">[</span><span class="s">'problem of evil'</span><span class="p">,</span>
          <span class="s">'evil queen'</span><span class="p">,</span>
          <span class="s">'horizon problem'</span><span class="p">]</span>
</code></pre></div>    </div>
  </div>

</div>

<p>For a vectorization of this data based on word count, we could construct a column representing the word “problem,” the word “evil,” the word “horizon,” and so on.
While doing this by hand would be possible, the tedium can be avoided by using Scikit-Learn’s <code class="highlighter-rouge">CountVectorizer</code>:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>

<span class="n">vec</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vec</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
<span class="n">X</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;3x5 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
	with 7 stored elements in Compressed Sparse Row format&gt;
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>The result is a sparse matrix recording the number of times each word appears; it is easier to inspect if we convert this to a <code class="highlighter-rouge">DataFrame</code> with labeled columns:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="n">vec</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output output_html">
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>evil</th>
      <th>horizon</th>
      <th>of</th>
      <th>problem</th>
      <th>queen</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

    </div>
  </div>
</div>

<p>There are some issues with this approach, however: the raw word counts lead to features which put too much weight on words that appear very frequently, and this can be sub-optimal in some classification algorithms.
One approach to fix this is known as <em>term frequency-inverse document frequency</em> (<em>TF–IDF</em>) which weights the word counts by a measure of how often they appear in the documents.
The syntax for computing these features is similar to the previous example:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="n">vec</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vec</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="n">vec</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output output_html">
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>evil</th>
      <th>horizon</th>
      <th>of</th>
      <th>problem</th>
      <th>queen</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.517856</td>
      <td>0.000000</td>
      <td>0.680919</td>
      <td>0.517856</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.605349</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.795961</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.000000</td>
      <td>0.795961</td>
      <td>0.000000</td>
      <td>0.605349</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
</div>
</div>

    </div>
  </div>
</div>

<p>For an example of using TF-IDF in a classification problem, see <a href="05.05-Naive-Bayes.ipynb">In Depth: Naive Bayes Classification</a>.</p>

<h2 id="image-features">Image Features</h2>

<p>Another common need is to suitably encode <em>images</em> for machine learning analysis.
The simplest approach is what we used for the digits data in <a href="05.02-Introducing-Scikit-Learn.ipynb">Introducing Scikit-Learn</a>: simply using the pixel values themselves.
But depending on the application, such approaches may not be optimal.</p>

<p>A comprehensive summary of feature extraction techniques for images is well beyond the scope of this section, but you can find excellent implementations of many of the standard approaches in the <a href="http://scikit-image.org">Scikit-Image project</a>.
For one example of using Scikit-Learn and Scikit-Image together, see <a href="05.14-Image-Features.ipynb">Feature Engineering: Working with Images</a>.</p>

<h2 id="derived-features">Derived Features</h2>

<p>Another useful type of feature is one that is mathematically derived from some input features.
We saw an example of this in <a href="05.03-Hyperparameters-and-Model-Validation.ipynb">Hyperparameters and Model Validation</a> when we constructed <em>polynomial features</em> from our input data.
We saw that we could convert a linear regression into a polynomial regression not by changing the model, but by transforming the input!
This is sometimes known as <em>basis function regression</em>, and is explored further in <a href="05.06-Linear-Regression.ipynb">In Depth: Linear Regression</a>.</p>

<p>For example, this data clearly cannot be well described by a straight line:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_217_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Still, we can fit a line to the data using <code class="highlighter-rouge">LinearRegression</code> and get the optimal result:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">yfit</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">yfit</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_219_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>It’s clear that we need a more sophisticated model to describe the relationship between $x$ and $y$.</p>

<p>One approach to this is to transform the data, adding extra columns of features to drive more flexibility in the model.
For example, we can add polynomial features to the data this way:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">X2</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">
      <div class="output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[   1.    1.    1.]
 [   2.    4.    8.]
 [   3.    9.   27.]
 [   4.   16.   64.]
 [   5.   25.  125.]]
</code></pre></div>      </div>
    </div>
  </div>
</div>

<p>The derived feature matrix has one column representing $x$, and a second column representing $x^2$, and a third column representing $x^3$.
Computing a linear regression on this expanded input gives a much closer fit to our data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">yfit</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">yfit</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_223_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>This idea of improving a model not by changing the model, but by transforming the inputs, is fundamental to many of the more powerful machine learning methods.
We explore this idea further in <a href="05.06-Linear-Regression.ipynb">In Depth: Linear Regression</a> in the context of <em>basis function regression</em>.
More generally, this is one motivational path to the powerful set of techniques known as <em>kernel methods</em>, which we will explore in <a href="05.07-Support-Vector-Machines.ipynb">In-Depth: Support Vector Machines</a>.</p>

<h2 id="imputation-of-missing-data">Imputation of Missing Data</h2>

<p>Another common need in feature engineering is handling of missing data.
We discussed the handling of missing data in <code class="highlighter-rouge">DataFrame</code>s in <a href="03.04-Missing-Values.ipynb">Handling Missing Data</a>, and saw that often the <code class="highlighter-rouge">NaN</code> value is used to mark missing values.
For example, we might have a dataset that looks like this:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">nan</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="n">nan</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>   <span class="mi">3</span>  <span class="p">],</span>
              <span class="p">[</span> <span class="mi">3</span><span class="p">,</span>   <span class="mi">7</span><span class="p">,</span>   <span class="mi">9</span>  <span class="p">],</span>
              <span class="p">[</span> <span class="mi">3</span><span class="p">,</span>   <span class="mi">5</span><span class="p">,</span>   <span class="mi">2</span>  <span class="p">],</span>
              <span class="p">[</span> <span class="mi">4</span><span class="p">,</span>   <span class="n">nan</span><span class="p">,</span> <span class="mi">6</span>  <span class="p">],</span>
              <span class="p">[</span> <span class="mi">8</span><span class="p">,</span>   <span class="mi">8</span><span class="p">,</span>   <span class="mi">1</span>  <span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">14</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">])</span>
</code></pre></div>    </div>
  </div>

</div>

<p>When applying a typical machine learning model to such data, we will need to first replace such missing data with some appropriate fill value.
This is known as <em>imputation</em> of missing values, and strategies range from simple (e.g., replacing missing values with the mean of the column) to sophisticated (e.g., using matrix completion or a robust model to handle such data).</p>

<p>The sophisticated approaches tend to be very application-specific, and we won’t dive into them here.
For a baseline imputation approach, using the mean, median, or most frequent value, Scikit-Learn provides the <code class="highlighter-rouge">Imputer</code> class:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">Imputer</span>
<span class="n">imp</span> <span class="o">=</span> <span class="n">Imputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s">'mean'</span><span class="p">)</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">imp</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X2</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[ 4.5,  0. ,  3. ],
       [ 3. ,  7. ,  9. ],
       [ 3. ,  5. ,  2. ],
       [ 4. ,  5. ,  6. ],
       [ 8. ,  8. ,  1. ]])
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>We see that in the resulting data, the two missing values have been replaced with the mean of the remaining values in the column. This imputed data can then be fed directly into, for example, a <code class="highlighter-rouge">LinearRegression</code> estimator:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X2</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([ 13.14869292,  14.3784627 ,  -1.15539732,  10.96606197,  -5.33782027])
</code></pre></div>      </div>

    </div>
  </div>
</div>

<h2 id="feature-pipelines">Feature Pipelines</h2>

<p>With any of the preceding examples, it can quickly become tedious to do the transformations by hand, especially if you wish to string together multiple steps.
For example, we might want a processing pipeline that looks something like this:</p>

<ol>
  <li>Impute missing values using the mean</li>
  <li>Transform features to quadratic</li>
  <li>Fit a linear regression</li>
</ol>

<p>To streamline this type of processing pipeline, Scikit-Learn provides a <code class="highlighter-rouge">Pipeline</code> object, which can be used as follows:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">Imputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s">'mean'</span><span class="p">),</span>
                      <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
                      <span class="n">LinearRegression</span><span class="p">())</span>
</code></pre></div>    </div>
  </div>

</div>

<p>This pipeline looks and acts like a standard Scikit-Learn object, and will apply all the specified steps to any input data.</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c"># X with missing values, from above</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">
      <div class="output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[14 16 -1  8 -5]
[ 14.  16.  -1.   8.  -5.]
</code></pre></div>      </div>
    </div>
  </div>
</div>

<p>All the steps of the model are applied automatically.
Notice that for the simplicity of this demonstration, we’ve applied the model to the data it was trained on; this is why it was able to perfectly predict the result (refer back to <a href="05.03-Hyperparameters-and-Model-Validation.ipynb">Hyperparameters and Model Validation</a> for further discussion of this).</p>

<p>For some examples of Scikit-Learn pipelines in action, see the following section on naive Bayes classification, as well as <a href="05.06-Linear-Regression.ipynb">In Depth: Linear Regression</a>, and <a href="05.07-Support-Vector-Machines.ipynb">In-Depth: Support Vector Machines</a>.</p>

<!--NAVIGATION-->
<p>&lt; <a href="05.03-Hyperparameters-and-Model-Validation.ipynb">Hyperparameters and Model Validation</a> | <a href="Index.ipynb">Contents</a> | <a href="05.05-Naive-Bayes.ipynb">In Depth: Naive Bayes Classification</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.04-Feature-Engineering.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<!--BOOK_INFORMATION-->
<p><img align="left" style="padding-right:10px;" src="figures/PDSH-cover-small.png" /></p>

<p><em>This notebook contains an excerpt from the <a href="http://shop.oreilly.com/product/0636920034919.do">Python Data Science Handbook</a> by Jake VanderPlas; the content is available <a href="https://github.com/jakevdp/PythonDataScienceHandbook">on GitHub</a>.</em></p>

<p><em>The text is released under the <a href="https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode">CC-BY-NC-ND license</a>, and code is released under the <a href="https://opensource.org/licenses/MIT">MIT license</a>. If you find this content useful, please consider supporting the work by <a href="http://shop.oreilly.com/product/0636920034919.do">buying the book</a>!</em></p>

<!--NAVIGATION-->
<p>&lt; <a href="05.04-Feature-Engineering.ipynb">Feature Engineering</a> | <a href="Index.ipynb">Contents</a> | <a href="05.06-Linear-Regression.ipynb">In Depth: Linear Regression</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.05-Naive-Bayes.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<h1 id="in-depth-naive-bayes-classification">In Depth: Naive Bayes Classification</h1>

<p>The previous four sections have given a general overview of the concepts of machine learning.
In this section and the ones that follow, we will be taking a closer look at several specific algorithms for supervised and unsupervised learning, starting here with naive Bayes classification.</p>

<p>Naive Bayes models are a group of extremely fast and simple classification algorithms that are often suitable for very high-dimensional datasets.
Because they are so fast and have so few tunable parameters, they end up being very useful as a quick-and-dirty baseline for a classification problem.
This section will focus on an intuitive explanation of how naive Bayes classifiers work, followed by a couple examples of them in action on some datasets.</p>

<h2 id="bayesian-classification">Bayesian Classification</h2>

<p>Naive Bayes classifiers are built on Bayesian classification methods.
These rely on Bayes’s theorem, which is an equation describing the relationship of conditional probabilities of statistical quantities.
In Bayesian classification, we’re interested in finding the probability of a label given some observed features, which we can write as $P(L~|~{\rm features})$.
Bayes’s theorem tells us how to express this in terms of quantities we can compute more directly:</p>

<script type="math/tex; mode=display">P(L~|~{\rm features}) = \frac{P({\rm features}~|~L)P(L)}{P({\rm features})}</script>

<p>If we are trying to decide between two labels—let’s call them $L_1$ and $L_2$—then one way to make this decision is to compute the ratio of the posterior probabilities for each label:</p>

<script type="math/tex; mode=display">\frac{P(L_1~|~{\rm features})}{P(L_2~|~{\rm features})} = \frac{P({\rm features}~|~L_1)}{P({\rm features}~|~L_2)}\frac{P(L_1)}{P(L_2)}</script>

<p>All we need now is some model by which we can compute $P({\rm features}~|~L_i)$ for each label.
Such a model is called a <em>generative model</em> because it specifies the hypothetical random process that generates the data.
Specifying this generative model for each label is the main piece of the training of such a Bayesian classifier.
The general version of such a training step is a very difficult task, but we can make it simpler through the use of some simplifying assumptions about the form of this model.</p>

<p>This is where the “naive” in “naive Bayes” comes in: if we make very naive assumptions about the generative model for each label, we can find a rough approximation of the generative model for each class, and then proceed with the Bayesian classification.
Different types of naive Bayes classifiers rest on different naive assumptions about the data, and we will examine a few of these in the following sections.</p>

<p>We begin with the standard imports:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span><span class="p">;</span> <span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">()</span>
</code></pre></div>    </div>
  </div>

</div>

<h2 id="gaussian-naive-bayes">Gaussian Naive Bayes</h2>

<p>Perhaps the easiest naive Bayes classifier to understand is Gaussian naive Bayes.
In this classifier, the assumption is that <em>data from each label is drawn from a simple Gaussian distribution</em>.
Imagine that you have the following data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'RdBu'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_244_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>One extremely fast way to create a simple model is to assume that the data is described by a Gaussian distribution with no covariance between dimensions.
This model can be fit by simply finding the mean and standard deviation of the points within each label, which is all you need to define such a distribution.
The result of this naive Gaussian assumption is shown in the following figure:</p>

<p><img src="figures/05.05-gaussian-NB.png" alt="(run code in Appendix to generate image)" />
<a href="06.00-Figure-Code.ipynb#Gaussian-Naive-Bayes">figure source in Appendix</a></p>

<p>The ellipses here represent the Gaussian generative model for each label, with larger probability toward the center of the ellipses.
With this generative model in place for each class, we have a simple recipe to compute the likelihood $P({\rm features}~|~L_1)$ for any data point, and thus we can quickly compute the posterior ratio and determine which label is the most probable for a given point.</p>

<p>This procedure is implemented in Scikit-Learn’s <code class="highlighter-rouge">sklearn.naive_bayes.GaussianNB</code> estimator:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Now let’s generate some new data and predict the label:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">Xnew</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">14</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">14</span><span class="p">,</span> <span class="mi">18</span><span class="p">]</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">ynew</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xnew</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Now we can plot this new data to get an idea of where the decision boundary is:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'RdBu'</span><span class="p">)</span>
<span class="n">lim</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Xnew</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">Xnew</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">ynew</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'RdBu'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="n">lim</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_252_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>We see a slightly curved boundary in the classifications—in general, the boundary in Gaussian naive Bayes is quadratic.</p>

<p>A nice piece of this Bayesian formalism is that it naturally allows for probabilistic classification, which we can compute using the <code class="highlighter-rouge">predict_proba</code> method:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">yprob</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">Xnew</span><span class="p">)</span>
<span class="n">yprob</span><span class="p">[</span><span class="o">-</span><span class="mi">8</span><span class="p">:]</span><span class="o">.</span><span class="nb">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[ 0.89,  0.11],
       [ 1.  ,  0.  ],
       [ 1.  ,  0.  ],
       [ 1.  ,  0.  ],
       [ 1.  ,  0.  ],
       [ 1.  ,  0.  ],
       [ 0.  ,  1.  ],
       [ 0.15,  0.85]])
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>The columns give the posterior probabilities of the first and second label, respectively.
If you are looking for estimates of uncertainty in your classification, Bayesian approaches like this can be a useful approach.</p>

<p>Of course, the final classification will only be as good as the model assumptions that lead to it, which is why Gaussian naive Bayes often does not produce very good results.
Still, in many cases—especially as the number of features becomes large—this assumption is not detrimental enough to prevent Gaussian naive Bayes from being a useful method.</p>

<h2 id="multinomial-naive-bayes">Multinomial Naive Bayes</h2>

<p>The Gaussian assumption just described is by no means the only simple assumption that could be used to specify the generative distribution for each label.
Another useful example is multinomial naive Bayes, where the features are assumed to be generated from a simple multinomial distribution.
The multinomial distribution describes the probability of observing counts among a number of categories, and thus multinomial naive Bayes is most appropriate for features that represent counts or count rates.</p>

<p>The idea is precisely the same as before, except that instead of modeling the data distribution with the best-fit Gaussian, we model the data distribuiton with a best-fit multinomial distribution.</p>

<h3 id="example-classifying-text">Example: Classifying Text</h3>

<p>One place where multinomial naive Bayes is often used is in text classification, where the features are related to word counts or frequencies within the documents to be classified.
We discussed the extraction of such features from text in <a href="05.04-Feature-Engineering.ipynb">Feature Engineering</a>; here we will use the sparse word count features from the 20 Newsgroups corpus to show how we might classify these short documents into categories.</p>

<p>Let’s download the data and take a look at the target names:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">()</span>
<span class="n">data</span><span class="o">.</span><span class="n">target_names</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['alt.atheism',
 'comp.graphics',
 'comp.os.ms-windows.misc',
 'comp.sys.ibm.pc.hardware',
 'comp.sys.mac.hardware',
 'comp.windows.x',
 'misc.forsale',
 'rec.autos',
 'rec.motorcycles',
 'rec.sport.baseball',
 'rec.sport.hockey',
 'sci.crypt',
 'sci.electronics',
 'sci.med',
 'sci.space',
 'soc.religion.christian',
 'talk.politics.guns',
 'talk.politics.mideast',
 'talk.politics.misc',
 'talk.religion.misc']
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>For simplicity here, we will select just a few of these categories, and download the training and testing set:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">categories</span> <span class="o">=</span> <span class="p">[</span><span class="s">'talk.religion.misc'</span><span class="p">,</span> <span class="s">'soc.religion.christian'</span><span class="p">,</span>
              <span class="s">'sci.space'</span><span class="p">,</span> <span class="s">'comp.graphics'</span><span class="p">]</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s">'train'</span><span class="p">,</span> <span class="n">categories</span><span class="o">=</span><span class="n">categories</span><span class="p">)</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s">'test'</span><span class="p">,</span> <span class="n">categories</span><span class="o">=</span><span class="n">categories</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Here is a representative entry from the data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">train</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">5</span><span class="p">])</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">
      <div class="output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>From: dmcgee@uluhe.soest.hawaii.edu (Don McGee)
Subject: Federal Hearing
Originator: dmcgee@uluhe
Organization: School of Ocean and Earth Science and Technology
Distribution: usa
Lines: 10


Fact or rumor....?  Madalyn Murray O'Hare an atheist who eliminated the
use of the bible reading and prayer in public schools 15 years ago is now
going to appear before the FCC with a petition to stop the reading of the
Gospel on the airways of America.  And she is also campaigning to remove
Christmas programs, songs, etc from the public schools.  If it is true
then mail to Federal Communications Commission 1919 H Street Washington DC
20054 expressing your opposition to her request.  Reference Petition number

2493.

</code></pre></div>      </div>
    </div>
  </div>
</div>

<p>In order to use this data for machine learning, we need to be able to convert the content of each string into a vector of numbers.
For this we will use the TF-IDF vectorizer (discussed in <a href="05.04-Feature-Engineering.ipynb">Feature Engineering</a>), and create a pipeline that attaches it to a multinomial naive Bayes classifier:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">TfidfVectorizer</span><span class="p">(),</span> <span class="n">MultinomialNB</span><span class="p">())</span>
</code></pre></div>    </div>
  </div>

</div>

<p>With this pipeline, we can apply the model to the training data, and predict labels for the test data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">train</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Now that we have predicted the labels for the test data, we can evaluate them to learn about the performance of the estimator.
For example, here is the confusion matrix between the true and predicted labels for the test data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="n">mat</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">test</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">mat</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s">'d'</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
            <span class="n">xticklabels</span><span class="o">=</span><span class="n">train</span><span class="o">.</span><span class="n">target_names</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="n">train</span><span class="o">.</span><span class="n">target_names</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'true label'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'predicted label'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_268_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Evidently, even this very simple classifier can successfully separate space talk from computer talk, but it gets confused between talk about religion and talk about Christianity.
This is perhaps an expected area of confusion!</p>

<p>The very cool thing here is that we now have the tools to determine the category for <em>any</em> string, using the <code class="highlighter-rouge">predict()</code> method of this pipeline.
Here’s a quick utility function that will return the prediction for a single string:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">predict_category</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">):</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">s</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">train</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">pred</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Let’s try it out:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">predict_category</span><span class="p">(</span><span class="s">'sending a payload to the ISS'</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'sci.space'
</code></pre></div>      </div>

    </div>
  </div>
</div>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">predict_category</span><span class="p">(</span><span class="s">'discussing islam vs atheism'</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'soc.religion.christian'
</code></pre></div>      </div>

    </div>
  </div>
</div>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">predict_category</span><span class="p">(</span><span class="s">'determining the screen resolution'</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'comp.graphics'
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>Remember that this is nothing more sophisticated than a simple probability model for the (weighted) frequency of each word in the string; nevertheless, the result is striking.
Even a very naive algorithm, when used carefully and trained on a large set of high-dimensional data, can be surprisingly effective.</p>

<h2 id="when-to-use-naive-bayes">When to Use Naive Bayes</h2>

<p>Because naive Bayesian classifiers make such stringent assumptions about data, they will generally not perform as well as a more complicated model.
That said, they have several advantages:</p>

<ul>
  <li>They are extremely fast for both training and prediction</li>
  <li>They provide straightforward probabilistic prediction</li>
  <li>They are often very easily interpretable</li>
  <li>They have very few (if any) tunable parameters</li>
</ul>

<p>These advantages mean a naive Bayesian classifier is often a good choice as an initial baseline classification.
If it performs suitably, then congratulations: you have a very fast, very interpretable classifier for your problem.
If it does not perform well, then you can begin exploring more sophisticated models, with some baseline knowledge of how well they should perform.</p>

<p>Naive Bayes classifiers tend to perform especially well in one of the following situations:</p>

<ul>
  <li>When the naive assumptions actually match the data (very rare in practice)</li>
  <li>For very well-separated categories, when model complexity is less important</li>
  <li>For very high-dimensional data, when model complexity is less important</li>
</ul>

<p>The last two points seem distinct, but they actually are related: as the dimension of a dataset grows, it is much less likely for any two points to be found close together (after all, they must be close in <em>every single dimension</em> to be close overall).
This means that clusters in high dimensions tend to be more separated, on average, than clusters in low dimensions, assuming the new dimensions actually add information.
For this reason, simplistic classifiers like naive Bayes tend to work as well or better than more complicated classifiers as the dimensionality grows: once you have enough data, even a simple model can be very powerful.</p>

<!--NAVIGATION-->
<p>&lt; <a href="05.04-Feature-Engineering.ipynb">Feature Engineering</a> | <a href="Index.ipynb">Contents</a> | <a href="05.06-Linear-Regression.ipynb">In Depth: Linear Regression</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.05-Naive-Bayes.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<!--BOOK_INFORMATION-->
<p><img align="left" style="padding-right:10px;" src="figures/PDSH-cover-small.png" /></p>

<p><em>This notebook contains an excerpt from the <a href="http://shop.oreilly.com/product/0636920034919.do">Python Data Science Handbook</a> by Jake VanderPlas; the content is available <a href="https://github.com/jakevdp/PythonDataScienceHandbook">on GitHub</a>.</em></p>

<p><em>The text is released under the <a href="https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode">CC-BY-NC-ND license</a>, and code is released under the <a href="https://opensource.org/licenses/MIT">MIT license</a>. If you find this content useful, please consider supporting the work by <a href="http://shop.oreilly.com/product/0636920034919.do">buying the book</a>!</em></p>

<!--NAVIGATION-->
<p>&lt; <a href="05.05-Naive-Bayes.ipynb">In Depth: Naive Bayes Classification</a> | <a href="Index.ipynb">Contents</a> | <a href="05.07-Support-Vector-Machines.ipynb">In-Depth: Support Vector Machines</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.06-Linear-Regression.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<h1 id="in-depth-linear-regression">In Depth: Linear Regression</h1>

<p>Just as naive Bayes (discussed earlier in <a href="05.05-Naive-Bayes.ipynb">In Depth: Naive Bayes Classification</a>) is a good starting point for classification tasks, linear regression models are a good starting point for regression tasks.
Such models are popular because they can be fit very quickly, and are very interpretable.
You are probably familiar with the simplest form of a linear regression model (i.e., fitting a straight line to data) but such models can be extended to model more complicated data behavior.</p>

<p>In this section we will start with a quick intuitive walk-through of the mathematics behind this well-known problem, before seeing how before moving on to see how linear models can be generalized to account for more complicated patterns in data.</p>

<p>We begin with the standard imports:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span><span class="p">;</span> <span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">()</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
</code></pre></div>    </div>
  </div>

</div>

<h2 id="simple-linear-regression">Simple Linear Regression</h2>

<p>We will start with the most familiar linear regression, a straight-line fit to data.
A straight-line fit is a model of the form
<script type="math/tex">y = ax + b</script>
where $a$ is commonly known as the <em>slope</em>, and $b$ is commonly known as the <em>intercept</em>.</p>

<p>Consider the following data, which is scattered about a line with a slope of 2 and an intercept of -5:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">5</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_284_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>We can use Scikit-Learn’s <code class="highlighter-rouge">LinearRegression</code> estimator to fit this data and construct the best-fit line:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>

<span class="n">xfit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">yfit</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xfit</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xfit</span><span class="p">,</span> <span class="n">yfit</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_286_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The slope and intercept of the data are contained in the model’s fit parameters, which in Scikit-Learn are always marked by a trailing underscore.
Here the relevant parameters are <code class="highlighter-rouge">coef_</code> and <code class="highlighter-rouge">intercept_</code>:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"Model slope:    "</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Model intercept:"</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">
      <div class="output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model slope:     2.02720881036
Model intercept: -4.99857708555
</code></pre></div>      </div>
    </div>
  </div>
</div>

<p>We see that the results are very close to the inputs, as we might hope.</p>

<p>The <code class="highlighter-rouge">LinearRegression</code> estimator is much more capable than this, however—in addition to simple straight-line fits, it can also handle multidimensional linear models of the form
<script type="math/tex">y = a_0 + a_1 x_1 + a_2 x_2 + \cdots</script>
where there are multiple $x$ values.
Geometrically, this is akin to fitting a plane to points in three dimensions, or fitting a hyper-plane to points in higher dimensions.</p>

<p>The multidimensional nature of such regressions makes them more difficult to visualize, but we can see one of these fits in action by building some example data, using NumPy’s matrix multiplication operator:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">
      <div class="output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.5
[ 1.5 -2.   1. ]
</code></pre></div>      </div>
    </div>
  </div>
</div>

<p>Here the $y$ data is constructed from three random $x$ values, and the linear regression recovers the coefficients used to construct the data.</p>

<p>In this way, we can use the single <code class="highlighter-rouge">LinearRegression</code> estimator to fit lines, planes, or hyperplanes to our data.
It still appears that this approach would be limited to strictly linear relationships between variables, but it turns out we can relax this as well.</p>

<h2 id="basis-function-regression">Basis Function Regression</h2>

<p>One trick you can use to adapt linear regression to nonlinear relationships between variables is to transform the data according to <em>basis functions</em>.
We have seen one version of this before, in the <code class="highlighter-rouge">PolynomialRegression</code> pipeline used in <a href="05.03-Hyperparameters-and-Model-Validation.ipynb">Hyperparameters and Model Validation</a> and <a href="05.04-Feature-Engineering.ipynb">Feature Engineering</a>.
The idea is to take our multidimensional linear model:
<script type="math/tex">y = a_0 + a_1 x_1 + a_2 x_2 + a_3 x_3 + \cdots</script>
and build the $x_1, x_2, x_3,$ and so on, from our single-dimensional input $x$.
That is, we let $x_n = f_n(x)$, where $f_n()$ is some function that transforms our data.</p>

<p>For example, if $f_n(x) = x^n$, our model becomes a polynomial regression:
<script type="math/tex">y = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + \cdots</script>
Notice that this is <em>still a linear model</em>—the linearity refers to the fact that the coefficients $a_n$ never multiply or divide each other.
What we have effectively done is taken our one-dimensional $x$ values and projected them into a higher dimension, so that a linear fit can fit more complicated relationships between $x$ and $y$.</p>

<h3 id="polynomial-basis-functions">Polynomial basis functions</h3>

<p>This polynomial projection is useful enough that it is built into Scikit-Learn, using the <code class="highlighter-rouge">PolynomialFeatures</code> transformer:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">])</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[  2.,   4.,   8.],
       [  3.,   9.,  27.],
       [  4.,  16.,  64.]])
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>We see here that the transformer has converted our one-dimensional array into a three-dimensional array by taking the exponent of each value.
This new, higher-dimensional data representation can then be plugged into a linear regression.</p>

<p>As we saw in <a href="05.04-Feature-Engineering.ipynb">Feature Engineering</a>, the cleanest way to accomplish this is to use a pipeline.
Let’s make a 7th-degree polynomial model in this way:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="n">poly_model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">7</span><span class="p">),</span>
                           <span class="n">LinearRegression</span><span class="p">())</span>
</code></pre></div>    </div>
  </div>

</div>

<p>With this transform in place, we can use the linear model to fit much more complicated relationships between $x$ and $y$. 
For example, here is a sine wave with noise:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>

<span class="n">poly_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">yfit</span> <span class="o">=</span> <span class="n">poly_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xfit</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xfit</span><span class="p">,</span> <span class="n">yfit</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_299_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Our linear model, through the use of 7th-order polynomial basis functions, can provide an excellent fit to this non-linear data!</p>

<h3 id="gaussian-basis-functions">Gaussian basis functions</h3>

<p>Of course, other basis functions are possible.
For example, one useful pattern is to fit a model that is not a sum of polynomial bases, but a sum of Gaussian bases.
The result might look something like the following figure:</p>

<p><img src="figures/05.06-gaussian-basis.png" alt="" />
<a href="#Gaussian-Basis">figure source in Appendix</a></p>

<p>The shaded regions in the plot are the scaled basis functions, and when added together they reproduce the smooth curve through the data.
These Gaussian basis functions are not built into Scikit-Learn, but we can write a custom transformer that will create them, as shown here and illustrated in the following figure (Scikit-Learn transformers are implemented as Python classes; reading Scikit-Learn’s source is a good way to see how they can be created):</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span>

<span class="k">class</span> <span class="nc">GaussianFeatures</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="s">"""Uniformly spaced Gaussian features for one-dimensional input"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">width_factor</span><span class="o">=</span><span class="mf">2.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N</span> <span class="o">=</span> <span class="n">N</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">width_factor</span> <span class="o">=</span> <span class="n">width_factor</span>
    
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_gauss_basis</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">arg</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">width</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">arg</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="p">))</span>
        
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c"># create N centers spread along the data range</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">centers_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">X</span><span class="o">.</span><span class="nb">max</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">width_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">width_factor</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">centers_</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">centers_</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">return</span> <span class="bp">self</span>
        
    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gauss_basis</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">centers_</span><span class="p">,</span>
                                 <span class="bp">self</span><span class="o">.</span><span class="n">width_</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
<span class="n">gauss_model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">GaussianFeatures</span><span class="p">(</span><span class="mi">20</span><span class="p">),</span>
                            <span class="n">LinearRegression</span><span class="p">())</span>
<span class="n">gauss_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">yfit</span> <span class="o">=</span> <span class="n">gauss_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xfit</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xfit</span><span class="p">,</span> <span class="n">yfit</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_304_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>We put this example here just to make clear that there is nothing magic about polynomial basis functions: if you have some sort of intuition into the generating process of your data that makes you think one basis or another might be appropriate, you can use them as well.</p>

<h2 id="regularization">Regularization</h2>

<p>The introduction of basis functions into our linear regression makes the model much more flexible, but it also can very quickly lead to over-fitting (refer back to <a href="05.03-Hyperparameters-and-Model-Validation.ipynb">Hyperparameters and Model Validation</a> for a discussion of this).
For example, if we choose too many Gaussian basis functions, we end up with results that don’t look so good:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">GaussianFeatures</span><span class="p">(</span><span class="mi">30</span><span class="p">),</span>
                      <span class="n">LinearRegression</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xfit</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xfit</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_307_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>With the data projected to the 30-dimensional basis, the model has far too much flexibility and goes to extreme values between locations where it is constrained by data.
We can see the reason for this if we plot the coefficients of the Gaussian bases with respect to their locations:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">basis_plot</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xfit</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xfit</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]))</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s">'x'</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s">'y'</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">))</span>
    
    <span class="k">if</span> <span class="n">title</span><span class="p">:</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>

    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">steps</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">centers_</span><span class="p">,</span>
               <span class="n">model</span><span class="o">.</span><span class="n">steps</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s">'basis location'</span><span class="p">,</span>
              <span class="n">ylabel</span><span class="o">=</span><span class="s">'coefficient'</span><span class="p">,</span>
              <span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    
<span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">GaussianFeatures</span><span class="p">(</span><span class="mi">30</span><span class="p">),</span> <span class="n">LinearRegression</span><span class="p">())</span>
<span class="n">basis_plot</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_309_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The lower panel of this figure shows the amplitude of the basis function at each location.
This is typical over-fitting behavior when basis functions overlap: the coefficients of adjacent basis functions blow up and cancel each other out.
We know that such behavior is problematic, and it would be nice if we could limit such spikes expliticly in the model by penalizing large values of the model parameters.
Such a penalty is known as <em>regularization</em>, and comes in several forms.</p>

<h3 id="ridge-regression-l_2-regularization">Ridge regression ($L_2$ Regularization)</h3>

<p>Perhaps the most common form of regularization is known as <em>ridge regression</em> or $L_2$ <em>regularization</em>, sometimes also called <em>Tikhonov regularization</em>.
This proceeds by penalizing the sum of squares (2-norms) of the model coefficients; in this case, the penalty on the model fit would be 
<script type="math/tex">P = \alpha\sum_{n=1}^N \theta_n^2</script>
where $\alpha$ is a free parameter that controls the strength of the penalty.
This type of penalized model is built into Scikit-Learn with the <code class="highlighter-rouge">Ridge</code> estimator:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">GaussianFeatures</span><span class="p">(</span><span class="mi">30</span><span class="p">),</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>
<span class="n">basis_plot</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s">'Ridge Regression'</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_312_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The $\alpha$ parameter is essentially a knob controlling the complexity of the resulting model.
In the limit $\alpha \to 0$, we recover the standard linear regression result; in the limit $\alpha \to \infty$, all model responses will be suppressed.
One advantage of ridge regression in particular is that it can be computed very efficiently—at hardly more computational cost than the original linear regression model.</p>

<h3 id="lasso-regression-l_1-regularization">Lasso regression ($L_1$ regularization)</h3>

<p>Another very common type of regularization is known as lasso, and involves penalizing the sum of absolute values (1-norms) of regression coefficients:
<script type="math/tex">P = \alpha\sum_{n=1}^N |\theta_n|</script>
Though this is conceptually very similar to ridge regression, the results can differ surprisingly: for example, due to geometric reasons lasso regression tends to favor <em>sparse models</em> where possible: that is, it preferentially sets model coefficients to exactly zero.</p>

<p>We can see this behavior in duplicating the ridge regression figure, but using L1-normalized coefficients:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">GaussianFeatures</span><span class="p">(</span><span class="mi">30</span><span class="p">),</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.001</span><span class="p">))</span>
<span class="n">basis_plot</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s">'Lasso Regression'</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_315_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>With the lasso regression penalty, the majority of the coefficients are exactly zero, with the functional behavior being modeled by a small subset of the available basis functions.
As with ridge regularization, the $\alpha$ parameter tunes the strength of the penalty, and should be determined via, for example, cross-validation (refer back to <a href="05.03-Hyperparameters-and-Model-Validation.ipynb">Hyperparameters and Model Validation</a> for a discussion of this).</p>

<h2 id="example-predicting-bicycle-traffic">Example: Predicting Bicycle Traffic</h2>

<p>As an example, let’s take a look at whether we can predict the number of bicycle trips across Seattle’s Fremont Bridge based on weather, season, and other factors.
We have seen this data already in <a href="03.11-Working-with-Time-Series.ipynb">Working With Time Series</a>.</p>

<p>In this section, we will join the bike data with another dataset, and try to determine the extent to which weather and seasonal factors—temperature, precipitation, and daylight hours—affect the volume of bicycle traffic through this corridor.
Fortunately, the NOAA makes available their daily <a href="http://www.ncdc.noaa.gov/cdo-web/search?datasetid=GHCND">weather station data</a> (I used station ID USW00024233) and we can easily use Pandas to join the two data sources.
We will perform a simple linear regression to relate weather and other information to bicycle counts, in order to estimate how a change in any one of these parameters affects the number of riders on a given day.</p>

<p>In particular, this is an example of how the tools of Scikit-Learn can be used in a statistical modeling framework, in which the parameters of the model are assumed to have interpretable meaning.
As discussed previously, this is not a standard approach within machine learning, but such interpretation is possible for some models.</p>

<p>Let’s start by loading the two datasets, indexing by date:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># !curl -o FremontBridge.csv https://data.seattle.gov/api/views/65db-xm6k/rows.csv?accessType=DOWNLOAD</span>
</code></pre></div>    </div>
  </div>

</div>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'FremontBridge.csv'</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s">'Date'</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">weather</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'data/BicycleWeather.csv'</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s">'DATE'</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Next we will compute the total daily bicycle traffic, and put this in its own dataframe:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">daily</span> <span class="o">=</span> <span class="n">counts</span><span class="o">.</span><span class="n">resample</span><span class="p">(</span><span class="s">'d'</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
<span class="n">daily</span><span class="p">[</span><span class="s">'Total'</span><span class="p">]</span> <span class="o">=</span> <span class="n">daily</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">daily</span> <span class="o">=</span> <span class="n">daily</span><span class="p">[[</span><span class="s">'Total'</span><span class="p">]]</span> <span class="c"># remove other columns</span>
</code></pre></div>    </div>
  </div>

</div>

<p>We saw previously that the patterns of use generally vary from day to day; let’s account for this in our data by adding binary columns that indicate the day of the week:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">days</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Mon'</span><span class="p">,</span> <span class="s">'Tue'</span><span class="p">,</span> <span class="s">'Wed'</span><span class="p">,</span> <span class="s">'Thu'</span><span class="p">,</span> <span class="s">'Fri'</span><span class="p">,</span> <span class="s">'Sat'</span><span class="p">,</span> <span class="s">'Sun'</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">7</span><span class="p">):</span>
    <span class="n">daily</span><span class="p">[</span><span class="n">days</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(</span><span class="n">daily</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">dayofweek</span> <span class="o">==</span> <span class="n">i</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Similarly, we might expect riders to behave differently on holidays; let’s add an indicator of this as well:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pandas.tseries.holiday</span> <span class="kn">import</span> <span class="n">USFederalHolidayCalendar</span>
<span class="n">cal</span> <span class="o">=</span> <span class="n">USFederalHolidayCalendar</span><span class="p">()</span>
<span class="n">holidays</span> <span class="o">=</span> <span class="n">cal</span><span class="o">.</span><span class="n">holidays</span><span class="p">(</span><span class="s">'2012'</span><span class="p">,</span> <span class="s">'2016'</span><span class="p">)</span>
<span class="n">daily</span> <span class="o">=</span> <span class="n">daily</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">holidays</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'holiday'</span><span class="p">))</span>
<span class="n">daily</span><span class="p">[</span><span class="s">'holiday'</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>We also might suspect that the hours of daylight would affect how many people ride; let’s use the standard astronomical calculation to add this information:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">hours_of_daylight</span><span class="p">(</span><span class="n">date</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mf">23.44</span><span class="p">,</span> <span class="n">latitude</span><span class="o">=</span><span class="mf">47.61</span><span class="p">):</span>
    <span class="s">"""Compute the hours of daylight for the given date"""</span>
    <span class="n">days</span> <span class="o">=</span> <span class="p">(</span><span class="n">date</span> <span class="o">-</span> <span class="n">pd</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">21</span><span class="p">))</span><span class="o">.</span><span class="n">days</span>
    <span class="n">m</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">tan</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">radians</span><span class="p">(</span><span class="n">latitude</span><span class="p">))</span>
         <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">tan</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">radians</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">days</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mf">365.25</span><span class="p">)))</span>
    <span class="k">return</span> <span class="mf">24.</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">degrees</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arccos</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span> <span class="o">/</span> <span class="mf">180.</span>

<span class="n">daily</span><span class="p">[</span><span class="s">'daylight_hrs'</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">hours_of_daylight</span><span class="p">,</span> <span class="n">daily</span><span class="o">.</span><span class="n">index</span><span class="p">))</span>
<span class="n">daily</span><span class="p">[[</span><span class="s">'daylight_hrs'</span><span class="p">]]</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">17</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(8, 17)
</code></pre></div>      </div>

    </div>
  </div>
  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_328_1.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>We can also add the average temperature and total precipitation to the data.
In addition to the inches of precipitation, let’s add a flag that indicates whether a day is dry (has zero precipitation):</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># temperatures are in 1/10 deg C; convert to C</span>
<span class="n">weather</span><span class="p">[</span><span class="s">'TMIN'</span><span class="p">]</span> <span class="o">/=</span> <span class="mi">10</span>
<span class="n">weather</span><span class="p">[</span><span class="s">'TMAX'</span><span class="p">]</span> <span class="o">/=</span> <span class="mi">10</span>
<span class="n">weather</span><span class="p">[</span><span class="s">'Temp (C)'</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">weather</span><span class="p">[</span><span class="s">'TMIN'</span><span class="p">]</span> <span class="o">+</span> <span class="n">weather</span><span class="p">[</span><span class="s">'TMAX'</span><span class="p">])</span>

<span class="c"># precip is in 1/10 mm; convert to inches</span>
<span class="n">weather</span><span class="p">[</span><span class="s">'PRCP'</span><span class="p">]</span> <span class="o">/=</span> <span class="mi">254</span>
<span class="n">weather</span><span class="p">[</span><span class="s">'dry day'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">weather</span><span class="p">[</span><span class="s">'PRCP'</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="n">daily</span> <span class="o">=</span> <span class="n">daily</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">weather</span><span class="p">[[</span><span class="s">'PRCP'</span><span class="p">,</span> <span class="s">'Temp (C)'</span><span class="p">,</span> <span class="s">'dry day'</span><span class="p">]])</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Finally, let’s add a counter that increases from day 1, and measures how many years have passed.
This will let us measure any observed annual increase or decrease in daily crossings:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">daily</span><span class="p">[</span><span class="s">'annual'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">daily</span><span class="o">.</span><span class="n">index</span> <span class="o">-</span> <span class="n">daily</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">days</span> <span class="o">/</span> <span class="mf">365.</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Now our data is in order, and we can take a look at it:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">daily</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output output_html">
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Total</th>
      <th>Mon</th>
      <th>Tue</th>
      <th>Wed</th>
      <th>Thu</th>
      <th>Fri</th>
      <th>Sat</th>
      <th>Sun</th>
      <th>holiday</th>
      <th>daylight_hrs</th>
      <th>PRCP</th>
      <th>Temp (C)</th>
      <th>dry day</th>
      <th>annual</th>
    </tr>
    <tr>
      <th>Date</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2012-10-03</th>
      <td>3521.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>11.277359</td>
      <td>0.0</td>
      <td>13.35</td>
      <td>1.0</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2012-10-04</th>
      <td>3475.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>11.219142</td>
      <td>0.0</td>
      <td>13.60</td>
      <td>1.0</td>
      <td>0.002740</td>
    </tr>
    <tr>
      <th>2012-10-05</th>
      <td>3148.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>11.161038</td>
      <td>0.0</td>
      <td>15.30</td>
      <td>1.0</td>
      <td>0.005479</td>
    </tr>
    <tr>
      <th>2012-10-06</th>
      <td>2006.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>11.103056</td>
      <td>0.0</td>
      <td>15.85</td>
      <td>1.0</td>
      <td>0.008219</td>
    </tr>
    <tr>
      <th>2012-10-07</th>
      <td>2142.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>11.045208</td>
      <td>0.0</td>
      <td>15.85</td>
      <td>1.0</td>
      <td>0.010959</td>
    </tr>
  </tbody>
</table>
</div>
</div>

    </div>
  </div>
</div>

<p>With this in place, we can choose the columns to use, and fit a linear regression model to our data.
We will set <code class="highlighter-rouge">fit_intercept = False</code>, because the daily flags essentially operate as their own day-specific intercepts:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Drop any rows with null values</span>
<span class="n">daily</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s">'any'</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">column_names</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Mon'</span><span class="p">,</span> <span class="s">'Tue'</span><span class="p">,</span> <span class="s">'Wed'</span><span class="p">,</span> <span class="s">'Thu'</span><span class="p">,</span> <span class="s">'Fri'</span><span class="p">,</span> <span class="s">'Sat'</span><span class="p">,</span> <span class="s">'Sun'</span><span class="p">,</span> <span class="s">'holiday'</span><span class="p">,</span>
                <span class="s">'daylight_hrs'</span><span class="p">,</span> <span class="s">'PRCP'</span><span class="p">,</span> <span class="s">'dry day'</span><span class="p">,</span> <span class="s">'Temp (C)'</span><span class="p">,</span> <span class="s">'annual'</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">daily</span><span class="p">[</span><span class="n">column_names</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">daily</span><span class="p">[</span><span class="s">'Total'</span><span class="p">]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">daily</span><span class="p">[</span><span class="s">'predicted'</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Finally, we can compare the total and predicted bicycle traffic visually:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">daily</span><span class="p">[[</span><span class="s">'Total'</span><span class="p">,</span> <span class="s">'predicted'</span><span class="p">]]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_338_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>It is evident that we have missed some key features, especially during the summer time.
Either our features are not complete (i.e., people decide whether to ride to work based on more than just these) or there are some nonlinear relationships that we have failed to take into account (e.g., perhaps people ride less at both high and low temperatures).
Nevertheless, our rough approximation is enough to give us some insights, and we can take a look at the coefficients of the linear model to estimate how much each feature contributes to the daily bicycle count:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">params</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">params</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Mon              504.882756
Tue              610.233936
Wed              592.673642
Thu              482.358115
Fri              177.980345
Sat            -1103.301710
Sun            -1133.567246
holiday        -1187.401381
daylight_hrs     128.851511
PRCP            -664.834882
dry day          547.698592
Temp (C)          65.162791
annual            26.942713
dtype: float64
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>These numbers are difficult to interpret without some measure of their uncertainty.
We can compute these uncertainties quickly using bootstrap resamplings of the data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">resample</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">err</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">([</span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="o">*</span><span class="n">resample</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span><span class="o">.</span><span class="n">coef_</span>
              <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">)],</span> <span class="mi">0</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>With these errors estimated, let’s again look at the results:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">'effect'</span><span class="p">:</span> <span class="n">params</span><span class="o">.</span><span class="nb">round</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
                    <span class="s">'error'</span><span class="p">:</span> <span class="n">err</span><span class="o">.</span><span class="nb">round</span><span class="p">(</span><span class="mi">0</span><span class="p">)}))</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">
      <div class="output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>              effect  error
Mon            505.0   86.0
Tue            610.0   83.0
Wed            593.0   83.0
Thu            482.0   85.0
Fri            178.0   81.0
Sat          -1103.0   80.0
Sun          -1134.0   83.0
holiday      -1187.0  163.0
daylight_hrs   129.0    9.0
PRCP          -665.0   62.0
dry day        548.0   33.0
Temp (C)        65.0    4.0
annual          27.0   18.0
</code></pre></div>      </div>
    </div>
  </div>
</div>

<p>We first see that there is a relatively stable trend in the weekly baseline: there are many more riders on weekdays than on weekends and holidays.
We see that for each additional hour of daylight, 129 ± 9 more people choose to ride; a temperature increase of one degree Celsius encourages 65 ± 4 people to grab their bicycle; a dry day means an average of 548 ± 33 more riders, and each inch of precipitation means 665 ± 62 more people leave their bike at home.
Once all these effects are accounted for, we see a modest increase of 27 ± 18 new daily riders each year.</p>

<p>Our model is almost certainly missing some relevant information. For example, nonlinear effects (such as effects of precipitation <em>and</em> cold temperature) and nonlinear trends within each variable (such as disinclination to ride at very cold and very hot temperatures) cannot be accounted for in this model.
Additionally, we have thrown away some of the finer-grained information (such as the difference between a rainy morning and a rainy afternoon), and we have ignored correlations between days (such as the possible effect of a rainy Tuesday on Wednesday’s numbers, or the effect of an unexpected sunny day after a streak of rainy days).
These are all potentially interesting effects, and you now have the tools to begin exploring them if you wish!</p>

<!--NAVIGATION-->
<p>&lt; <a href="05.05-Naive-Bayes.ipynb">In Depth: Naive Bayes Classification</a> | <a href="Index.ipynb">Contents</a> | <a href="05.07-Support-Vector-Machines.ipynb">In-Depth: Support Vector Machines</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.06-Linear-Regression.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<!--BOOK_INFORMATION-->
<p><img align="left" style="padding-right:10px;" src="figures/PDSH-cover-small.png" /></p>

<p><em>This notebook contains an excerpt from the <a href="http://shop.oreilly.com/product/0636920034919.do">Python Data Science Handbook</a> by Jake VanderPlas; the content is available <a href="https://github.com/jakevdp/PythonDataScienceHandbook">on GitHub</a>.</em></p>

<p><em>The text is released under the <a href="https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode">CC-BY-NC-ND license</a>, and code is released under the <a href="https://opensource.org/licenses/MIT">MIT license</a>. If you find this content useful, please consider supporting the work by <a href="http://shop.oreilly.com/product/0636920034919.do">buying the book</a>!</em></p>

<!--NAVIGATION-->
<p>&lt; <a href="05.06-Linear-Regression.ipynb">In Depth: Linear Regression</a> | <a href="Index.ipynb">Contents</a> | <a href="05.08-Random-Forests.ipynb">In-Depth: Decision Trees and Random Forests</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.07-Support-Vector-Machines.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<h1 id="in-depth-support-vector-machines">In-Depth: Support Vector Machines</h1>

<p>Support vector machines (SVMs) are a particularly powerful and flexible class of supervised algorithms for both classification and regression.
In this section, we will develop the intuition behind support vector machines and their use in classification problems.</p>

<p>We begin with the standard imports:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>

<span class="c"># use seaborn plotting defaults</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span><span class="p">;</span> <span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">()</span>
</code></pre></div>    </div>
  </div>

</div>

<h2 id="motivating-support-vector-machines">Motivating Support Vector Machines</h2>

<p>As part of our disussion of Bayesian classification (see <a href="05.05-Naive-Bayes.ipynb">In Depth: Naive Bayes Classification</a>), we learned a simple model describing the distribution of each underlying class, and used these generative models to probabilistically determine labels for new points.
That was an example of <em>generative classification</em>; here we will consider instead <em>discriminative classification</em>: rather than modeling each class, we simply find a line or curve (in two dimensions) or manifold (in multiple dimensions) that divides the classes from each other.</p>

<p>As an example of this, consider the simple case of a classification task, in which the two classes of points are well separated:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets.samples_generator</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                  <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.60</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'autumn'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_354_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>A linear discriminative classifier would attempt to draw a straight line separating the two sets of data, and thereby create a model for classification.
For two dimensional data like that shown here, this is a task we could do by hand.
But immediately we see a problem: there is more than one possible dividing line that can perfectly discriminate between the two classes!</p>

<p>We can draw them as follows:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xfit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'autumn'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mf">0.6</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.1</span><span class="p">],</span> <span class="s">'x'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">,</span> <span class="n">markeredgewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.65</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.6</span><span class="p">),</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">2.9</span><span class="p">)]:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xfit</span><span class="p">,</span> <span class="n">m</span> <span class="o">*</span> <span class="n">xfit</span> <span class="o">+</span> <span class="n">b</span><span class="p">,</span> <span class="s">'-k'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_356_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>These are three <em>very</em> different separators which, nevertheless, perfectly discriminate between these samples.
Depending on which you choose, a new data point (e.g., the one marked by the “X” in this plot) will be assigned a different label!
Evidently our simple intuition of “drawing a line between classes” is not enough, and we need to think a bit deeper.</p>

<h2 id="support-vector-machines-maximizing-the-margin">Support Vector Machines: Maximizing the <em>Margin</em></h2>

<p>Support vector machines offer one way to improve on this.
The intuition is this: rather than simply drawing a zero-width line between the classes, we can draw around each line a <em>margin</em> of some width, up to the nearest point.
Here is an example of how this might look:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xfit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'autumn'</span><span class="p">)</span>

<span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.65</span><span class="p">,</span> <span class="mf">0.33</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.6</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">),</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">2.9</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)]:</span>
    <span class="n">yfit</span> <span class="o">=</span> <span class="n">m</span> <span class="o">*</span> <span class="n">xfit</span> <span class="o">+</span> <span class="n">b</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xfit</span><span class="p">,</span> <span class="n">yfit</span><span class="p">,</span> <span class="s">'-k'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">xfit</span><span class="p">,</span> <span class="n">yfit</span> <span class="o">-</span> <span class="n">d</span><span class="p">,</span> <span class="n">yfit</span> <span class="o">+</span> <span class="n">d</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s">'none'</span><span class="p">,</span>
                     <span class="n">color</span><span class="o">=</span><span class="s">'#AAAAAA'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_359_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>In support vector machines, the line that maximizes this margin is the one we will choose as the optimal model.
Support vector machines are an example of such a <em>maximum margin</em> estimator.</p>

<h3 id="fitting-a-support-vector-machine">Fitting a support vector machine</h3>

<p>Let’s see the result of an actual fit to this data: we will use Scikit-Learn’s support vector classifier to train an SVM model on this data.
For the time being, we will use a linear kernel and set the <code class="highlighter-rouge">C</code> parameter to a very large number (we’ll discuss the meaning of these in more depth momentarily).</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span> <span class="c"># "Support vector classifier"</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">'linear'</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1E10</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SVC(C=10000000000.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>To better visualize what’s happening here, let’s create a quick convenience function that will plot SVM decision boundaries for us:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_svc_decision_function</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">plot_support</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="s">"""Plot the decision function for a 2D SVC"""</span>
    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">xlim</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">()</span>
    <span class="n">ylim</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">()</span>
    
    <span class="c"># create grid to evaluate model</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xlim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xlim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">ylim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ylim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">Y</span><span class="p">,</span> <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">xy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">X</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">Y</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span><span class="o">.</span><span class="n">T</span>
    <span class="n">P</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">xy</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    
    <span class="c"># plot decision boundary and margins</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span>
               <span class="n">levels</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
               <span class="n">linestyles</span><span class="o">=</span><span class="p">[</span><span class="s">'--'</span><span class="p">,</span> <span class="s">'-'</span><span class="p">,</span> <span class="s">'--'</span><span class="p">])</span>
    
    <span class="c"># plot support vectors</span>
    <span class="k">if</span> <span class="n">plot_support</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">support_vectors_</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
                   <span class="n">model</span><span class="o">.</span><span class="n">support_vectors_</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
                   <span class="n">s</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">facecolors</span><span class="o">=</span><span class="s">'none'</span><span class="p">);</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">xlim</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">ylim</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'autumn'</span><span class="p">)</span>
<span class="n">plot_svc_decision_function</span><span class="p">(</span><span class="n">model</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_365_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>This is the dividing line that maximizes the margin between the two sets of points.
Notice that a few of the training points just touch the margin: they are indicated by the black circles in this figure.
These points are the pivotal elements of this fit, and are known as the <em>support vectors</em>, and give the algorithm its name.
In Scikit-Learn, the identity of these points are stored in the <code class="highlighter-rouge">support_vectors_</code> attribute of the classifier:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="n">support_vectors_</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[ 0.44359863,  3.11530945],
       [ 2.33812285,  3.43116792],
       [ 2.06156753,  1.96918596]])
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>A key to this classifier’s success is that for the fit, only the position of the support vectors matter; any points further from the margin which are on the correct side do not modify the fit!
Technically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin.</p>

<p>We can see this, for example, if we plot the model learned from the first 60 points and first 120 points of this dataset:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_svm</span><span class="p">(</span><span class="n">N</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                      <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.60</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="n">N</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="n">N</span><span class="p">]</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">'linear'</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1E10</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    
    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span> <span class="ow">or</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'autumn'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
    <span class="n">plot_svc_decision_function</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">ax</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0625</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">axi</span><span class="p">,</span> <span class="n">N</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mi">120</span><span class="p">]):</span>
    <span class="n">plot_svm</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">axi</span><span class="p">)</span>
    <span class="n">axi</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'N = {0}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">N</span><span class="p">))</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_369_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>In the left panel, we see the model and the support vectors for 60 training points.
In the right panel, we have doubled the number of training points, but the model has not changed: the three support vectors from the left panel are still the support vectors from the right panel.
This insensitivity to the exact behavior of distant points is one of the strengths of the SVM model.</p>

<p>If you are running this notebook live, you can use IPython’s interactive widgets to view this feature of the SVM model interactively:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">fixed</span>
<span class="n">interact</span><span class="p">(</span><span class="n">plot_svm</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">200</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">fixed</span><span class="p">(</span><span class="bp">None</span><span class="p">));</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_372_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<h3 id="beyond-linear-boundaries-kernel-svm">Beyond linear boundaries: Kernel SVM</h3>

<p>Where SVM becomes extremely powerful is when it is combined with <em>kernels</em>.
We have seen a version of kernels before, in the basis function regressions of <a href="05.06-Linear-Regression.ipynb">In Depth: Linear Regression</a>.
There we projected our data into higher-dimensional space defined by polynomials and Gaussian basis functions, and thereby were able to fit for nonlinear relationships with a linear classifier.</p>

<p>In SVM models, we can use a version of the same idea.
To motivate the need for kernels, let’s look at some data that is not linearly separable:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets.samples_generator</span> <span class="kn">import</span> <span class="n">make_circles</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_circles</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">factor</span><span class="o">=.</span><span class="mi">1</span><span class="p">,</span> <span class="n">noise</span><span class="o">=.</span><span class="mi">1</span><span class="p">)</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">'linear'</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'autumn'</span><span class="p">)</span>
<span class="n">plot_svc_decision_function</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">plot_support</span><span class="o">=</span><span class="bp">False</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_374_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>It is clear that no linear discrimination will <em>ever</em> be able to separate this data.
But we can draw a lesson from the basis function regressions in <a href="05.06-Linear-Regression.ipynb">In Depth: Linear Regression</a>, and think about how we might project the data into a higher dimension such that a linear separator <em>would</em> be sufficient.
For example, one simple projection we could use would be to compute a <em>radial basis function</em> centered on the middle clump:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">X</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div>    </div>
  </div>

</div>

<p>We can visualize this extra data dimension using a three-dimensional plot—if you are running this notebook live, you will be able to use the sliders to rotate the plot:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">mpl_toolkits</span> <span class="kn">import</span> <span class="n">mplot3d</span>

<span class="k">def</span> <span class="nf">plot_3D</span><span class="p">(</span><span class="n">elev</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">azim</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s">'3d'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter3D</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'autumn'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="n">elev</span><span class="o">=</span><span class="n">elev</span><span class="p">,</span> <span class="n">azim</span><span class="o">=</span><span class="n">azim</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'x'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'y'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s">'r'</span><span class="p">)</span>

<span class="n">interact</span><span class="p">(</span><span class="n">plot_3D</span><span class="p">,</span> <span class="n">elev</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">90</span><span class="p">,</span> <span class="mi">90</span><span class="p">],</span> <span class="n">azip</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">180</span><span class="p">,</span> <span class="mi">180</span><span class="p">),</span>
         <span class="n">X</span><span class="o">=</span><span class="n">fixed</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="o">=</span><span class="n">fixed</span><span class="p">(</span><span class="n">y</span><span class="p">));</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_378_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>We can see that with this additional dimension, the data becomes trivially linearly separable, by drawing a separating plane at, say, <em>r</em>=0.7.</p>

<p>Here we had to choose and carefully tune our projection: if we had not centered our radial basis function in the right location, we would not have seen such clean, linearly separable results.
In general, the need to make such a choice is a problem: we would like to somehow automatically find the best basis functions to use.</p>

<p>One strategy to this end is to compute a basis function centered at <em>every</em> point in the dataset, and let the SVM algorithm sift through the results.
This type of basis function transformation is known as a <em>kernel transformation</em>, as it is based on a similarity relationship (or kernel) between each pair of points.</p>

<p>A potential problem with this strategy—projecting $N$ points into $N$ dimensions—is that it might become very computationally intensive as $N$ grows large.
However, because of a neat little procedure known as the <a href="https://en.wikipedia.org/wiki/Kernel_trick"><em>kernel trick</em></a>, a fit on kernel-transformed data can be done implicitly—that is, without ever building the full $N$-dimensional representation of the kernel projection!
This kernel trick is built into the SVM, and is one of the reasons the method is so powerful.</p>

<p>In Scikit-Learn, we can apply kernelized SVM simply by changing our linear kernel to an RBF (radial basis function) kernel, using the <code class="highlighter-rouge">kernel</code> model hyperparameter:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">'rbf'</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1E6</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SVC(C=1000000.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'autumn'</span><span class="p">)</span>
<span class="n">plot_svc_decision_function</span><span class="p">(</span><span class="n">clf</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">support_vectors_</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">clf</span><span class="o">.</span><span class="n">support_vectors_</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">s</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">facecolors</span><span class="o">=</span><span class="s">'none'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_381_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Using this kernelized support vector machine, we learn a suitable nonlinear decision boundary.
This kernel transformation strategy is used often in machine learning to turn fast linear methods into fast nonlinear methods, especially for models in which the kernel trick can be used.</p>

<h3 id="tuning-the-svm-softening-margins">Tuning the SVM: Softening Margins</h3>

<p>Our discussion thus far has centered around very clean datasets, in which a perfect decision boundary exists.
But what if your data has some amount of overlap?
For example, you may have data like this:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                  <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">1.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'autumn'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_384_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>To handle this case, the SVM implementation has a bit of a fudge-factor which “softens” the margin: that is, it allows some of the points to creep into the margin if that allows a better fit.
The hardness of the margin is controlled by a tuning parameter, most often known as $C$.
For very large $C$, the margin is hard, and points cannot lie in it.
For smaller $C$, the margin is softer, and can grow to encompass some points.</p>

<p>The plot shown below gives a visual picture of how a changing $C$ parameter affects the final fit, via the softening of the margin:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                  <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0625</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="k">for</span> <span class="n">axi</span><span class="p">,</span> <span class="n">C</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="p">[</span><span class="mf">10.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">'linear'</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">axi</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'autumn'</span><span class="p">)</span>
    <span class="n">plot_svc_decision_function</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">axi</span><span class="p">)</span>
    <span class="n">axi</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">support_vectors_</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
                <span class="n">model</span><span class="o">.</span><span class="n">support_vectors_</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">s</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">facecolors</span><span class="o">=</span><span class="s">'none'</span><span class="p">);</span>
    <span class="n">axi</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'C = {0:.1f}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">C</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_386_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The optimal value of the $C$ parameter will depend on your dataset, and should be tuned using cross-validation or a similar procedure (refer back to <a href="05.03-Hyperparameters-and-Model-Validation.ipynb">Hyperparameters and Model Validation</a>).</p>

<h2 id="example-face-recognition">Example: Face Recognition</h2>

<p>As an example of support vector machines in action, let’s take a look at the facial recognition problem.
We will use the Labeled Faces in the Wild dataset, which consists of several thousand collated photos of various public figures.
A fetcher for the dataset is built into Scikit-Learn:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_lfw_people</span>
<span class="n">faces</span> <span class="o">=</span> <span class="n">fetch_lfw_people</span><span class="p">(</span><span class="n">min_faces_per_person</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">target_names</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">
      <div class="output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush'
 'Gerhard Schroeder' 'Hugo Chavez' 'Junichiro Koizumi' 'Tony Blair']
(1348, 62, 47)
</code></pre></div>      </div>
    </div>
  </div>
</div>

<p>Let’s plot a few of these faces to see what we’re working with:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axi</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">flat</span><span class="p">):</span>
    <span class="n">axi</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'bone'</span><span class="p">)</span>
    <span class="n">axi</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[],</span>
            <span class="n">xlabel</span><span class="o">=</span><span class="n">faces</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">faces</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_391_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Each image contains [62×47] or nearly 3,000 pixels.
We could proceed by simply using each pixel value as a feature, but often it is more effective to use some sort of preprocessor to extract more meaningful features; here we will use a principal component analysis (see <a href="05.09-Principal-Component-Analysis.ipynb">In Depth: Principal Component Analysis</a>) to extract 150 fundamental components to feed into our support vector machine classifier.
We can do this most straightforwardly by packaging the preprocessor and the classifier into a single pipeline:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">RandomizedPCA</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>

<span class="n">pca</span> <span class="o">=</span> <span class="n">RandomizedPCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">whiten</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">svc</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">'rbf'</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="s">'balanced'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">pca</span><span class="p">,</span> <span class="n">svc</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>For the sake of testing our classifier output, we will split the data into a training and testing set:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">Xtrain</span><span class="p">,</span> <span class="n">Xtest</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">ytest</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">faces</span><span class="o">.</span><span class="n">target</span><span class="p">,</span>
                                                <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Finally, we can use a grid search cross-validation to explore combinations of parameters.
Here we will adjust <code class="highlighter-rouge">C</code> (which controls the margin hardness) and <code class="highlighter-rouge">gamma</code> (which controls the size of the radial basis function kernel), and determine the best model:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.grid_search</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s">'svc__C'</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span>
              <span class="s">'svc__gamma'</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.0005</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.005</span><span class="p">]}</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">)</span>

<span class="o">%</span><span class="n">time</span> <span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">
      <div class="output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CPU times: user 47.8 s, sys: 4.08 s, total: 51.8 s
Wall time: 26 s
{'svc__gamma': 0.001, 'svc__C': 10}
</code></pre></div>      </div>
    </div>
  </div>
</div>

<p>The optimal values fall toward the middle of our grid; if they fell at the edges, we would want to expand the grid to make sure we have found the true optimum.</p>

<p>Now with this cross-validated model, we can predict the labels for the test data, which the model has not yet seen:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">best_estimator_</span>
<span class="n">yfit</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xtest</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Let’s take a look at a few of the test images along with their predicted values:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axi</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">flat</span><span class="p">):</span>
    <span class="n">axi</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">Xtest</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">62</span><span class="p">,</span> <span class="mi">47</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'bone'</span><span class="p">)</span>
    <span class="n">axi</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[])</span>
    <span class="n">axi</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">yfit</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                   <span class="n">color</span><span class="o">=</span><span class="s">'black'</span> <span class="k">if</span> <span class="n">yfit</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">ytest</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">else</span> <span class="s">'red'</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s">'Predicted Names; Incorrect Labels in Red'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">14</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_401_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Out of this small sample, our optimal estimator mislabeled only a single face (Bush’s
face in the bottom row was mislabeled as Blair).
We can get a better sense of our estimator’s performance using the classification report, which lists recovery statistics label by label:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>
<span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">yfit</span><span class="p">,</span>
                            <span class="n">target_names</span><span class="o">=</span><span class="n">faces</span><span class="o">.</span><span class="n">target_names</span><span class="p">))</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">
      <div class="output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                   precision    recall  f1-score   support

     Ariel Sharon       0.65      0.73      0.69        15
     Colin Powell       0.81      0.87      0.84        68
  Donald Rumsfeld       0.75      0.87      0.81        31
    George W Bush       0.93      0.83      0.88       126
Gerhard Schroeder       0.86      0.78      0.82        23
      Hugo Chavez       0.93      0.70      0.80        20
Junichiro Koizumi       0.80      1.00      0.89        12
       Tony Blair       0.83      0.93      0.88        42

      avg / total       0.85      0.85      0.85       337

</code></pre></div>      </div>
    </div>
  </div>
</div>

<p>We might also display the confusion matrix between these classes:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="n">mat</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">yfit</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">mat</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s">'d'</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
            <span class="n">xticklabels</span><span class="o">=</span><span class="n">faces</span><span class="o">.</span><span class="n">target_names</span><span class="p">,</span>
            <span class="n">yticklabels</span><span class="o">=</span><span class="n">faces</span><span class="o">.</span><span class="n">target_names</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'true label'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'predicted label'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_405_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>This helps us get a sense of which labels are likely to be confused by the estimator.</p>

<p>For a real-world facial recognition task, in which the photos do not come pre-cropped into nice grids, the only difference in the facial classification scheme is the feature selection: you would need to use a more sophisticated algorithm to find the faces, and extract features that are independent of the pixellation.
For this kind of application, one good option is to make use of <a href="http://opencv.org">OpenCV</a>, which, among other things, includes pre-trained implementations of state-of-the-art feature extraction tools for images in general and faces in particular.</p>

<h2 id="support-vector-machine-summary">Support Vector Machine Summary</h2>

<p>We have seen here a brief intuitive introduction to the principals behind support vector machines.
These methods are a powerful classification method for a number of reasons:</p>

<ul>
  <li>Their dependence on relatively few support vectors means that they are very compact models, and take up very little memory.</li>
  <li>Once the model is trained, the prediction phase is very fast.</li>
  <li>Because they are affected only by points near the margin, they work well with high-dimensional data—even data with more dimensions than samples, which is a challenging regime for other algorithms.</li>
  <li>Their integration with kernel methods makes them very versatile, able to adapt to many types of data.</li>
</ul>

<p>However, SVMs have several disadvantages as well:</p>

<ul>
  <li>The scaling with the number of samples $N$ is $\mathcal{O}[N^3]$ at worst, or $\mathcal{O}[N^2]$ for efficient implementations. For large numbers of training samples, this computational cost can be prohibitive.</li>
  <li>The results are strongly dependent on a suitable choice for the softening parameter $C$. This must be carefully chosen via cross-validation, which can be expensive as datasets grow in size.</li>
  <li>The results do not have a direct probabilistic interpretation. This can be estimated via an internal cross-validation (see the <code class="highlighter-rouge">probability</code> parameter of <code class="highlighter-rouge">SVC</code>), but this extra estimation is costly.</li>
</ul>

<p>With those traits in mind, I generally only turn to SVMs once other simpler, faster, and less tuning-intensive methods have been shown to be insufficient for my needs.
Nevertheless, if you have the CPU cycles to commit to training and cross-validating an SVM on your data, the method can lead to excellent results.</p>

<!--NAVIGATION-->
<p>&lt; <a href="05.06-Linear-Regression.ipynb">In Depth: Linear Regression</a> | <a href="Index.ipynb">Contents</a> | <a href="05.08-Random-Forests.ipynb">In-Depth: Decision Trees and Random Forests</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.07-Support-Vector-Machines.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<!--BOOK_INFORMATION-->
<p><img align="left" style="padding-right:10px;" src="figures/PDSH-cover-small.png" /></p>

<p><em>This notebook contains an excerpt from the <a href="http://shop.oreilly.com/product/0636920034919.do">Python Data Science Handbook</a> by Jake VanderPlas; the content is available <a href="https://github.com/jakevdp/PythonDataScienceHandbook">on GitHub</a>.</em></p>

<p><em>The text is released under the <a href="https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode">CC-BY-NC-ND license</a>, and code is released under the <a href="https://opensource.org/licenses/MIT">MIT license</a>. If you find this content useful, please consider supporting the work by <a href="http://shop.oreilly.com/product/0636920034919.do">buying the book</a>!</em></p>

<!--NAVIGATION-->
<p>&lt; <a href="05.07-Support-Vector-Machines.ipynb">In-Depth: Support Vector Machines</a> | <a href="Index.ipynb">Contents</a> | <a href="05.09-Principal-Component-Analysis.ipynb">In Depth: Principal Component Analysis</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.08-Random-Forests.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<h1 id="in-depth-decision-trees-and-random-forests">In-Depth: Decision Trees and Random Forests</h1>

<p>Previously we have looked in depth at a simple generative classifier (naive Bayes; see <a href="05.05-Naive-Bayes.ipynb">In Depth: Naive Bayes Classification</a>) and a powerful discriminative classifier (support vector machines; see <a href="05.07-Support-Vector-Machines.ipynb">In-Depth: Support Vector Machines</a>).
Here we’ll take a look at motivating another powerful algorithm—a non-parametric algorithm called <em>random forests</em>.
Random forests are an example of an <em>ensemble</em> method, meaning that it relies on aggregating the results of an ensemble of simpler estimators.
The somewhat surprising result with such ensemble methods is that the sum can be greater than the parts: that is, a majority vote among a number of estimators can end up being better than any of the individual estimators doing the voting!
We will see examples of this in the following sections.
We begin with the standard imports:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span><span class="p">;</span> <span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">()</span>
</code></pre></div>    </div>
  </div>

</div>

<h2 id="motivating-random-forests-decision-trees">Motivating Random Forests: Decision Trees</h2>

<p>Random forests are an example of an <em>ensemble learner</em> built on decision trees.
For this reason we’ll start by discussing decision trees themselves.</p>

<p>Decision trees are extremely intuitive ways to classify or label objects: you simply ask a series of questions designed to zero-in on the classification.
For example, if you wanted to build a decision tree to classify an animal you come across while on a hike, you might construct the one shown here:</p>

<p><img src="figures/05.08-decision-tree.png" alt="" />
<a href="06.00-Figure-Code.ipynb#Decision-Tree-Example">figure source in Appendix</a></p>

<p>The binary splitting makes this extremely efficient: in a well-constructed tree, each question will cut the number of options by approximately half, very quickly narrowing the options even among a large number of classes.
The trick, of course, comes in deciding which questions to ask at each step.
In machine learning implementations of decision trees, the questions generally take the form of axis-aligned splits in the data: that is, each node in the tree splits the data into two groups using a cutoff value within one of the features.
Let’s now look at an example of this.</p>

<h3 id="creating-a-decision-tree">Creating a decision tree</h3>

<p>Consider the following two-dimensional data, which has one of four class labels:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                  <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'rainbow'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_419_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>A simple decision tree built on this data will iteratively split the data along one or the other axis according to some quantitative criterion, and at each level assign the label of the new region according to a majority vote of points within it.
This figure presents a visualization of the first four levels of a decision tree classifier for this data:</p>

<p><img src="figures/05.08-decision-tree-levels.png" alt="" />
<a href="06.00-Figure-Code.ipynb#Decision-Tree-Levels">figure source in Appendix</a></p>

<p>Notice that after the first split, every point in the upper branch remains unchanged, so there is no need to further subdivide this branch.
Except for nodes that contain all of one color, at each level <em>every</em> region is again split along one of the two features.</p>

<p>This process of fitting a decision tree to our data can be done in Scikit-Learn with the <code class="highlighter-rouge">DecisionTreeClassifier</code> estimator:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Let’s write a quick utility function to help us visualize the output of the classifier:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">visualize_classifier</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'rainbow'</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span> <span class="ow">or</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    
    <span class="c"># Plot the training points</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span>
               <span class="n">clim</span><span class="o">=</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="nb">max</span><span class="p">()),</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'tight'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
    <span class="n">xlim</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">()</span>
    <span class="n">ylim</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">()</span>
    
    <span class="c"># fit the estimator</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">*</span><span class="n">xlim</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">200</span><span class="p">),</span>
                         <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">*</span><span class="n">ylim</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">200</span><span class="p">))</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="c"># Create a color plot with the results</span>
    <span class="n">n_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
    <span class="n">contours</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
                           <span class="n">levels</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_classes</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">,</span>
                           <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">clim</span><span class="o">=</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="nb">max</span><span class="p">()),</span>
                           <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="n">xlim</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="n">ylim</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Now we can examine what the decision tree classification looks like:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">visualize_classifier</span><span class="p">(</span><span class="n">DecisionTreeClassifier</span><span class="p">(),</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_428_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>If you’re running this notebook live, you can use the helpers script included in <a href="06.00-Figure-Code.ipynb#Helper-Code">The Online Appendix</a> to bring up an interactive visualization of the decision tree building process:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># helpers_05_08 is found in the online appendix</span>
<span class="kn">import</span> <span class="nn">helpers_05_08</span>
<span class="n">helpers_05_08</span><span class="o">.</span><span class="n">plot_tree_interactive</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_430_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Notice that as the depth increases, we tend to get very strangely shaped classification regions; for example, at a depth of five, there is a tall and skinny purple region between the yellow and blue regions.
It’s clear that this is less a result of the true, intrinsic data distribution, and more a result of the particular sampling or noise properties of the data.
That is, this decision tree, even at only five levels deep, is clearly over-fitting our data.</p>

<h3 id="decision-trees-and-over-fitting">Decision trees and over-fitting</h3>

<p>Such over-fitting turns out to be a general property of decision trees: it is very easy to go too deep in the tree, and thus to fit details of the particular data rather than the overall properties of the distributions they are drawn from.
Another way to see this over-fitting is to look at models trained on different subsets of the data—for example, in this figure we train two different trees, each on half of the original data:</p>

<p><img src="figures/05.08-decision-tree-overfitting.png" alt="" />
<a href="06.00-Figure-Code.ipynb#Decision-Tree-Overfitting">figure source in Appendix</a></p>

<p>It is clear that in some places, the two trees produce consistent results (e.g., in the four corners), while in other places, the two trees give very different classifications (e.g., in the regions between any two clusters).
The key observation is that the inconsistencies tend to happen where the classification is less certain, and thus by using information from <em>both</em> of these trees, we might come up with a better result!</p>

<p>If you are running this notebook live, the following function will allow you to interactively display the fits of trees trained on a random subset of the data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># helpers_05_08 is found in the online appendix</span>
<span class="kn">import</span> <span class="nn">helpers_05_08</span>
<span class="n">helpers_05_08</span><span class="o">.</span><span class="n">randomized_tree_interactive</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_436_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Just as using information from two trees improves our results, we might expect that using information from many trees would improve our results even further.</p>

<h2 id="ensembles-of-estimators-random-forests">Ensembles of Estimators: Random Forests</h2>

<p>This notion—that multiple overfitting estimators can be combined to reduce the effect of this overfitting—is what underlies an ensemble method called <em>bagging</em>.
Bagging makes use of an ensemble (a grab bag, perhaps) of parallel estimators, each of which over-fits the data, and averages the results to find a better classification.
An ensemble of randomized decision trees is known as a <em>random forest</em>.</p>

<p>This type of bagging classification can be done manually using Scikit-Learn’s <code class="highlighter-rouge">BaggingClassifier</code> meta-estimator, as shown here:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingClassifier</span>

<span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>
<span class="n">bag</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_samples</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
                        <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">bag</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">visualize_classifier</span><span class="p">(</span><span class="n">bag</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_439_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>In this example, we have randomized the data by fitting each estimator with a random subset of 80% of the training points.
In practice, decision trees are more effectively randomized by injecting some stochasticity in how the splits are chosen: this way all the data contributes to the fit each time, but the results of the fit still have the desired randomness.
For example, when determining which feature to split on, the randomized tree might select from among the top several features.
You can read more technical details about these randomization strategies in the <a href="http://scikit-learn.org/stable/modules/ensemble.html#forest">Scikit-Learn documentation</a> and references within.</p>

<p>In Scikit-Learn, such an optimized ensemble of randomized decision trees is implemented in the <code class="highlighter-rouge">RandomForestClassifier</code> estimator, which takes care of all the randomization automatically.
All you need to do is select a number of estimators, and it will very quickly (in parallel, if desired) fit the ensemble of trees:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">visualize_classifier</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_441_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>We see that by averaging over 100 randomly perturbed models, we end up with an overall model that is much closer to our intuition about how the parameter space should be split.</p>

<h2 id="random-forest-regression">Random Forest Regression</h2>

<p>In the previous section we considered random forests within the context of classification.
Random forests can also be made to work in the case of regression (that is, continuous rather than categorical variables). The estimator to use for this is the <code class="highlighter-rouge">RandomForestRegressor</code>, and the syntax is very similar to what we saw earlier.</p>

<p>Consider the following data, drawn from the combination of a fast and slow oscillation:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.3</span><span class="p">):</span>
    <span class="n">fast_oscillation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">5</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">slow_oscillation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">slow_oscillation</span> <span class="o">+</span> <span class="n">fast_oscillation</span> <span class="o">+</span> <span class="n">noise</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s">'o'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_444_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Using the random forest regressor, we can find the best fit curve as follows:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="n">forest</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
<span class="n">forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>

<span class="n">xfit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">yfit</span> <span class="o">=</span> <span class="n">forest</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xfit</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">])</span>
<span class="n">ytrue</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xfit</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s">'o'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xfit</span><span class="p">,</span> <span class="n">yfit</span><span class="p">,</span> <span class="s">'-r'</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xfit</span><span class="p">,</span> <span class="n">ytrue</span><span class="p">,</span> <span class="s">'-k'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_446_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Here the true model is shown in the smooth gray curve, while the random forest model is shown by the jagged red curve.
As you can see, the non-parametric random forest model is flexible enough to fit the multi-period data, without us needing to specifying a multi-period model!</p>

<h2 id="example-random-forest-for-classifying-digits">Example: Random Forest for Classifying Digits</h2>

<p>Earlier we took a quick look at the hand-written digits data (see <a href="05.02-Introducing-Scikit-Learn.ipynb">Introducing Scikit-Learn</a>).
Let’s use that again here to see how the random forest classifier can be used in this context.</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="n">digits</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>
<span class="n">digits</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dict_keys(['target', 'data', 'target_names', 'DESCR', 'images'])
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>To remind us what we’re looking at, we’ll visualize the first few data points:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># set up the figure</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>  <span class="c"># figure size in inches</span>
<span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>

<span class="c"># plot the digits: each image is 8x8 pixels</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">64</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">binary</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s">'nearest'</span><span class="p">)</span>
    
    <span class="c"># label the image with the target value</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_451_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>We can quickly classify the digits using a random forest as follows:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">Xtrain</span><span class="p">,</span> <span class="n">Xtest</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">ytest</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">,</span>
                                                <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">)</span>
<span class="n">ypred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xtest</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>We can take a look at the classification report for this classifier:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="k">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">ypred</span><span class="p">,</span> <span class="n">ytest</span><span class="p">))</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">
      <div class="output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>             precision    recall  f1-score   support

          0       1.00      0.97      0.99        38
          1       1.00      0.98      0.99        44
          2       0.95      1.00      0.98        42
          3       0.98      0.96      0.97        46
          4       0.97      1.00      0.99        37
          5       0.98      0.96      0.97        49
          6       1.00      1.00      1.00        52
          7       1.00      0.96      0.98        50
          8       0.94      0.98      0.96        46
          9       0.96      0.98      0.97        46

avg / total       0.98      0.98      0.98       450

</code></pre></div>      </div>
    </div>
  </div>
</div>

<p>And for good measure, plot the confusion matrix:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="n">mat</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">ypred</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">mat</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s">'d'</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'true label'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'predicted label'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_457_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>We find that a simple, untuned random forest results in a very accurate classification of the digits data.</p>

<h2 id="summary-of-random-forests">Summary of Random Forests</h2>

<p>This section contained a brief introduction to the concept of <em>ensemble estimators</em>, and in particular the random forest – an ensemble of randomized decision trees.
Random forests are a powerful method with several advantages:</p>

<ul>
  <li>Both training and prediction are very fast, because of the simplicity of the underlying decision trees. In addition, both tasks can be straightforwardly parallelized, because the individual trees are entirely independent entities.</li>
  <li>The multiple trees allow for a probabilistic classification: a majority vote among estimators gives an estimate of the probability (accessed in Scikit-Learn with the <code class="highlighter-rouge">predict_proba()</code> method).</li>
  <li>The nonparametric model is extremely flexible, and can thus perform well on tasks that are under-fit by other estimators.</li>
</ul>

<p>A primary disadvantage of random forests is that the results are not easily interpretable: that is, if you would like to draw conclusions about the <em>meaning</em> of the classification model, random forests may not be the best choice.</p>

<!--NAVIGATION-->
<p>&lt; <a href="05.07-Support-Vector-Machines.ipynb">In-Depth: Support Vector Machines</a> | <a href="Index.ipynb">Contents</a> | <a href="05.09-Principal-Component-Analysis.ipynb">In Depth: Principal Component Analysis</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.08-Random-Forests.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<!--BOOK_INFORMATION-->
<p><img align="left" style="padding-right:10px;" src="figures/PDSH-cover-small.png" /></p>

<p><em>This notebook contains an excerpt from the <a href="http://shop.oreilly.com/product/0636920034919.do">Python Data Science Handbook</a> by Jake VanderPlas; the content is available <a href="https://github.com/jakevdp/PythonDataScienceHandbook">on GitHub</a>.</em></p>

<p><em>The text is released under the <a href="https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode">CC-BY-NC-ND license</a>, and code is released under the <a href="https://opensource.org/licenses/MIT">MIT license</a>. If you find this content useful, please consider supporting the work by <a href="http://shop.oreilly.com/product/0636920034919.do">buying the book</a>!</em></p>

<!--NAVIGATION-->
<p>&lt; <a href="05.08-Random-Forests.ipynb">In-Depth: Decision Trees and Random Forests</a> | <a href="Index.ipynb">Contents</a> | <a href="05.10-Manifold-Learning.ipynb">In-Depth: Manifold Learning</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.09-Principal-Component-Analysis.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<h1 id="in-depth-principal-component-analysis">In Depth: Principal Component Analysis</h1>

<p>Up until now, we have been looking in depth at supervised learning estimators: those estimators that predict labels based on labeled training data.
Here we begin looking at several unsupervised estimators, which can highlight interesting aspects of the data without reference to any known labels.</p>

<p>In this section, we explore what is perhaps one of the most broadly used of unsupervised algorithms, principal component analysis (PCA).
PCA is fundamentally a dimensionality reduction algorithm, but it can also be useful as a tool for visualization, for noise filtering, for feature extraction and engineering, and much more.
After a brief conceptual discussion of the PCA algorithm, we will see a couple examples of these further applications.</p>

<p>We begin with the standard imports:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span><span class="p">;</span> <span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">()</span>
</code></pre></div>    </div>
  </div>

</div>

<h2 id="introducing-principal-component-analysis">Introducing Principal Component Analysis</h2>

<p>Principal component analysis is a fast and flexible unsupervised method for dimensionality reduction in data, which we saw briefly in <a href="05.02-Introducing-Scikit-Learn.ipynb">Introducing Scikit-Learn</a>.
Its behavior is easiest to visualize by looking at a two-dimensional dataset.
Consider the following 200 points:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">200</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'equal'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_467_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>By eye, it is clear that there is a nearly linear relationship between the x and y variables.
This is reminiscent of the linear regression data we explored in <a href="05.06-Linear-Regression.ipynb">In Depth: Linear Regression</a>, but the problem setting here is slightly different: rather than attempting to <em>predict</em> the y values from the x values, the unsupervised learning problem attempts to learn about the <em>relationship</em> between the x and y values.</p>

<p>In principal component analysis, this relationship is quantified by finding a list of the <em>principal axes</em> in the data, and using those axes to describe the dataset.
Using Scikit-Learn’s <code class="highlighter-rouge">PCA</code> estimator, we can compute this as follows:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>PCA(copy=True, n_components=2, whiten=False)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>The fit learns some quantities from the data, most importantly the “components” and “explained variance”:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">
      <div class="output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[ 0.94446029  0.32862557]
 [ 0.32862557 -0.94446029]]
</code></pre></div>      </div>
    </div>
  </div>
</div>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">
      <div class="output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[ 0.75871884  0.01838551]
</code></pre></div>      </div>
    </div>
  </div>
</div>

<p>To see what these numbers mean, let’s visualize them as vectors over the input data, using the “components” to define the direction of the vector, and the “explained variance” to define the squared-length of the vector:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">draw_vector</span><span class="p">(</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span> <span class="ow">or</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s">'-&gt;'</span><span class="p">,</span>
                    <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                    <span class="n">shrinkA</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shrinkB</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s">''</span><span class="p">,</span> <span class="n">v1</span><span class="p">,</span> <span class="n">v0</span><span class="p">,</span> <span class="n">arrowprops</span><span class="o">=</span><span class="n">arrowprops</span><span class="p">)</span>

<span class="c"># plot data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="k">for</span> <span class="n">length</span><span class="p">,</span> <span class="n">vector</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_</span><span class="p">,</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">):</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">vector</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">length</span><span class="p">)</span>
    <span class="n">draw_vector</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">mean_</span><span class="p">,</span> <span class="n">pca</span><span class="o">.</span><span class="n">mean_</span> <span class="o">+</span> <span class="n">v</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'equal'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_474_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>These vectors represent the <em>principal axes</em> of the data, and the length of the vector is an indication of how “important” that axis is in describing the distribution of the data—more precisely, it is a measure of the variance of the data when projected onto that axis.
The projection of each data point onto the principal axes are the “principal components” of the data.</p>

<p>If we plot these principal components beside the original data, we see the plots shown here:</p>

<p><img src="figures/05.09-PCA-rotation.png" alt="" />
<a href="06.00-Figure-Code.ipynb#Principal-Components-Rotation">figure source in Appendix</a></p>

<p>This transformation from data axes to principal axes is an <em>affine transformation</em>, which basically means it is composed of a translation, rotation, and uniform scaling.</p>

<p>While this algorithm to find principal components may seem like just a mathematical curiosity, it turns out to have very far-reaching applications in the world of machine learning and data exploration.</p>

<h3 id="pca-as-dimensionality-reduction">PCA as dimensionality reduction</h3>

<p>Using PCA for dimensionality reduction involves zeroing out one or more of the smallest principal components, resulting in a lower-dimensional projection of the data that preserves the maximal data variance.</p>

<p>Here is an example of using PCA as a dimensionality reduction transform:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"original shape:   "</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"transformed shape:"</span><span class="p">,</span> <span class="n">X_pca</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">
      <div class="output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>original shape:    (200, 2)
transformed shape: (200, 1)
</code></pre></div>      </div>
    </div>
  </div>
</div>

<p>The transformed data has been reduced to a single dimension.
To understand the effect of this dimensionality reduction, we can perform the inverse transform of this reduced data and plot it along with the original data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_new</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X_pca</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_new</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_new</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'equal'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_481_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The light points are the original data, while the dark points are the projected version.
This makes clear what a PCA dimensionality reduction means: the information along the least important principal axis or axes is removed, leaving only the component(s) of the data with the highest variance.
The fraction of variance that is cut out (proportional to the spread of points about the line formed in this figure) is roughly a measure of how much “information” is discarded in this reduction of dimensionality.</p>

<p>This reduced-dimension dataset is in some senses “good enough” to encode the most important relationships between the points: despite reducing the dimension of the data by 50%, the overall relationship between the data points are mostly preserved.</p>

<h3 id="pca-for-visualization-hand-written-digits">PCA for visualization: Hand-written digits</h3>

<p>The usefulness of the dimensionality reduction may not be entirely apparent in only two dimensions, but becomes much more clear when looking at high-dimensional data.
To see this, let’s take a quick look at the application of PCA to the digits data we saw in <a href="05.08-Random-Forests.ipynb">In-Depth: Decision Trees and Random Forests</a>.</p>

<p>We start by loading the data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="n">digits</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>
<span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(1797, 64)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>Recall that the data consists of 8×8 pixel images, meaning that they are 64-dimensional.
To gain some intuition into the relationships between these points, we can use PCA to project them to a more manageable number of dimensions, say two:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>  <span class="c"># project from 64 to 2 dimensions</span>
<span class="n">projected</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">projected</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">
      <div class="output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(1797, 64)
(1797, 2)
</code></pre></div>      </div>
    </div>
  </div>
</div>

<p>We can now plot the first two principal components of each point to learn about the data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">projected</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">projected</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">c</span><span class="o">=</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s">'none'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
            <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s">'spectral'</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'component 1'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'component 2'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">();</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_488_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Recall what these components mean: the full data is a 64-dimensional point cloud, and these points are the projection of each data point along the directions with the largest variance.
Essentially, we have found the optimal stretch and rotation in 64-dimensional space that allows us to see the layout of the digits in two dimensions, and have done this in an unsupervised manner—that is, without reference to the labels.</p>

<h3 id="what-do-the-components-mean">What do the components mean?</h3>

<p>We can go a bit further here, and begin to ask what the reduced dimensions <em>mean</em>.
This meaning can be understood in terms of combinations of basis vectors.
For example, each image in the training set is defined by a collection of 64 pixel values, which we will call the vector $x$:</p>

<script type="math/tex; mode=display">x = [x_1, x_2, x_3 \cdots x_{64}]</script>

<p>One way we can think about this is in terms of a pixel basis.
That is, to construct the image, we multiply each element of the vector by the pixel it describes, and then add the results together to build the image:</p>

<script type="math/tex; mode=display">{\rm image}(x) = x_1 \cdot{\rm (pixel~1)} + x_2 \cdot{\rm (pixel~2)} + x_3 \cdot{\rm (pixel~3)} \cdots x_{64} \cdot{\rm (pixel~64)}</script>

<p>One way we might imagine reducing the dimension of this data is to zero out all but a few of these basis vectors.
For example, if we use only the first eight pixels, we get an eight-dimensional projection of the data, but it is not very reflective of the whole image: we’ve thrown out nearly 90% of the pixels!</p>

<p><img src="figures/05.09-digits-pixel-components.png" alt="" />
<a href="06.00-Figure-Code.ipynb#Digits-Pixel-Components">figure source in Appendix</a></p>

<p>The upper row of panels shows the individual pixels, and the lower row shows the cumulative contribution of these pixels to the construction of the image.
Using only eight of the pixel-basis components, we can only construct a small portion of the 64-pixel image.
Were we to continue this sequence and use all 64 pixels, we would recover the original image.</p>

<p>But the pixel-wise representation is not the only choice of basis. We can also use other basis functions, which each contain some pre-defined contribution from each pixel, and write something like</p>

<script type="math/tex; mode=display">image(x) = {\rm mean} + x_1 \cdot{\rm (basis~1)} + x_2 \cdot{\rm (basis~2)} + x_3 \cdot{\rm (basis~3)} \cdots</script>

<p>PCA can be thought of as a process of choosing optimal basis functions, such that adding together just the first few of them is enough to suitably reconstruct the bulk of the elements in the dataset.
The principal components, which act as the low-dimensional representation of our data, are simply the coefficients that multiply each of the elements in this series.
This figure shows a similar depiction of reconstructing this digit using the mean plus the first eight PCA basis functions:</p>

<p><img src="figures/05.09-digits-pca-components.png" alt="" />
<a href="06.00-Figure-Code.ipynb#Digits-PCA-Components">figure source in Appendix</a></p>

<p>Unlike the pixel basis, the PCA basis allows us to recover the salient features of the input image with just a mean plus eight components!
The amount of each pixel in each component is the corollary of the orientation of the vector in our two-dimensional example.
This is the sense in which PCA provides a low-dimensional representation of the data: it discovers a set of basis functions that are more efficient than the native pixel-basis of the input data.</p>

<h3 id="choosing-the-number-of-components">Choosing the number of components</h3>

<p>A vital part of using PCA in practice is the ability to estimate how many components are needed to describe the data.
This can be determined by looking at the cumulative <em>explained variance ratio</em> as a function of the number of components:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'number of components'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'cumulative explained variance'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_497_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>This curve quantifies how much of the total, 64-dimensional variance is contained within the first $N$ components.
For example, we see that with the digits the first 10 components contain approximately 75% of the variance, while you need around 50 components to describe close to 100% of the variance.</p>

<p>Here we see that our two-dimensional projection loses a lot of information (as measured by the explained variance) and that we’d need about 20 components to retain 90% of the variance.  Looking at this plot for a high-dimensional dataset can help you understand the level of redundancy present in multiple observations.</p>

<h2 id="pca-as-noise-filtering">PCA as Noise Filtering</h2>

<p>PCA can also be used as a filtering approach for noisy data.
The idea is this: any components with variance much larger than the effect of the noise should be relatively unaffected by the noise.
So if you reconstruct the data using just the largest subset of principal components, you should be preferentially keeping the signal and throwing out the noise.</p>

<p>Let’s see how this looks with the digits data.
First we will plot several of the input noise-free data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_digits</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
                             <span class="n">subplot_kw</span><span class="o">=</span><span class="p">{</span><span class="s">'xticks'</span><span class="p">:[],</span> <span class="s">'yticks'</span><span class="p">:[]},</span>
                             <span class="n">gridspec_kw</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flat</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
                  <span class="n">cmap</span><span class="o">=</span><span class="s">'binary'</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s">'nearest'</span><span class="p">,</span>
                  <span class="n">clim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
<span class="n">plot_digits</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_500_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Now lets add some random noise to create a noisy dataset, and re-plot it:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">noisy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plot_digits</span><span class="p">(</span><span class="n">noisy</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_502_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>It’s clear by eye that the images are noisy, and contain spurious pixels.
Let’s train a PCA on the noisy data, requesting that the projection preserve 50% of the variance:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="mf">0.50</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">noisy</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">n_components_</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>12
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>Here 50% of the variance amounts to 12 principal components.
Now we compute these components, and then use the inverse of the transform to reconstruct the filtered digits:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">components</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">noisy</span><span class="p">)</span>
<span class="n">filtered</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">components</span><span class="p">)</span>
<span class="n">plot_digits</span><span class="p">(</span><span class="n">filtered</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_506_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>This signal preserving/noise filtering property makes PCA a very useful feature selection routine—for example, rather than training a classifier on very high-dimensional data, you might instead train the classifier on the lower-dimensional representation, which will automatically serve to filter out random noise in the inputs.</p>

<h2 id="example-eigenfaces">Example: Eigenfaces</h2>

<p>Earlier we explored an example of using a PCA projection as a feature selector for facial recognition with a support vector machine (see <a href="05.07-Support-Vector-Machines.ipynb">In-Depth: Support Vector Machines</a>).
Here we will take a look back and explore a bit more of what went into that.
Recall that we were using the Labeled Faces in the Wild dataset made available through Scikit-Learn:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_lfw_people</span>
<span class="n">faces</span> <span class="o">=</span> <span class="n">fetch_lfw_people</span><span class="p">(</span><span class="n">min_faces_per_person</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">target_names</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">
      <div class="output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush'
 'Gerhard Schroeder' 'Hugo Chavez' 'Junichiro Koizumi' 'Tony Blair']
(1348, 62, 47)
</code></pre></div>      </div>
    </div>
  </div>
</div>

<p>Let’s take a look at the principal axes that span this dataset.
Because this is a large dataset, we will use <code class="highlighter-rouge">RandomizedPCA</code>—it contains a randomized method to approximate the first $N$ principal components much more quickly than the standard <code class="highlighter-rouge">PCA</code> estimator, and thus is very useful for high-dimensional data (here, a dimensionality of nearly 3,000).
We will take a look at the first 150 components:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">RandomizedPCA</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">RandomizedPCA</span><span class="p">(</span><span class="mi">150</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RandomizedPCA(copy=True, iterated_power=3, n_components=150,
       random_state=None, whiten=False)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>In this case, it can be interesting to visualize the images associated with the first several principal components (these components are technically known as “eigenvectors,”
so these types of images are often called “eigenfaces”).
As you can see in this figure, they are as creepy as they sound:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
                         <span class="n">subplot_kw</span><span class="o">=</span><span class="p">{</span><span class="s">'xticks'</span><span class="p">:[],</span> <span class="s">'yticks'</span><span class="p">:[]},</span>
                         <span class="n">gridspec_kw</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flat</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">62</span><span class="p">,</span> <span class="mi">47</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'bone'</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_513_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The results are very interesting, and give us insight into how the images vary: for example, the first few eigenfaces (from the top left) seem to be associated with the angle of lighting on the face, and later principal vectors seem to be picking out certain features, such as eyes, noses, and lips.
Let’s take a look at the cumulative variance of these components to see how much of the data information the projection is preserving:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'number of components'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'cumulative explained variance'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_515_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>We see that these 150 components account for just over 90% of the variance.
That would lead us to believe that using these 150 components, we would recover most of the essential characteristics of the data.
To make this more concrete, we can compare the input images with the images reconstructed from these 150 components:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Compute the components and projected faces</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">RandomizedPCA</span><span class="p">(</span><span class="mi">150</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">components</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">projected</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">components</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Plot the results</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">),</span>
                       <span class="n">subplot_kw</span><span class="o">=</span><span class="p">{</span><span class="s">'xticks'</span><span class="p">:[],</span> <span class="s">'yticks'</span><span class="p">:[]},</span>
                       <span class="n">gridspec_kw</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">62</span><span class="p">,</span> <span class="mi">47</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'binary_r'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">projected</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">62</span><span class="p">,</span> <span class="mi">47</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'binary_r'</span><span class="p">)</span>
    
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'full-dim</span><span class="se">\n</span><span class="s">input'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'150-dim</span><span class="se">\n</span><span class="s">reconstruction'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_518_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The top row here shows the input images, while the bottom row shows the reconstruction of the images from just 150 of the ~3,000 initial features.
This visualization makes clear why the PCA feature selection used in <a href="05.07-Support-Vector-Machines.ipynb">In-Depth: Support Vector Machines</a> was so successful: although it reduces the dimensionality of the data by nearly a factor of 20, the projected images contain enough information that we might, by eye, recognize the individuals in the image.
What this means is that our classification algorithm needs to be trained on 150-dimensional data rather than 3,000-dimensional data, which depending on the particular algorithm we choose, can lead to a much more efficient classification.</p>

<h2 id="principal-component-analysis-summary">Principal Component Analysis Summary</h2>

<p>In this section we have discussed the use of principal component analysis for dimensionality reduction, for visualization of high-dimensional data, for noise filtering, and for feature selection within high-dimensional data.
Because of the versatility and interpretability of PCA, it has been shown to be effective in a wide variety of contexts and disciplines.
Given any high-dimensional dataset, I tend to start with PCA in order to visualize the relationship between points (as we did with the digits), to understand the main variance in the data (as we did with the eigenfaces), and to understand the intrinsic dimensionality (by plotting the explained variance ratio).
Certainly PCA is not useful for every high-dimensional dataset, but it offers a straightforward and efficient path to gaining insight into high-dimensional data.</p>

<p>PCA’s main weakness is that it tends to be highly affected by outliers in the data.
For this reason, many robust variants of PCA have been developed, many of which act to iteratively discard data points that are poorly described by the initial components.
Scikit-Learn contains a couple interesting variants on PCA, including <code class="highlighter-rouge">RandomizedPCA</code> and <code class="highlighter-rouge">SparsePCA</code>, both also in the <code class="highlighter-rouge">sklearn.decomposition</code> submodule.
<code class="highlighter-rouge">RandomizedPCA</code>, which we saw earlier, uses a non-deterministic method to quickly approximate the first few principal components in very high-dimensional data, while <code class="highlighter-rouge">SparsePCA</code> introduces a regularization term (see <a href="05.06-Linear-Regression.ipynb">In Depth: Linear Regression</a>) that serves to enforce sparsity of the components.</p>

<p>In the following sections, we will look at other unsupervised learning methods that build on some of the ideas of PCA.</p>

<!--NAVIGATION-->
<p>&lt; <a href="05.08-Random-Forests.ipynb">In-Depth: Decision Trees and Random Forests</a> | <a href="Index.ipynb">Contents</a> | <a href="05.10-Manifold-Learning.ipynb">In-Depth: Manifold Learning</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.09-Principal-Component-Analysis.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<!--BOOK_INFORMATION-->
<p><img align="left" style="padding-right:10px;" src="figures/PDSH-cover-small.png" /></p>

<p><em>This notebook contains an excerpt from the <a href="http://shop.oreilly.com/product/0636920034919.do">Python Data Science Handbook</a> by Jake VanderPlas; the content is available <a href="https://github.com/jakevdp/PythonDataScienceHandbook">on GitHub</a>.</em></p>

<p><em>The text is released under the <a href="https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode">CC-BY-NC-ND license</a>, and code is released under the <a href="https://opensource.org/licenses/MIT">MIT license</a>. If you find this content useful, please consider supporting the work by <a href="http://shop.oreilly.com/product/0636920034919.do">buying the book</a>!</em></p>

<!--NAVIGATION-->
<p>&lt; <a href="05.09-Principal-Component-Analysis.ipynb">In Depth: Principal Component Analysis</a> | <a href="Index.ipynb">Contents</a> | <a href="05.11-K-Means.ipynb">In Depth: k-Means Clustering</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.10-Manifold-Learning.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<h1 id="in-depth-manifold-learning">In-Depth: Manifold Learning</h1>

<p>We have seen how principal component analysis (PCA) can be used in the dimensionality reduction task—reducing the number of features of a dataset while maintaining the essential relationships between the points.
While PCA is flexible, fast, and easily interpretable, it does not perform so well when there are <em>nonlinear</em> relationships within the data; we will see some examples of these below.</p>

<p>To address this deficiency, we can turn to a class of methods known as <em>manifold learning</em>—a class of unsupervised estimators that seeks to describe datasets as low-dimensional manifolds embedded in high-dimensional spaces.
When you think of a manifold, I’d suggest imagining a sheet of paper: this is a two-dimensional object that lives in our familiar three-dimensional world, and can be bent or rolled in that two dimensions.
In the parlance of manifold learning, we can think of this sheet as a two-dimensional manifold embedded in three-dimensional space.</p>

<p>Rotating, re-orienting, or stretching the piece of paper in three-dimensional space doesn’t change the flat geometry of the paper: such operations are akin to linear embeddings.
If you bend, curl, or crumple the paper, it is still a two-dimensional manifold, but the embedding into the three-dimensional space is no longer linear.
Manifold learning algorithms would seek to learn about the fundamental two-dimensional nature of the paper, even as it is contorted to fill the three-dimensional space.</p>

<p>Here we will demonstrate a number of manifold methods, going most deeply into a couple techniques: multidimensional scaling (MDS), locally linear embedding (LLE), and isometric mapping (IsoMap).</p>

<p>We begin with the standard imports:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span><span class="p">;</span> <span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">()</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
</code></pre></div>    </div>
  </div>

</div>

<h2 id="manifold-learning-hello">Manifold Learning: “HELLO”</h2>

<p>To make these concepts more clear, let’s start by generating some two-dimensional data that we can use to define a manifold.
Here is a function that will create data in the shape of the word “HELLO”:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_hello</span><span class="p">(</span><span class="n">N</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">rseed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
    <span class="c"># Make a plot with "HELLO" text; save as PNG</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="s">'HELLO'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s">'center'</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s">'center'</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s">'bold'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">85</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">'hello.png'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
    
    <span class="c"># Open this PNG and draw random points from it</span>
    <span class="kn">from</span> <span class="nn">matplotlib.image</span> <span class="kn">import</span> <span class="n">imread</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">imread</span><span class="p">(</span><span class="s">'hello.png'</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">rseed</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">N</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">*</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
    <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">*=</span> <span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="n">N</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])]</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Let’s call the function and visualize the resulting data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">make_hello</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">colorize</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s">'rainbow'</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">**</span><span class="n">colorize</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'equal'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_530_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The output is two dimensional, and consists of points drawn in the shape of the word, “HELLO”.
This data form will help us to see visually what these algorithms are doing.</p>

<h2 id="multidimensional-scaling-mds">Multidimensional Scaling (MDS)</h2>

<p>Looking at data like this, we can see that the particular choice of <em>x</em> and <em>y</em> values of the dataset are not the most fundamental description of the data: we can scale, shrink, or rotate the data, and the “HELLO” will still be apparent.
For example, if we use a rotation matrix to rotate the data, the <em>x</em> and <em>y</em> values change, but the data is still fundamentally the same:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">rotate</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">angle</span><span class="p">):</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">deg2rad</span><span class="p">(</span><span class="n">angle</span><span class="p">)</span>
    <span class="n">R</span> <span class="o">=</span> <span class="p">[[</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)],</span>
         <span class="p">[</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">)]]</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">R</span><span class="p">)</span>
    
<span class="n">X2</span> <span class="o">=</span> <span class="n">rotate</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span> <span class="o">+</span> <span class="mi">5</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X2</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">**</span><span class="n">colorize</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'equal'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_533_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>This tells us that the <em>x</em> and <em>y</em> values are not necessarily fundamental to the relationships in the data.
What <em>is</em> fundamental, in this case, is the <em>distance</em> between each point and the other points in the dataset.
A common way to represent this is to use a distance matrix: for $N$ points, we construct an $N \times N$ array such that entry $(i, j)$ contains the distance between point $i$ and point $j$.
Let’s use Scikit-Learn’s efficient <code class="highlighter-rouge">pairwise_distances</code> function to do this for our original data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">pairwise_distances</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">pairwise_distances</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">D</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(1000, 1000)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>As promised, for our <em>N</em>=1,000 points, we obtain a 1000×1000 matrix, which can be visualized as shown here:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'Blues'</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s">'nearest'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">();</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_537_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>If we similarly construct a distance matrix for our rotated and translated data, we see that it is the same:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">D2</span> <span class="o">=</span> <span class="n">pairwise_distances</span><span class="p">(</span><span class="n">X2</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">D2</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>True
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>This distance matrix gives us a representation of our data that is invariant to rotations and translations, but the visualization of the matrix above is not entirely intuitive.
In the representation shown in this figure, we have lost any visible sign of the interesting structure in the data: the “HELLO” that we saw before.</p>

<p>Further, while computing this distance matrix from the (x, y) coordinates is straightforward, transforming the distances back into <em>x</em> and <em>y</em> coordinates is rather difficult.
This is exactly what the multidimensional scaling algorithm aims to do: given a distance matrix between points, it recovers a $D$-dimensional coordinate representation of the data.
Let’s see how it works for our distance matrix, using the <code class="highlighter-rouge">precomputed</code> dissimilarity to specify that we are passing a distance matrix:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">MDS</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MDS</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dissimilarity</span><span class="o">=</span><span class="s">'precomputed'</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">out</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">out</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">**</span><span class="n">colorize</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'equal'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_541_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The MDS algorithm recovers one of the possible two-dimensional coordinate representations of our data, using <em>only</em> the $N\times N$ distance matrix describing the relationship between the data points.</p>

<h2 id="mds-as-manifold-learning">MDS as Manifold Learning</h2>

<p>The usefulness of this becomes more apparent when we consider the fact that distance matrices can be computed from data in <em>any</em> dimension.
So, for example, instead of simply rotating the data in the two-dimensional plane, we can project it into three dimensions using the following function (essentially a three-dimensional generalization of the rotation matrix used earlier):</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">random_projection</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dimension</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">rseed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">dimension</span> <span class="o">&gt;=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">rseed</span><span class="p">)</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">dimension</span><span class="p">,</span> <span class="n">dimension</span><span class="p">)</span>
    <span class="n">e</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">C</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">V</span><span class="p">[:</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
    
<span class="n">X3</span> <span class="o">=</span> <span class="n">random_projection</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">X3</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(1000, 3)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>Let’s visualize these points to see what we’re working with:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">mpl_toolkits</span> <span class="kn">import</span> <span class="n">mplot3d</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s">'3d'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter3D</span><span class="p">(</span><span class="n">X3</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X3</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">X3</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span>
             <span class="o">**</span><span class="n">colorize</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="n">azim</span><span class="o">=</span><span class="mi">70</span><span class="p">,</span> <span class="n">elev</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_546_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>We can now ask the <code class="highlighter-rouge">MDS</code> estimator to input this three-dimensional data, compute the distance matrix, and then determine the optimal two-dimensional embedding for this distance matrix.
The result recovers a representation of the original data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">MDS</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">out3</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">out3</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">out3</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">**</span><span class="n">colorize</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'equal'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_548_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>This is essentially the goal of a manifold learning estimator: given high-dimensional embedded data, it seeks a low-dimensional representation of the data that preserves certain relationships within the data.
In the case of MDS, the quantity preserved is the distance between every pair of points.</p>

<h2 id="nonlinear-embeddings-where-mds-fails">Nonlinear Embeddings: Where MDS Fails</h2>

<p>Our discussion thus far has considered <em>linear</em> embeddings, which essentially consist of rotations, translations, and scalings of data into higher-dimensional spaces.
Where MDS breaks down is when the embedding is nonlinear—that is, when it goes beyond this simple set of operations.
Consider the following embedding, which takes the input and contorts it into an “S” shape in three dimensions:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_hello_s_curve</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.75</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>

<span class="n">XS</span> <span class="o">=</span> <span class="n">make_hello_s_curve</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>This is again three-dimensional data, but we can see that the embedding is much more complicated:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">mpl_toolkits</span> <span class="kn">import</span> <span class="n">mplot3d</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s">'3d'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter3D</span><span class="p">(</span><span class="n">XS</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">XS</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">XS</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span>
             <span class="o">**</span><span class="n">colorize</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_553_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The fundamental relationships between the data points are still there, but this time the data has been transformed in a nonlinear way: it has been wrapped-up into the shape of an “S.”</p>

<p>If we try a simple MDS algorithm on this data, it is not able to “unwrap” this nonlinear embedding, and we lose track of the fundamental relationships in the embedded manifold:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">MDS</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MDS</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">outS</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">XS</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">outS</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">outS</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">**</span><span class="n">colorize</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'equal'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_555_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The best two-dimensional <em>linear</em> embeding does not unwrap the S-curve, but instead throws out the original y-axis.</p>

<h2 id="nonlinear-manifolds-locally-linear-embedding">Nonlinear Manifolds: Locally Linear Embedding</h2>

<p>How can we move forward here? Stepping back, we can see that the source of the problem is that MDS tries to preserve distances between faraway points when constructing the embedding.
But what if we instead modified the algorithm such that it only preserves distances between nearby points?
The resulting embedding would be closer to what we want.</p>

<p>Visually, we can think of it as illustrated in this figure:</p>

<p><img src="figures/05.10-LLE-vs-MDS.png" alt="(LLE vs MDS linkages)" />
<a href="06.00-Figure-Code.ipynb#LLE-vs-MDS-Linkages">figure source in Appendix</a></p>

<p>Here each faint line represents a distance that should be preserved in the embedding.
On the left is a representation of the model used by MDS: it tries to preserve the distances between each pair of points in the dataset.
On the right is a representation of the model used by a manifold learning algorithm called locally linear embedding (LLE): rather than preserving <em>all</em> distances, it instead tries to preserve only the distances between <em>neighboring points</em>: in this case, the nearest 100 neighbors of each point.</p>

<p>Thinking about the left panel, we can see why MDS fails: there is no way to flatten this data while adequately preserving the length of every line drawn between the two points.
For the right panel, on the other hand, things look a bit more optimistic. We could imagine unrolling the data in a way that keeps the lengths of the lines approximately the same.
This is precisely what LLE does, through a global optimization of a cost function reflecting this logic.</p>

<p>LLE comes in a number of flavors; here we will use the <em>modified LLE</em> algorithm to recover the embedded two-dimensional manifold.
In general, modified LLE does better than other flavors of the algorithm at recovering well-defined manifolds with very little distortion:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">LocallyLinearEmbedding</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LocallyLinearEmbedding</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'modified'</span><span class="p">,</span>
                               <span class="n">eigen_solver</span><span class="o">=</span><span class="s">'dense'</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">XS</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">out</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">out</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">**</span><span class="n">colorize</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">0.15</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.15</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_560_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The result remains somewhat distorted compared to our original manifold, but captures the essential relationships in the data!</p>

<h2 id="some-thoughts-on-manifold-methods">Some Thoughts on Manifold Methods</h2>

<p>Though this story and motivation is compelling, in practice manifold learning techniques tend to be finicky enough that they are rarely used for anything more than simple qualitative visualization of high-dimensional data.</p>

<p>The following are some of the particular challenges of manifold learning, which all contrast poorly with PCA:</p>

<ul>
  <li>In manifold learning, there is no good framework for handling missing data. In contrast, there are straightforward iterative approaches for missing data in PCA.</li>
  <li>In manifold learning, the presence of noise in the data can “short-circuit” the manifold and drastically change the embedding. In contrast, PCA naturally filters noise from the most important components.</li>
  <li>The manifold embedding result is generally highly dependent on the number of neighbors chosen, and there is generally no solid quantitative way to choose an optimal number of neighbors. In contrast, PCA does not involve such a choice.</li>
  <li>In manifold learning, the globally optimal number of output dimensions is difficult to determine. In contrast, PCA lets you find the output dimension based on the explained variance.</li>
  <li>In manifold learning, the meaning of the embedded dimensions is not always clear. In PCA, the principal components have a very clear meaning.</li>
  <li>In manifold learning the computational expense of manifold methods scales as O[N^2] or O[N^3]. For PCA, there exist randomized approaches that are generally much faster (though see the <a href="https://github.com/mmp2/megaman">megaman</a> package for some more scalable implementations of manifold learning).</li>
</ul>

<p>With all that on the table, the only clear advantage of manifold learning methods over PCA is their ability to preserve nonlinear relationships in the data; for that reason I tend to explore data with manifold methods only after first exploring them with PCA.</p>

<p>Scikit-Learn implements several common variants of manifold learning beyond Isomap and LLE: the Scikit-Learn documentation has a <a href="http://scikit-learn.org/stable/modules/manifold.html">nice discussion and comparison of them</a>.
Based on my own experience, I would give the following recommendations:</p>

<ul>
  <li>For toy problems such as the S-curve we saw before, locally linear embedding (LLE) and its variants (especially <em>modified LLE</em>), perform very well. This is implemented in <code class="highlighter-rouge">sklearn.manifold.LocallyLinearEmbedding</code>.</li>
  <li>For high-dimensional data from real-world sources, LLE often produces poor results, and isometric mapping (IsoMap) seems to generally lead to more meaningful embeddings. This is implemented in <code class="highlighter-rouge">sklearn.manifold.Isomap</code></li>
  <li>For data that is highly clustered, <em>t-distributed stochastic neighbor embedding</em> (t-SNE) seems to work very well, though can be very slow compared to other methods. This is implemented in <code class="highlighter-rouge">sklearn.manifold.TSNE</code>.</li>
</ul>

<p>If you’re interested in getting a feel for how these work, I’d suggest running each of the methods on the data in this section.</p>

<h2 id="example-isomap-on-faces">Example: Isomap on Faces</h2>

<p>One place manifold learning is often used is in understanding the relationship between high-dimensional data points.
A common case of high-dimensional data is images: for example, a set of images with 1,000 pixels each can be thought of as a collection of points in 1,000 dimensions – the brightness of each pixel in each image defines the coordinate in that dimension.</p>

<p>Here let’s apply Isomap on some faces data.
We will use the Labeled Faces in the Wild dataset, which we previously saw in <a href="05.07-Support-Vector-Machines.ipynb">In-Depth: Support Vector Machines</a> and <a href="05.09-Principal-Component-Analysis.ipynb">In Depth: Principal Component Analysis</a>.
Running this command will download the data and cache it in your home directory for later use:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_lfw_people</span>
<span class="n">faces</span> <span class="o">=</span> <span class="n">fetch_lfw_people</span><span class="p">(</span><span class="n">min_faces_per_person</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">faces</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(2370, 2914)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>We have 2,370 images, each with 2,914 pixels.
In other words, the images can be thought of as data points in a 2,914-dimensional space!</p>

<p>Let’s quickly visualize several of these images to see what we’re working with:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">subplot_kw</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[]))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axi</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">flat</span><span class="p">):</span>
    <span class="n">axi</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_567_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>We would like to plot a low-dimensional embedding of the 2,914-dimensional data to learn the fundamental relationships between the images.
One useful way to start is to compute a PCA, and examine the explained variance ratio, which will give us an idea of how many linear features are required to describe the data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">RandomizedPCA</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">RandomizedPCA</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'n components'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'cumulative variance'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_569_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>We see that for this data, nearly 100 components are required to preserve 90% of the variance: this tells us that the data is intrinsically very high dimensional—it can’t be described linearly with just a few components.</p>

<p>When this is the case, nonlinear manifold embeddings like LLE and Isomap can be helpful.
We can compute an Isomap embedding on these faces using the same pattern shown before:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">Isomap</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Isomap</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">proj</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">proj</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(2370, 2)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>The output is a two-dimensional projection of all the input images.
To get a better idea of what the projection tells us, let’s define a function that will output image thumbnails at the locations of the projections:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">offsetbox</span>

<span class="k">def</span> <span class="nf">plot_components</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">images</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                    <span class="n">thumb_frac</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span> <span class="ow">or</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    
    <span class="n">proj</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">proj</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">proj</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s">'.k'</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">images</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">min_dist_2</span> <span class="o">=</span> <span class="p">(</span><span class="n">thumb_frac</span> <span class="o">*</span> <span class="nb">max</span><span class="p">(</span><span class="n">proj</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">proj</span><span class="o">.</span><span class="nb">min</span><span class="p">(</span><span class="mi">0</span><span class="p">)))</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="n">shown_images</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span> <span class="o">*</span> <span class="n">proj</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">)])</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">proj</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">shown_images</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="nb">min</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">min_dist_2</span><span class="p">:</span>
                <span class="c"># don't show points that are too close</span>
                <span class="k">continue</span>
            <span class="n">shown_images</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">shown_images</span><span class="p">,</span> <span class="n">proj</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>
            <span class="n">imagebox</span> <span class="o">=</span> <span class="n">offsetbox</span><span class="o">.</span><span class="n">AnnotationBbox</span><span class="p">(</span>
                <span class="n">offsetbox</span><span class="o">.</span><span class="n">OffsetImage</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">),</span>
                                      <span class="n">proj</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span><span class="n">imagebox</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Calling this function now, we see the result:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plot_components</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
                <span class="n">model</span><span class="o">=</span><span class="n">Isomap</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
                <span class="n">images</span><span class="o">=</span><span class="n">faces</span><span class="o">.</span><span class="n">images</span><span class="p">[:,</span> <span class="p">::</span><span class="mi">2</span><span class="p">,</span> <span class="p">::</span><span class="mi">2</span><span class="p">])</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_575_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The result is interesting: the first two Isomap dimensions seem to describe global image features: the overall darkness or lightness of the image from left to right, and the general orientation of the face from bottom to top.
This gives us a nice visual indication of some of the fundamental features in our data.</p>

<p>We could then go on to classify this data (perhaps using manifold features as inputs to the classification algorithm) as we did in <a href="05.07-Support-Vector-Machines.ipynb">In-Depth: Support Vector Machines</a>.</p>

<h2 id="example-visualizing-structure-in-digits">Example: Visualizing Structure in Digits</h2>

<p>As another example of using manifold learning for visualization, let’s take a look at the MNIST handwritten digits set.
This data is similar to the digits we saw in <a href="05.08-Random-Forests.ipynb">In-Depth: Decision Trees and Random Forests</a>, but with many more pixels per image.
It can be downloaded from http://mldata.org/ with the Scikit-Learn utility:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_mldata</span>
<span class="n">mnist</span> <span class="o">=</span> <span class="n">fetch_mldata</span><span class="p">(</span><span class="s">'MNIST original'</span><span class="p">)</span>
<span class="n">mnist</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(70000, 784)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>This consists of 70,000 images, each with 784 pixels (i.e. the images are 28×28).
As before, we can take a look at the first few images:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">subplot_kw</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[]))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axi</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">flat</span><span class="p">):</span>
    <span class="n">axi</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">mnist</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">1250</span> <span class="o">*</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray_r'</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_580_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>This gives us an idea of the variety of handwriting styles in the dataset.</p>

<p>Let’s compute a manifold learning projection across the data.
For speed here, we’ll only use 1/30 of the data, which is about ~2000 points
(because of the relatively poor scaling of manifold learning, I find that a few thousand samples is a good number to start with for relatively quick exploration before moving to a full calculation):</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># use only 1/30 of the data: full dataset takes a long time!</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">data</span><span class="p">[::</span><span class="mi">30</span><span class="p">]</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">target</span><span class="p">[::</span><span class="mi">30</span><span class="p">]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Isomap</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">proj</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">proj</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">proj</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s">'jet'</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">9.5</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_582_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The resulting scatter plot shows some of the relationships between the data points, but is a bit crowded.
We can gain more insight by looking at just a single number at a time:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">Isomap</span>

<span class="c"># Choose 1/4 of the "1" digits to project</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">mnist</span><span class="o">.</span><span class="n">target</span> <span class="o">==</span> <span class="mi">1</span><span class="p">][::</span><span class="mi">4</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Isomap</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">eigen_solver</span><span class="o">=</span><span class="s">'dense'</span><span class="p">)</span>
<span class="n">plot_components</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">images</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)),</span>
                <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">thumb_frac</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray_r'</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_584_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The result gives you an idea of the variety of forms that the number “1” can take within the dataset.
The data lies along a broad curve in the projected space, which appears to trace the orientation of the digit.
As you move up the plot, you find ones that have hats and/or bases, though these are very sparse within the dataset.
The projection lets us identify outliers that have data issues: for example, pieces of the neighboring digits that snuck into the extracted images.</p>

<p>Now, this in itself may not be useful for the task of classifying digits, but it does help us get an understanding of the data, and may give us ideas about how to move forward, such as how we might want to preprocess the data before building a classification pipeline.</p>

<!--NAVIGATION-->
<p>&lt; <a href="05.09-Principal-Component-Analysis.ipynb">In Depth: Principal Component Analysis</a> | <a href="Index.ipynb">Contents</a> | <a href="05.11-K-Means.ipynb">In Depth: k-Means Clustering</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.10-Manifold-Learning.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<!--BOOK_INFORMATION-->
<p><img align="left" style="padding-right:10px;" src="figures/PDSH-cover-small.png" /></p>

<p><em>This notebook contains an excerpt from the <a href="http://shop.oreilly.com/product/0636920034919.do">Python Data Science Handbook</a> by Jake VanderPlas; the content is available <a href="https://github.com/jakevdp/PythonDataScienceHandbook">on GitHub</a>.</em></p>

<p><em>The text is released under the <a href="https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode">CC-BY-NC-ND license</a>, and code is released under the <a href="https://opensource.org/licenses/MIT">MIT license</a>. If you find this content useful, please consider supporting the work by <a href="http://shop.oreilly.com/product/0636920034919.do">buying the book</a>!</em></p>

<!--NAVIGATION-->
<p>&lt; <a href="05.10-Manifold-Learning.ipynb">In-Depth: Manifold Learning</a> | <a href="Index.ipynb">Contents</a> | <a href="05.12-Gaussian-Mixtures.ipynb">In Depth: Gaussian Mixture Models</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.11-K-Means.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<h1 id="in-depth-k-means-clustering">In Depth: k-Means Clustering</h1>

<p>In the previous few sections, we have explored one category of unsupervised machine learning models: dimensionality reduction.
Here we will move on to another class of unsupervised machine learning models: clustering algorithms.
Clustering algorithms seek to learn, from the properties of the data, an optimal division or discrete labeling of groups of points.</p>

<p>Many clustering algorithms are available in Scikit-Learn and elsewhere, but perhaps the simplest to understand is an algorithm known as <em>k-means clustering</em>, which is implemented in <code class="highlighter-rouge">sklearn.cluster.KMeans</code>.</p>

<p>We begin with the standard imports:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span><span class="p">;</span> <span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">()</span>  <span class="c"># for plot styling</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
</code></pre></div>    </div>
  </div>

</div>

<h2 id="introducing-k-means">Introducing k-Means</h2>

<p>The <em>k</em>-means algorithm searches for a pre-determined number of clusters within an unlabeled multidimensional dataset.
It accomplishes this using a simple conception of what the optimal clustering looks like:</p>

<ul>
  <li>The “cluster center” is the arithmetic mean of all the points belonging to the cluster.</li>
  <li>Each point is closer to its own cluster center than to other cluster centers.</li>
</ul>

<p>Those two assumptions are the basis of the <em>k</em>-means model.
We will soon dive into exactly <em>how</em> the algorithm reaches this solution, but for now let’s take a look at a simple dataset and see the <em>k</em>-means result.</p>

<p>First, let’s generate a two-dimensional dataset containing four distinct blobs.
To emphasize that this is an unsupervised algorithm, we will leave the labels out of the visualization</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets.samples_generator</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y_true</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                       <span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.60</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_594_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>By eye, it is relatively easy to pick out the four clusters.
The <em>k</em>-means algorithm does this automatically, and in Scikit-Learn uses the typical estimator API:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">y_kmeans</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Let’s visualize the results by plotting the data colored by these labels.
We will also plot the cluster centers as determined by the <em>k</em>-means estimator:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_kmeans</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'viridis'</span><span class="p">)</span>

<span class="n">centers</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">centers</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">centers</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">'black'</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_598_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The good news is that the <em>k</em>-means algorithm (at least in this simple case) assigns the points to clusters very similarly to how we might assign them by eye.
But you might wonder how this algorithm finds these clusters so quickly! After all, the number of possible combinations of cluster assignments is exponential in the number of data points—an exhaustive search would be very, very costly.
Fortunately for us, such an exhaustive search is not necessary: instead, the typical approach to <em>k</em>-means involves an intuitive iterative approach known as <em>expectation–maximization</em>.</p>

<h2 id="k-means-algorithm-expectationmaximization">k-Means Algorithm: Expectation–Maximization</h2>

<p>Expectation–maximization (E–M) is a powerful algorithm that comes up in a variety of contexts within data science.
<em>k</em>-means is a particularly simple and easy-to-understand application of the algorithm, and we will walk through it briefly here.
In short, the expectation–maximization approach here consists of the following procedure:</p>

<ol>
  <li>Guess some cluster centers</li>
  <li>Repeat until converged
    <ol>
      <li><em>E-Step</em>: assign points to the nearest cluster center</li>
      <li><em>M-Step</em>: set the cluster centers to the mean</li>
    </ol>
  </li>
</ol>

<p>Here the “E-step” or “Expectation step” is so-named because it involves updating our expectation of which cluster each point belongs to.
The “M-step” or “Maximization step” is so-named because it involves maximizing some fitness function that defines the location of the cluster centers—in this case, that maximization is accomplished by taking a simple mean of the data in each cluster.</p>

<p>The literature about this algorithm is vast, but can be summarized as follows: under typical circumstances, each repetition of the E-step and M-step will always result in a better estimate of the cluster characteristics.</p>

<p>We can visualize the algorithm as shown in the following figure.
For the particular initialization shown here, the clusters converge in just three iterations.
For an interactive version of this figure, refer to the code in <a href="06.00-Figure-Code.ipynb#Interactive-K-Means">the Appendix</a>.</p>

<p><img src="figures/05.11-expectation-maximization.png" alt="(run code in Appendix to generate image)" />
<a href="06.00-Figure-Code.ipynb#Expectation-Maximization">figure source in Appendix</a></p>

<p>The <em>k</em>-Means algorithm is simple enough that we can write it in a few lines of code.
The following is a very basic implementation:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">pairwise_distances_argmin</span>

<span class="k">def</span> <span class="nf">find_clusters</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_clusters</span><span class="p">,</span> <span class="n">rseed</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="c"># 1. Randomly choose clusters</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">rseed</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])[:</span><span class="n">n_clusters</span><span class="p">]</span>
    <span class="n">centers</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="c"># 2a. Assign labels based on closest center</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">pairwise_distances_argmin</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">centers</span><span class="p">)</span>
        
        <span class="c"># 2b. Find new centers from means of points</span>
        <span class="n">new_centers</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">X</span><span class="p">[</span><span class="n">labels</span> <span class="o">==</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_clusters</span><span class="p">)])</span>
        
        <span class="c"># 2c. Check for convergence</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="nb">all</span><span class="p">(</span><span class="n">centers</span> <span class="o">==</span> <span class="n">new_centers</span><span class="p">):</span>
            <span class="k">break</span>
        <span class="n">centers</span> <span class="o">=</span> <span class="n">new_centers</span>
    
    <span class="k">return</span> <span class="n">centers</span><span class="p">,</span> <span class="n">labels</span>

<span class="n">centers</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">find_clusters</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
            <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'viridis'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_604_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Most well-tested implementations will do a bit more than this under the hood, but the preceding function gives the gist of the expectation–maximization approach.</p>

<h3 id="caveats-of-expectationmaximization">Caveats of expectation–maximization</h3>

<p>There are a few issues to be aware of when using the expectation–maximization algorithm.</p>

<h4 id="the-globally-optimal-result-may-not-be-achieved">The globally optimal result may not be achieved</h4>
<p>First, although the E–M procedure is guaranteed to improve the result in each step, there is no assurance that it will lead to the <em>global</em> best solution.
For example, if we use a different random seed in our simple procedure, the particular starting guesses lead to poor results:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">centers</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">find_clusters</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">rseed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
            <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'viridis'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_608_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Here the E–M approach has converged, but has not converged to a globally optimal configuration. For this reason, it is common for the algorithm to be run for multiple starting guesses, as indeed Scikit-Learn does by default (set by the <code class="highlighter-rouge">n_init</code> parameter, which defaults to 10).</p>

<h4 id="the-number-of-clusters-must-be-selected-beforehand">The number of clusters must be selected beforehand</h4>
<p>Another common challenge with <em>k</em>-means is that you must tell it how many clusters you expect: it cannot learn the number of clusters from the data.
For example, if we ask the algorithm to identify six clusters, it will happily proceed and find the best six clusters:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">labels</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
            <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'viridis'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_611_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Whether the result is meaningful is a question that is difficult to answer definitively; one approach that is rather intuitive, but that we won’t discuss further here, is called <a href="http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html">silhouette analysis</a>.</p>

<p>Alternatively, you might use a more complicated clustering algorithm which has a better quantitative measure of the fitness per number of clusters (e.g., Gaussian mixture models; see <a href="05.12-Gaussian-Mixtures.ipynb">In Depth: Gaussian Mixture Models</a>) or which <em>can</em> choose a suitable number of clusters (e.g., DBSCAN, mean-shift, or affinity propagation, all available in the <code class="highlighter-rouge">sklearn.cluster</code> submodule)</p>

<h4 id="k-means-is-limited-to-linear-cluster-boundaries">k-means is limited to linear cluster boundaries</h4>
<p>The fundamental model assumptions of <em>k</em>-means (points will be closer to their own cluster center than to others) means that the algorithm will often be ineffective if the clusters have complicated geometries.</p>

<p>In particular, the boundaries between <em>k</em>-means clusters will always be linear, which means that it will fail for more complicated boundaries.
Consider the following data, along with the cluster labels found by the typical <em>k</em>-means approach:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_moons</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="n">noise</span><span class="o">=.</span><span class="mo">05</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">labels</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
            <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'viridis'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_615_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>This situation is reminiscent of the discussion in <a href="05.07-Support-Vector-Machines.ipynb">In-Depth: Support Vector Machines</a>, where we used a kernel transformation to project the data into a higher dimension where a linear separation is possible.
We might imagine using the same trick to allow <em>k</em>-means to discover non-linear boundaries.</p>

<p>One version of this kernelized <em>k</em>-means is implemented in Scikit-Learn within the <code class="highlighter-rouge">SpectralClustering</code> estimator.
It uses the graph of nearest neighbors to compute a higher-dimensional representation of the data, and then assigns labels using a <em>k</em>-means algorithm:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">SpectralClustering</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SpectralClustering</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">affinity</span><span class="o">=</span><span class="s">'nearest_neighbors'</span><span class="p">,</span>
                           <span class="n">assign_labels</span><span class="o">=</span><span class="s">'kmeans'</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
            <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'viridis'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_617_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>We see that with this kernel transform approach, the kernelized <em>k</em>-means is able to find the more complicated nonlinear boundaries between clusters.</p>

<h4 id="k-means-can-be-slow-for-large-numbers-of-samples">k-means can be slow for large numbers of samples</h4>
<p>Because each iteration of <em>k</em>-means must access every point in the dataset, the algorithm can be relatively slow as the number of samples grows.
You might wonder if this requirement to use all data at each iteration can be relaxed; for example, you might just use a subset of the data to update the cluster centers at each step.
This is the idea behind batch-based <em>k</em>-means algorithms, one form of which is implemented in <code class="highlighter-rouge">sklearn.cluster.MiniBatchKMeans</code>.
The interface for this is the same as for standard <code class="highlighter-rouge">KMeans</code>; we will see an example of its use as we continue our discussion.</p>

<h2 id="examples">Examples</h2>

<p>Being careful about these limitations of the algorithm, we can use <em>k</em>-means to our advantage in a wide variety of situations.
We’ll now take a look at a couple examples.</p>

<h3 id="example-1-k-means-on-digits">Example 1: k-means on digits</h3>

<p>To start, let’s take a look at applying <em>k</em>-means on the same simple digits data that we saw in <a href="05.08-Random-Forests.ipynb">In-Depth: Decision Trees and Random Forests</a> and <a href="05.09-Principal-Component-Analysis.ipynb">In Depth: Principal Component Analysis</a>.
Here we will attempt to use <em>k</em>-means to try to identify similar digits <em>without using the original label information</em>; this might be similar to a first step in extracting meaning from a new dataset about which you don’t have any <em>a priori</em> label information.</p>

<p>We will start by loading the digits and then finding the <code class="highlighter-rouge">KMeans</code> clusters.
Recall that the digits consist of 1,797 samples with 64 features, where each of the 64 features is the brightness of one pixel in an 8×8 image:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="n">digits</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>
<span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(1797, 64)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>The clustering can be performed as we did before:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">clusters</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(10, 64)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>The result is 10 clusters in 64 dimensions.
Notice that the cluster centers themselves are 64-dimensional points, and can themselves be interpreted as the “typical” digit within the cluster.
Let’s see what these cluster centers look like:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">centers</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="k">for</span> <span class="n">axi</span><span class="p">,</span> <span class="n">center</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">flat</span><span class="p">,</span> <span class="n">centers</span><span class="p">):</span>
    <span class="n">axi</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[])</span>
    <span class="n">axi</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">center</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s">'nearest'</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">binary</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_626_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>We see that <em>even without the labels</em>, <code class="highlighter-rouge">KMeans</code> is able to find clusters whose centers are recognizable digits, with perhaps the exception of 1 and 8.</p>

<p>Because <em>k</em>-means knows nothing about the identity of the cluster, the 0–9 labels may be permuted.
We can fix this by matching each learned cluster label with the true labels found in them:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">mode</span>

<span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">clusters</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">clusters</span> <span class="o">==</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">labels</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">mode</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="n">mask</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Now we can check how accurate our unsupervised clustering was in finding similar digits within the data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="n">accuracy_score</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.79354479688369506
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>With just a simple <em>k</em>-means algorithm, we discovered the correct grouping for 80% of the input digits!
Let’s check the confusion matrix for this:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="n">mat</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">mat</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s">'d'</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
            <span class="n">xticklabels</span><span class="o">=</span><span class="n">digits</span><span class="o">.</span><span class="n">target_names</span><span class="p">,</span>
            <span class="n">yticklabels</span><span class="o">=</span><span class="n">digits</span><span class="o">.</span><span class="n">target_names</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'true label'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'predicted label'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_632_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>As we might expect from the cluster centers we visualized before, the main point of confusion is between the eights and ones.
But this still shows that using <em>k</em>-means, we can essentially build a digit classifier <em>without reference to any known labels</em>!</p>

<p>Just for fun, let’s try to push this even farther.
We can use the t-distributed stochastic neighbor embedding (t-SNE) algorithm (mentioned in <a href="05.10-Manifold-Learning.ipynb">In-Depth: Manifold Learning</a>) to pre-process the data before performing <em>k</em>-means.
t-SNE is a nonlinear embedding algorithm that is particularly adept at preserving points within clusters.
Let’s see how it does:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>

<span class="c"># Project the data: this step will take several seconds</span>
<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s">'random'</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">digits_proj</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

<span class="c"># Compute the clusters</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">clusters</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">digits_proj</span><span class="p">)</span>

<span class="c"># Permute the labels</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">clusters</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">clusters</span> <span class="o">==</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">labels</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">mode</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="n">mask</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>

<span class="c"># Compute the accuracy</span>
<span class="n">accuracy_score</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.91930996104618812
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>That’s nearly 92% classification accuracy <em>without using the labels</em>.
This is the power of unsupervised learning when used carefully: it can extract information from the dataset that it might be difficult to do by hand or by eye.</p>

<h3 id="example-2-k-means-for-color-compression">Example 2: <em>k</em>-means for color compression</h3>

<p>One interesting application of clustering is in color compression within images.
For example, imagine you have an image with millions of colors.
In most images, a large number of the colors will be unused, and many of the pixels in the image will have similar or even identical colors.</p>

<p>For example, consider the image shown in the following figure, which is from the Scikit-Learn <code class="highlighter-rouge">datasets</code> module (for this to work, you’ll have to have the <code class="highlighter-rouge">pillow</code> Python package installed).</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Note: this requires the ``pillow`` package to be installed</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_sample_image</span>
<span class="n">china</span> <span class="o">=</span> <span class="n">load_sample_image</span><span class="p">(</span><span class="s">"china.jpg"</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">(</span><span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">china</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_637_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The image itself is stored in a three-dimensional array of size <code class="highlighter-rouge">(height, width, RGB)</code>, containing red/blue/green contributions as integers from 0 to 255:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">china</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(427, 640, 3)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>One way we can view this set of pixels is as a cloud of points in a three-dimensional color space.
We will reshape the data to <code class="highlighter-rouge">[n_samples x n_features]</code>, and rescale the colors so that they lie between 0 and 1:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">china</span> <span class="o">/</span> <span class="mf">255.0</span> <span class="c"># use 0...1 scale</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">427</span> <span class="o">*</span> <span class="mi">640</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(273280, 3)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>We can visualize these pixels in this color space, using a subset of 10,000 pixels for efficiency:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_pixels</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">10000</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">colors</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">colors</span> <span class="o">=</span> <span class="n">data</span>
    
    <span class="c"># choose a random subset</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])[:</span><span class="n">N</span><span class="p">]</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">R</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">B</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
    
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'.'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s">'Red'</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s">'Green'</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'.'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s">'Red'</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s">'Blue'</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

</div>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot_pixels</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s">'Input color space: 16 million possible colors'</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_644_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Now let’s reduce these 16 million colors to just 16 colors, using a <em>k</em>-means clustering across the pixel space.
Because we are dealing with a very large dataset, we will use the mini batch <em>k</em>-means, which operates on subsets of the data to compute the result much more quickly than the standard <em>k</em>-means algorithm:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">warnings</span><span class="p">;</span> <span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s">'ignore'</span><span class="p">)</span>  <span class="c"># Fix NumPy issues.</span>

<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">MiniBatchKMeans</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">MiniBatchKMeans</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>
<span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">new_colors</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">[</span><span class="n">kmeans</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">)]</span>

<span class="n">plot_pixels</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="n">new_colors</span><span class="p">,</span>
            <span class="n">title</span><span class="o">=</span><span class="s">"Reduced color space: 16 colors"</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_646_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The result is a re-coloring of the original pixels, where each pixel is assigned the color of its closest cluster center.
Plotting these new colors in the image space rather than the pixel space shows us the effect of this:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">china_recolored</span> <span class="o">=</span> <span class="n">new_colors</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">china</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span>
                       <span class="n">subplot_kw</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[]))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">china</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'Original Image'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">china_recolored</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'16-color Image'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">16</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_648_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Some detail is certainly lost in the rightmost panel, but the overall image is still easily recognizable.
This image on the right achieves a compression factor of around 1 million!
While this is an interesting application of <em>k</em>-means, there are certainly better way to compress information in images.
But the example shows the power of thinking outside of the box with unsupervised methods like <em>k</em>-means.</p>

<!--NAVIGATION-->
<p>&lt; <a href="05.10-Manifold-Learning.ipynb">In-Depth: Manifold Learning</a> | <a href="Index.ipynb">Contents</a> | <a href="05.12-Gaussian-Mixtures.ipynb">In Depth: Gaussian Mixture Models</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.11-K-Means.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<!--BOOK_INFORMATION-->
<p><img align="left" style="padding-right:10px;" src="figures/PDSH-cover-small.png" /></p>

<p><em>This notebook contains an excerpt from the <a href="http://shop.oreilly.com/product/0636920034919.do">Python Data Science Handbook</a> by Jake VanderPlas; the content is available <a href="https://github.com/jakevdp/PythonDataScienceHandbook">on GitHub</a>.</em></p>

<p><em>The text is released under the <a href="https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode">CC-BY-NC-ND license</a>, and code is released under the <a href="https://opensource.org/licenses/MIT">MIT license</a>. If you find this content useful, please consider supporting the work by <a href="http://shop.oreilly.com/product/0636920034919.do">buying the book</a>!</em></p>

<!--NAVIGATION-->
<p>&lt; <a href="05.11-K-Means.ipynb">In Depth: k-Means Clustering</a> | <a href="Index.ipynb">Contents</a> | <a href="05.13-Kernel-Density-Estimation.ipynb">In-Depth: Kernel Density Estimation</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.12-Gaussian-Mixtures.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<h1 id="in-depth-gaussian-mixture-models">In Depth: Gaussian Mixture Models</h1>

<p>The <em>k</em>-means clustering model explored in the previous section is simple and relatively easy to understand, but its simplicity leads to practical challenges in its application.
In particular, the non-probabilistic nature of <em>k</em>-means and its use of simple distance-from-cluster-center to assign cluster membership leads to poor performance for many real-world situations.
In this section we will take a look at Gaussian mixture models (GMMs), which can be viewed as an extension of the ideas behind <em>k</em>-means, but can also be a powerful tool for estimation beyond simple clustering.</p>

<p>We begin with the standard imports:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span><span class="p">;</span> <span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">()</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
</code></pre></div>    </div>
  </div>

</div>

<h2 id="motivating-gmm-weaknesses-of-k-means">Motivating GMM: Weaknesses of k-Means</h2>

<p>Let’s take a look at some of the weaknesses of <em>k</em>-means and think about how we might improve the cluster model.
As we saw in the previous section, given simple, well-separated data, <em>k</em>-means finds suitable clustering results.</p>

<p>For example, if we have simple blobs of data, the <em>k</em>-means algorithm can quickly label those clusters in a way that closely matches what we might do by eye:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Generate some data</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets.samples_generator</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y_true</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                       <span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.60</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c"># flip axes for better plotting</span>
</code></pre></div>    </div>
  </div>

</div>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Plot the data with K Means Labels</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'viridis'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_658_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>From an intuitive standpoint, we might expect that the clustering assignment for some points is more certain than others: for example, there appears to be a very slight overlap between the two middle clusters, such that we might not have complete confidence in the cluster assigment of points between them.
Unfortunately, the <em>k</em>-means model has no intrinsic measure of probability or uncertainty of cluster assignments (although it may be possible to use a bootstrap approach to estimate this uncertainty).
For this, we must think about generalizing the model.</p>

<p>One way to think about the <em>k</em>-means model is that it places a circle (or, in higher dimensions, a hyper-sphere) at the center of each cluster, with a radius defined by the most distant point in the cluster.
This radius acts as a hard cutoff for cluster assignment within the training set: any point outside this circle is not considered a member of the cluster.
We can visualize this cluster model with the following function:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="kn">import</span> <span class="n">cdist</span>

<span class="k">def</span> <span class="nf">plot_kmeans</span><span class="p">(</span><span class="n">kmeans</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">n_clusters</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">rseed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="c"># plot the input data</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span> <span class="ow">or</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'equal'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'viridis'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="c"># plot the representation of the KMeans model</span>
    <span class="n">centers</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span>
    <span class="n">radii</span> <span class="o">=</span> <span class="p">[</span><span class="n">cdist</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">labels</span> <span class="o">==</span> <span class="n">i</span><span class="p">],</span> <span class="p">[</span><span class="n">center</span><span class="p">])</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span>
             <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">center</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">centers</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">c</span><span class="p">,</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">centers</span><span class="p">,</span> <span class="n">radii</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">Circle</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s">'#CCCCCC'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div>    </div>
  </div>

</div>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plot_kmeans</span><span class="p">(</span><span class="n">kmeans</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_661_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>An important observation for <em>k</em>-means is that these cluster models <em>must be circular</em>: <em>k</em>-means has no built-in way of accounting for oblong or elliptical clusters.
So, for example, if we take the same data and transform it, the cluster assignments end up becoming muddled:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">13</span><span class="p">)</span>
<span class="n">X_stretched</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plot_kmeans</span><span class="p">(</span><span class="n">kmeans</span><span class="p">,</span> <span class="n">X_stretched</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_663_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>By eye, we recognize that these transformed clusters are non-circular, and thus circular clusters would be a poor fit.
Nevertheless, <em>k</em>-means is not flexible enough to account for this, and tries to force-fit the data into four circular clusters.
This results in a mixing of cluster assignments where the resulting circles overlap: see especially the bottom-right of this plot.
One might imagine addressing this particular situation by preprocessing the data with PCA (see <a href="05.09-Principal-Component-Analysis.ipynb">In Depth: Principal Component Analysis</a>), but in practice there is no guarantee that such a global operation will circularize the individual data.</p>

<p>These two disadvantages of <em>k</em>-means—its lack of flexibility in cluster shape and lack of probabilistic cluster assignment—mean that for many datasets (especially low-dimensional datasets) it may not perform as well as you might hope.</p>

<p>You might imagine addressing these weaknesses by generalizing the <em>k</em>-means model: for example, you could measure uncertainty in cluster assignment by comparing the distances of each point to <em>all</em> cluster centers, rather than focusing on just the closest.
You might also imagine allowing the cluster boundaries to be ellipses rather than circles, so as to account for non-circular clusters.
It turns out these are two essential components of a different type of clustering model, Gaussian mixture models.</p>

<h2 id="generalizing-em-gaussian-mixture-models">Generalizing E–M: Gaussian Mixture Models</h2>

<p>A Gaussian mixture model (GMM) attempts to find a mixture of multi-dimensional Gaussian probability distributions that best model any input dataset.
In the simplest case, GMMs can be used for finding clusters in the same manner as <em>k</em>-means:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.mixture</span> <span class="kn">import</span> <span class="n">GMM</span>
<span class="n">gmm</span> <span class="o">=</span> <span class="n">GMM</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">gmm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'viridis'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_666_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>But because GMM contains a probabilistic model under the hood, it is also possible to find probabilistic cluster assignments—in Scikit-Learn this is done using the <code class="highlighter-rouge">predict_proba</code> method.
This returns a matrix of size <code class="highlighter-rouge">[n_samples, n_clusters]</code> which measures the probability that any point belongs to the given cluster:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">probs</span> <span class="o">=</span> <span class="n">gmm</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">probs</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span><span class="o">.</span><span class="nb">round</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">
      <div class="output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[ 0.     0.     0.475  0.525]
 [ 0.     1.     0.     0.   ]
 [ 0.     1.     0.     0.   ]
 [ 0.     0.     0.     1.   ]
 [ 0.     1.     0.     0.   ]]
</code></pre></div>      </div>
    </div>
  </div>
</div>

<p>We can visualize this uncertainty by, for example, making the size of each point proportional to the certainty of its prediction; looking at the following figure, we can see that it is precisely the points at the boundaries between clusters that reflect this uncertainty of cluster assignment:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">size</span> <span class="o">=</span> <span class="mi">50</span> <span class="o">*</span> <span class="n">probs</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>  <span class="c"># square emphasizes differences</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'viridis'</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">size</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_670_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Under the hood, a Gaussian mixture model is very similar to <em>k</em>-means: it uses an expectation–maximization approach which qualitatively does the following:</p>

<ol>
  <li>
    <p>Choose starting guesses for the location and shape</p>
  </li>
  <li>
    <p>Repeat until converged:</p>

    <ol>
      <li><em>E-step</em>: for each point, find weights encoding the probability of membership in each cluster</li>
      <li><em>M-step</em>: for each cluster, update its location, normalization, and shape based on <em>all</em> data points, making use of the weights</li>
    </ol>
  </li>
</ol>

<p>The result of this is that each cluster is associated not with a hard-edged sphere, but with a smooth Gaussian model.
Just as in the <em>k</em>-means expectation–maximization approach, this algorithm can sometimes miss the globally optimal solution, and thus in practice multiple random initializations are used.</p>

<p>Let’s create a function that will help us visualize the locations and shapes of the GMM clusters by drawing ellipses based on the GMM output:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">matplotlib.patches</span> <span class="kn">import</span> <span class="n">Ellipse</span>

<span class="k">def</span> <span class="nf">draw_ellipse</span><span class="p">(</span><span class="n">position</span><span class="p">,</span> <span class="n">covariance</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="s">"""Draw an ellipse with a given position and covariance"""</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span> <span class="ow">or</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    
    <span class="c"># Convert covariance to principal axes</span>
    <span class="k">if</span> <span class="n">covariance</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span>
        <span class="n">U</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">Vt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">covariance</span><span class="p">)</span>
        <span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">degrees</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arctan2</span><span class="p">(</span><span class="n">U</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">U</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]))</span>
        <span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">angle</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">covariance</span><span class="p">)</span>
    
    <span class="c"># Draw the Ellipse</span>
    <span class="k">for</span> <span class="n">nsig</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">Ellipse</span><span class="p">(</span><span class="n">position</span><span class="p">,</span> <span class="n">nsig</span> <span class="o">*</span> <span class="n">width</span><span class="p">,</span> <span class="n">nsig</span> <span class="o">*</span> <span class="n">height</span><span class="p">,</span>
                             <span class="n">angle</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>
        
<span class="k">def</span> <span class="nf">plot_gmm</span><span class="p">(</span><span class="n">gmm</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span> <span class="ow">or</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">gmm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">label</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'viridis'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'equal'</span><span class="p">)</span>
    
    <span class="n">w_factor</span> <span class="o">=</span> <span class="mf">0.2</span> <span class="o">/</span> <span class="n">gmm</span><span class="o">.</span><span class="n">weights_</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">pos</span><span class="p">,</span> <span class="n">covar</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">gmm</span><span class="o">.</span><span class="n">means_</span><span class="p">,</span> <span class="n">gmm</span><span class="o">.</span><span class="n">covars_</span><span class="p">,</span> <span class="n">gmm</span><span class="o">.</span><span class="n">weights_</span><span class="p">):</span>
        <span class="n">draw_ellipse</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">covar</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">w</span> <span class="o">*</span> <span class="n">w_factor</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>With this in place, we can take a look at what the four-component GMM gives us for our initial data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gmm</span> <span class="o">=</span> <span class="n">GMM</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">plot_gmm</span><span class="p">(</span><span class="n">gmm</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_674_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Similarly, we can use the GMM approach to fit our stretched dataset; allowing for a full covariance the model will fit even very oblong, stretched-out clusters:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gmm</span> <span class="o">=</span> <span class="n">GMM</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="s">'full'</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">plot_gmm</span><span class="p">(</span><span class="n">gmm</span><span class="p">,</span> <span class="n">X_stretched</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_676_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>This makes clear that GMM addresses the two main practical issues with <em>k</em>-means encountered before.</p>

<h3 id="choosing-the-covariance-type">Choosing the covariance type</h3>

<p>If you look at the details of the preceding fits, you will see that the <code class="highlighter-rouge">covariance_type</code> option was set differently within each.
This hyperparameter controls the degrees of freedom in the shape of each cluster; it is essential to set this carefully for any given problem.
The default is <code class="highlighter-rouge">covariance_type="diag"</code>, which means that the size of the cluster along each dimension can be set independently, with the resulting ellipse constrained to align with the axes.
A slightly simpler and faster model is <code class="highlighter-rouge">covariance_type="spherical"</code>, which constrains the shape of the cluster such that all dimensions are equal. The resulting clustering will have similar characteristics to that of <em>k</em>-means, though it is not entirely equivalent.
A more complicated and computationally expensive model (especially as the number of dimensions grows) is to use <code class="highlighter-rouge">covariance_type="full"</code>, which allows each cluster to be modeled as an ellipse with arbitrary orientation.</p>

<p>We can see a visual representation of these three choices for a single cluster within the following figure:</p>

<p><img src="figures/05.12-covariance-type.png" alt="(Covariance Type)" />
<a href="06.00-Figure-Code.ipynb#Covariance-Type">figure source in Appendix</a></p>

<h2 id="gmm-as-density-estimation">GMM as <em>Density Estimation</em></h2>

<p>Though GMM is often categorized as a clustering algorithm, fundamentally it is an algorithm for <em>density estimation</em>.
That is to say, the result of a GMM fit to some data is technically not a clustering model, but a generative probabilistic model describing the distribution of the data.</p>

<p>As an example, consider some data generated from Scikit-Learn’s <code class="highlighter-rouge">make_moons</code> function, which we saw in <a href="05.11-K-Means.ipynb">In Depth: K-Means Clustering</a>:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>
<span class="n">Xmoon</span><span class="p">,</span> <span class="n">ymoon</span> <span class="o">=</span> <span class="n">make_moons</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="n">noise</span><span class="o">=.</span><span class="mo">05</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Xmoon</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">Xmoon</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_681_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>If we try to fit this with a two-component GMM viewed as a clustering model, the results are not particularly useful:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gmm2</span> <span class="o">=</span> <span class="n">GMM</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="s">'full'</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plot_gmm</span><span class="p">(</span><span class="n">gmm2</span><span class="p">,</span> <span class="n">Xmoon</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_683_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>But if we instead use many more components and ignore the cluster labels, we find a fit that is much closer to the input data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gmm16</span> <span class="o">=</span> <span class="n">GMM</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="s">'full'</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plot_gmm</span><span class="p">(</span><span class="n">gmm16</span><span class="p">,</span> <span class="n">Xmoon</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_685_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Here the mixture of 16 Gaussians serves not to find separated clusters of data, but rather to model the overall <em>distribution</em> of the input data.
This is a generative model of the distribution, meaning that the GMM gives us the recipe to generate new random data distributed similarly to our input.
For example, here are 400 new points drawn from this 16-component GMM fit to our original data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Xnew</span> <span class="o">=</span> <span class="n">gmm16</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Xnew</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">Xnew</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_687_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>GMM is convenient as a flexible means of modeling an arbitrary multi-dimensional distribution of data.</p>

<h3 id="how-many-components">How many components?</h3>

<p>The fact that GMM is a generative model gives us a natural means of determining the optimal number of components for a given dataset.
A generative model is inherently a probability distribution for the dataset, and so we can simply evaluate the <em>likelihood</em> of the data under the model, using cross-validation to avoid over-fitting.
Another means of correcting for over-fitting is to adjust the model likelihoods using some analytic criterion such as the <a href="https://en.wikipedia.org/wiki/Akaike_information_criterion">Akaike information criterion (AIC)</a> or the <a href="https://en.wikipedia.org/wiki/Bayesian_information_criterion">Bayesian information criterion (BIC)</a>.
Scikit-Learn’s <code class="highlighter-rouge">GMM</code> estimator actually includes built-in methods that compute both of these, and so it is very easy to operate on this approach.</p>

<p>Let’s look at the AIC and BIC as a function as the number of GMM components for our moon dataset:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_components</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">21</span><span class="p">)</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="n">GMM</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="s">'full'</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xmoon</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">n_components</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="p">[</span><span class="n">m</span><span class="o">.</span><span class="n">bic</span><span class="p">(</span><span class="n">Xmoon</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">models</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'BIC'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="p">[</span><span class="n">m</span><span class="o">.</span><span class="n">aic</span><span class="p">(</span><span class="n">Xmoon</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">models</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'AIC'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'best'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'n_components'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_690_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The optimal number of clusters is the value that minimizes the AIC or BIC, depending on which approximation we wish to use. The AIC tells us that our choice of 16 components above was probably too many: around 8-12 components would have been a better choice.
As is typical with this sort of problem, the BIC recommends a simpler model.</p>

<p>Notice the important point: this choice of number of components measures how well GMM works <em>as a density estimator</em>, not how well it works <em>as a clustering algorithm</em>.
I’d encourage you to think of GMM primarily as a density estimator, and use it for clustering only when warranted within simple datasets.</p>

<h2 id="example-gmm-for-generating-new-data">Example: GMM for Generating New Data</h2>

<p>We just saw a simple example of using GMM as a generative model of data in order to create new samples from the distribution defined by the input data.
Here we will run with this idea and generate <em>new handwritten digits</em> from the standard digits corpus that we have used before.</p>

<p>To start with, let’s load the digits data using Scikit-Learn’s data tools:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="n">digits</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>
<span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(1797, 64)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>Next let’s plot the first 100 of these to recall exactly what we’re looking at:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_digits</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
                           <span class="n">subplot_kw</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[]))</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axi</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">flat</span><span class="p">):</span>
        <span class="n">im</span> <span class="o">=</span> <span class="n">axi</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'binary'</span><span class="p">)</span>
        <span class="n">im</span><span class="o">.</span><span class="n">set_clim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">plot_digits</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_695_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>We have nearly 1,800 digits in 64 dimensions, and we can build a GMM on top of these to generate more.
GMMs can have difficulty converging in such a high dimensional space, so we will start with an invertible dimensionality reduction algorithm on the data.
Here we will use a straightforward PCA, asking it to preserve 99% of the variance in the projected data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">whiten</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(1797, 41)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>The result is 41 dimensions, a reduction of nearly 1/3 with almost no information loss.
Given this projected data, let’s use the AIC to get a gauge for the number of GMM components we should use:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_components</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">210</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="n">GMM</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="s">'full'</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">n_components</span><span class="p">]</span>
<span class="n">aics</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">aic</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">models</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="n">aics</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_699_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>It appears that around 110 components minimizes the AIC; we will use this model.
Let’s quickly fit this to the data and confirm that it has converged:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gmm</span> <span class="o">=</span> <span class="n">GMM</span><span class="p">(</span><span class="mi">110</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="s">'full'</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">gmm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">gmm</span><span class="o">.</span><span class="n">converged_</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">
      <div class="output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>True
</code></pre></div>      </div>
    </div>
  </div>
</div>

<p>Now we can draw samples of 100 new points within this 41-dimensional projected space, using the GMM as a generative model:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_new</span> <span class="o">=</span> <span class="n">gmm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">data_new</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(100, 41)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>Finally, we can use the inverse transform of the PCA object to construct the new digits:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">digits_new</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">data_new</span><span class="p">)</span>
<span class="n">plot_digits</span><span class="p">(</span><span class="n">digits_new</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_705_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The results for the most part look like plausible digits from the dataset!</p>

<p>Consider what we’ve done here: given a sampling of handwritten digits, we have modeled the distribution of that data in such a way that we can generate brand new samples of digits from the data: these are “handwritten digits” which do not individually appear in the original dataset, but rather capture the general features of the input data as modeled by the mixture model.
Such a generative model of digits can prove very useful as a component of a Bayesian generative classifier, as we shall see in the next section.</p>

<!--NAVIGATION-->
<p>&lt; <a href="05.11-K-Means.ipynb">In Depth: k-Means Clustering</a> | <a href="Index.ipynb">Contents</a> | <a href="05.13-Kernel-Density-Estimation.ipynb">In-Depth: Kernel Density Estimation</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.12-Gaussian-Mixtures.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<!--BOOK_INFORMATION-->
<p><img align="left" style="padding-right:10px;" src="figures/PDSH-cover-small.png" /></p>

<p><em>This notebook contains an excerpt from the <a href="http://shop.oreilly.com/product/0636920034919.do">Python Data Science Handbook</a> by Jake VanderPlas; the content is available <a href="https://github.com/jakevdp/PythonDataScienceHandbook">on GitHub</a>.</em></p>

<p><em>The text is released under the <a href="https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode">CC-BY-NC-ND license</a>, and code is released under the <a href="https://opensource.org/licenses/MIT">MIT license</a>. If you find this content useful, please consider supporting the work by <a href="http://shop.oreilly.com/product/0636920034919.do">buying the book</a>!</em></p>

<!--NAVIGATION-->
<p>&lt; <a href="05.12-Gaussian-Mixtures.ipynb">In Depth: Gaussian Mixture Models</a> | <a href="Index.ipynb">Contents</a> | <a href="05.14-Image-Features.ipynb">Application: A Face Detection Pipeline</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.13-Kernel-Density-Estimation.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<h1 id="in-depth-kernel-density-estimation">In-Depth: Kernel Density Estimation</h1>

<p>In the previous section we covered Gaussian mixture models (GMM), which are a kind of hybrid between a clustering estimator and a density estimator.
Recall that a density estimator is an algorithm which takes a $D$-dimensional dataset and produces an estimate of the $D$-dimensional probability distribution which that data is drawn from.
The GMM algorithm accomplishes this by representing the density as a weighted sum of Gaussian distributions.
<em>Kernel density estimation</em> (KDE) is in some senses an algorithm which takes the mixture-of-Gaussians idea to its logical extreme: it uses a mixture consisting of one Gaussian component <em>per point</em>, resulting in an essentially non-parametric estimator of density.
In this section, we will explore the motivation and uses of KDE.</p>

<p>We begin with the standard imports:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span><span class="p">;</span> <span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">()</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
</code></pre></div>    </div>
  </div>

</div>

<h2 id="motivating-kde-histograms">Motivating KDE: Histograms</h2>

<p>As already discussed, a density estimator is an algorithm which seeks to model the probability distribution that generated a dataset.
For one dimensional data, you are probably already familiar with one simple density estimator: the histogram.
A histogram divides the data into discrete bins, counts the number of points that fall in each bin, and then visualizes the results in an intuitive manner.</p>

<p>For example, let’s create some data that is drawn from two normal distributions:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_data</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">rseed</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">rand</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">rseed</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">rand</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="n">x</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">f</span> <span class="o">*</span> <span class="n">N</span><span class="p">):]</span> <span class="o">+=</span> <span class="mi">5</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">make_data</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>We have previously seen that the standard count-based histogram can be created with the <code class="highlighter-rouge">plt.hist()</code> function.
By specifying the <code class="highlighter-rouge">normed</code> parameter of the histogram, we end up with a normalized histogram where the height of the bins does not reflect counts, but instead reflects probability density:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">hist</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">normed</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_716_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Notice that for equal binning, this normalization simply changes the scale on the y-axis, leaving the relative heights essentially the same as in a histogram built from counts.
This normalization is chosen so that the total area under the histogram is equal to 1, as we can confirm by looking at the output of the histogram function:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">density</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="n">patches</span> <span class="o">=</span> <span class="n">hist</span>
<span class="n">widths</span> <span class="o">=</span> <span class="n">bins</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">bins</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="p">(</span><span class="n">density</span> <span class="o">*</span> <span class="n">widths</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1.0
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>One of the issues with using a histogram as a density estimator is that the choice of bin size and location can lead to representations that have qualitatively different features.
For example, if we look at a version of this data with only 20 points, the choice of how to draw the bins can lead to an entirely different interpretation of the data!
Consider this example:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">make_data</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
                       <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                       <span class="n">subplot_kw</span><span class="o">=</span><span class="p">{</span><span class="s">'xlim'</span><span class="p">:(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span>
                                   <span class="s">'ylim'</span><span class="p">:(</span><span class="o">-</span><span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">)})</span>
<span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">offset</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]):</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span> <span class="o">+</span> <span class="n">offset</span><span class="p">,</span> <span class="n">normed</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">),</span> <span class="s">'|k'</span><span class="p">,</span>
               <span class="n">markeredgewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_721_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>On the left, the histogram makes clear that this is a bimodal distribution.
On the right, we see a unimodal distribution with a long tail.
Without seeing the preceding code, you would probably not guess that these two histograms were built from the same data: with that in mind, how can you trust the intuition that histograms confer?
And how might we improve on this?</p>

<p>Stepping back, we can think of a histogram as a stack of blocks, where we stack one block within each bin on top of each point in the dataset.
Let’s view this directly:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">),</span> <span class="s">'|k'</span><span class="p">,</span>
        <span class="n">markeredgewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">count</span><span class="p">,</span> <span class="n">edge</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">bins</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">count</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="n">edge</span><span class="p">,</span> <span class="n">i</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
                                   <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(-0.2, 8)
</code></pre></div>      </div>

    </div>
  </div>
  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_723_1.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The problem with our two binnings stems from the fact that the height of the block stack often reflects not on the actual density of points nearby, but on coincidences of how the bins align with the data points.
This mis-alignment between points and their blocks is a potential cause of the poor histogram results seen here.
But what if, instead of stacking the blocks aligned with the <em>bins</em>, we were to stack the blocks aligned with the <em>points they represent</em>?
If we do this, the blocks won’t be aligned, but we can add their contributions at each location along the x-axis to find the result.
Let’s try this:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>
<span class="n">density</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">((</span><span class="nb">abs</span><span class="p">(</span><span class="n">xi</span> <span class="o">-</span> <span class="n">x_d</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.5</span><span class="p">)</span> <span class="k">for</span> <span class="n">xi</span> <span class="ow">in</span> <span class="n">x</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_d</span><span class="p">,</span> <span class="n">density</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">),</span> <span class="s">'|k'</span><span class="p">,</span> <span class="n">markeredgewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mi">8</span><span class="p">]);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_725_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The result looks a bit messy, but is a much more robust reflection of the actual data characteristics than is the standard histogram.
Still, the rough edges are not aesthetically pleasing, nor are they reflective of any true properties of the data.
In order to smooth them out, we might decide to replace the blocks at each location with a smooth function, like a Gaussian.
Let’s use a standard normal curve at each point instead of a block:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="n">x_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">density</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">norm</span><span class="p">(</span><span class="n">xi</span><span class="p">)</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_d</span><span class="p">)</span> <span class="k">for</span> <span class="n">xi</span> <span class="ow">in</span> <span class="n">x</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_d</span><span class="p">,</span> <span class="n">density</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">),</span> <span class="s">'|k'</span><span class="p">,</span> <span class="n">markeredgewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mi">5</span><span class="p">]);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_727_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>This smoothed-out plot, with a Gaussian distribution contributed at the location of each input point, gives a much more accurate idea of the shape of the data distribution, and one which has much less variance (i.e., changes much less in response to differences in sampling).</p>

<p>These last two plots are examples of kernel density estimation in one dimension: the first uses a so-called “tophat” kernel and the second uses a Gaussian kernel.
We’ll now look at kernel density estimation in more detail.</p>

<h2 id="kernel-density-estimation-in-practice">Kernel Density Estimation in Practice</h2>

<p>The free parameters of kernel density estimation are the <em>kernel</em>, which specifies the shape of the distribution placed at each point, and the <em>kernel bandwidth</em>, which controls the size of the kernel at each point.
In practice, there are many kernels you might use for a kernel density estimation: in particular, the Scikit-Learn KDE implementation supports one of six kernels, which you can read about in Scikit-Learn’s <a href="http://scikit-learn.org/stable/modules/density.html">Density Estimation documentation</a>.</p>

<p>While there are several versions of kernel density estimation implemented in Python (notably in the SciPy and StatsModels packages), I prefer to use Scikit-Learn’s version because of its efficiency and flexibility.
It is implemented in the <code class="highlighter-rouge">sklearn.neighbors.KernelDensity</code> estimator, which handles KDE in multiple dimensions with one of six kernels and one of a couple dozen distance metrics.
Because KDE can be fairly computationally intensive, the Scikit-Learn estimator uses a tree-based algorithm under the hood and can trade off computation time for accuracy using the <code class="highlighter-rouge">atol</code> (absolute tolerance) and <code class="highlighter-rouge">rtol</code> (relative tolerance) parameters.
The kernel bandwidth, which is a free parameter, can be determined using Scikit-Learn’s standard cross validation tools as we will soon see.</p>

<p>Let’s first show a simple example of replicating the above plot using the Scikit-Learn <code class="highlighter-rouge">KernelDensity</code> estimator:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KernelDensity</span>

<span class="c"># instantiate and fit the KDE model</span>
<span class="n">kde</span> <span class="o">=</span> <span class="n">KernelDensity</span><span class="p">(</span><span class="n">bandwidth</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s">'gaussian'</span><span class="p">)</span>
<span class="n">kde</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">])</span>

<span class="c"># score_samples returns the log of the probability density</span>
<span class="n">logprob</span> <span class="o">=</span> <span class="n">kde</span><span class="o">.</span><span class="n">score_samples</span><span class="p">(</span><span class="n">x_d</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_d</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logprob</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">),</span> <span class="s">'|k'</span><span class="p">,</span> <span class="n">markeredgewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.22</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(-0.02, 0.22)
</code></pre></div>      </div>

    </div>
  </div>
  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_730_1.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The result here is normalized such that the area under the curve is equal to 1.</p>

<h3 id="selecting-the-bandwidth-via-cross-validation">Selecting the bandwidth via cross-validation</h3>

<p>The choice of bandwidth within KDE is extremely important to finding a suitable density estimate, and is the knob that controls the bias–variance trade-off in the estimate of density: too narrow a bandwidth leads to a high-variance estimate (i.e., over-fitting), where the presence or absence of a single point makes a large difference. Too wide a bandwidth leads to a high-bias estimate (i.e., under-fitting) where the structure in the data is washed out by the wide kernel.</p>

<p>There is a long history in statistics of methods to quickly estimate the best bandwidth based on rather stringent assumptions about the data: if you look up the KDE implementations in the SciPy and StatsModels packages, for example, you will see implementations based on some of these rules.</p>

<p>In machine learning contexts, we’ve seen that such hyperparameter tuning often is done empirically via a cross-validation approach.
With this in mind, the <code class="highlighter-rouge">KernelDensity</code> estimator in Scikit-Learn is designed such that it can be used directly within the Scikit-Learn’s standard grid search tools.
Here we will use <code class="highlighter-rouge">GridSearchCV</code> to optimize the bandwidth for the preceding dataset.
Because we are looking at such a small dataset, we will use leave-one-out cross-validation, which minimizes the reduction in training set size for each cross-validation trial:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.grid_search</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">LeaveOneOut</span>

<span class="n">bandwidths</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">**</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">KernelDensity</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">'gaussian'</span><span class="p">),</span>
                    <span class="p">{</span><span class="s">'bandwidth'</span><span class="p">:</span> <span class="n">bandwidths</span><span class="p">},</span>
                    <span class="n">cv</span><span class="o">=</span><span class="n">LeaveOneOut</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
<span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]);</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Now we can find the choice of bandwidth which maximizes the score (which in this case defaults to the log-likelihood):</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">grid</span><span class="o">.</span><span class="n">best_params_</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'bandwidth': 1.1233240329780276}
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>The optimal bandwidth happens to be very close to what we used in the example plot earlier, where the bandwidth was 1.0 (i.e., the default width of <code class="highlighter-rouge">scipy.stats.norm</code>).</p>

<h2 id="example-kde-on-a-sphere">Example: KDE on a Sphere</h2>

<p>Perhaps the most common use of KDE is in graphically representing distributions of points.
For example, in the Seaborn visualization library (see <a href="04.14-Visualization-With-Seaborn.ipynb">Visualization With Seaborn</a>), KDE is built in and automatically used to help visualize points in one and two dimensions.</p>

<p>Here we will look at a slightly more sophisticated use of KDE for visualization of distributions.
We will make use of some geographic data that can be loaded with Scikit-Learn: the geographic distributions of recorded observations of two South American mammals, <em>Bradypus variegatus</em> (the Brown-throated Sloth) and <em>Microryzomys minutus</em> (the Forest Small Rice Rat).</p>

<p>With Scikit-Learn, we can fetch this data as follows:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_species_distributions</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">fetch_species_distributions</span><span class="p">()</span>

<span class="c"># Get matrices/arrays of species IDs and locations</span>
<span class="n">latlon</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">data</span><span class="o">.</span><span class="n">train</span><span class="p">[</span><span class="s">'dd lat'</span><span class="p">],</span>
                    <span class="n">data</span><span class="o">.</span><span class="n">train</span><span class="p">[</span><span class="s">'dd long'</span><span class="p">]])</span><span class="o">.</span><span class="n">T</span>
<span class="n">species</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">d</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'ascii'</span><span class="p">)</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s">'micro'</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">train</span><span class="p">[</span><span class="s">'species'</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'int'</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>With this data loaded, we can use the Basemap toolkit (mentioned previously in <a href="04.13-Geographic-Data-With-Basemap.ipynb">Geographic Data with Basemap</a>) to plot the observed locations of these two species on the map of South America.</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">mpl_toolkits.basemap</span> <span class="kn">import</span> <span class="n">Basemap</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets.species_distributions</span> <span class="kn">import</span> <span class="n">construct_grids</span>

<span class="n">xgrid</span><span class="p">,</span> <span class="n">ygrid</span> <span class="o">=</span> <span class="n">construct_grids</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c"># plot coastlines with basemap</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">Basemap</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s">'cyl'</span><span class="p">,</span> <span class="n">resolution</span><span class="o">=</span><span class="s">'c'</span><span class="p">,</span>
            <span class="n">llcrnrlat</span><span class="o">=</span><span class="n">ygrid</span><span class="o">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">urcrnrlat</span><span class="o">=</span><span class="n">ygrid</span><span class="o">.</span><span class="nb">max</span><span class="p">(),</span>
            <span class="n">llcrnrlon</span><span class="o">=</span><span class="n">xgrid</span><span class="o">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">urcrnrlon</span><span class="o">=</span><span class="n">xgrid</span><span class="o">.</span><span class="nb">max</span><span class="p">())</span>
<span class="n">m</span><span class="o">.</span><span class="n">drawmapboundary</span><span class="p">(</span><span class="n">fill_color</span><span class="o">=</span><span class="s">'#DDEEFF'</span><span class="p">)</span>
<span class="n">m</span><span class="o">.</span><span class="n">fillcontinents</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s">'#FFEEDD'</span><span class="p">)</span>
<span class="n">m</span><span class="o">.</span><span class="n">drawcoastlines</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s">'gray'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">m</span><span class="o">.</span><span class="n">drawcountries</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s">'gray'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c"># plot locations</span>
<span class="n">m</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">latlon</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">latlon</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
          <span class="n">c</span><span class="o">=</span><span class="n">species</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'rainbow'</span><span class="p">,</span> <span class="n">latlon</span><span class="o">=</span><span class="bp">True</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_740_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Unfortunately, this doesn’t give a very good idea of the density of the species, because points in the species range may overlap one another.
You may not realize it by looking at this plot, but there are over 1,600 points shown here!</p>

<p>Let’s use kernel density estimation to show this distribution in a more interpretable way: as a smooth indication of density on the map.
Because the coordinate system here lies on a spherical surface rather than a flat plane, we will use the <code class="highlighter-rouge">haversine</code> distance metric, which will correctly represent distances on a curved surface.</p>

<p>There is a bit of boilerplate code here (one of the disadvantages of the Basemap toolkit) but the meaning of each code block should be clear:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Set up the data grid for the contour plot</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">xgrid</span><span class="p">[::</span><span class="mi">5</span><span class="p">],</span> <span class="n">ygrid</span><span class="p">[::</span><span class="mi">5</span><span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">land_reference</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">coverages</span><span class="p">[</span><span class="mi">6</span><span class="p">][::</span><span class="mi">5</span><span class="p">,</span> <span class="p">::</span><span class="mi">5</span><span class="p">]</span>
<span class="n">land_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">land_reference</span> <span class="o">&gt;</span> <span class="o">-</span><span class="mi">9999</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">xy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">Y</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">X</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span><span class="o">.</span><span class="n">T</span>
<span class="n">xy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">radians</span><span class="p">(</span><span class="n">xy</span><span class="p">[</span><span class="n">land_mask</span><span class="p">])</span>

<span class="c"># Create two side-by-side plots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">species_names</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Bradypus Variegatus'</span><span class="p">,</span> <span class="s">'Microryzomys Minutus'</span><span class="p">]</span>
<span class="n">cmaps</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Purples'</span><span class="p">,</span> <span class="s">'Reds'</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axi</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ax</span><span class="p">):</span>
    <span class="n">axi</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">species_names</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    
    <span class="c"># plot coastlines with basemap</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">Basemap</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s">'cyl'</span><span class="p">,</span> <span class="n">llcrnrlat</span><span class="o">=</span><span class="n">Y</span><span class="o">.</span><span class="nb">min</span><span class="p">(),</span>
                <span class="n">urcrnrlat</span><span class="o">=</span><span class="n">Y</span><span class="o">.</span><span class="nb">max</span><span class="p">(),</span> <span class="n">llcrnrlon</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="nb">min</span><span class="p">(),</span>
                <span class="n">urcrnrlon</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="nb">max</span><span class="p">(),</span> <span class="n">resolution</span><span class="o">=</span><span class="s">'c'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axi</span><span class="p">)</span>
    <span class="n">m</span><span class="o">.</span><span class="n">drawmapboundary</span><span class="p">(</span><span class="n">fill_color</span><span class="o">=</span><span class="s">'#DDEEFF'</span><span class="p">)</span>
    <span class="n">m</span><span class="o">.</span><span class="n">drawcoastlines</span><span class="p">()</span>
    <span class="n">m</span><span class="o">.</span><span class="n">drawcountries</span><span class="p">()</span>
    
    <span class="c"># construct a spherical kernel density estimate of the distribution</span>
    <span class="n">kde</span> <span class="o">=</span> <span class="n">KernelDensity</span><span class="p">(</span><span class="n">bandwidth</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s">'haversine'</span><span class="p">)</span>
    <span class="n">kde</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">radians</span><span class="p">(</span><span class="n">latlon</span><span class="p">[</span><span class="n">species</span> <span class="o">==</span> <span class="n">i</span><span class="p">]))</span>

    <span class="c"># evaluate only on the land: -9999 indicates ocean</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">land_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mf">9999.0</span><span class="p">)</span>
    <span class="n">Z</span><span class="p">[</span><span class="n">land_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">kde</span><span class="o">.</span><span class="n">score_samples</span><span class="p">(</span><span class="n">xy</span><span class="p">))</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="c"># plot contours of the density</span>
    <span class="n">levels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Z</span><span class="o">.</span><span class="nb">max</span><span class="p">(),</span> <span class="mi">25</span><span class="p">)</span>
    <span class="n">axi</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="n">levels</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_742_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Compared to the simple scatter plot we initially used, this visualization paints a much clearer picture of the geographical distribution of observations of these two species.</p>

<h2 id="example-not-so-naive-bayes">Example: Not-So-Naive Bayes</h2>

<p>This example looks at Bayesian generative classification with KDE, and demonstrates how to use the Scikit-Learn architecture to create a custom estimator.</p>

<p>In <a href="05.05-Naive-Bayes.ipynb">In Depth: Naive Bayes Classification</a>, we took a look at naive Bayesian classification, in which we created a simple generative model for each class, and used these models to build a fast classifier.
For Gaussian naive Bayes, the generative model is a simple axis-aligned Gaussian.
With a density estimation algorithm like KDE, we can remove the “naive” element and perform the same classification with a more sophisticated generative model for each class.
It’s still Bayesian classification, but it’s no longer naive.</p>

<p>The general approach for generative classification is this:</p>

<ol>
  <li>
    <p>Split the training data by label.</p>
  </li>
  <li>
    <p>For each set, fit a KDE to obtain a generative model of the data.
This allows you for any observation $x$ and label $y$ to compute a likelihood $P(x~|~y)$.</p>
  </li>
  <li>
    <p>From the number of examples of each class in the training set, compute the <em>class prior</em>, $P(y)$.</p>
  </li>
  <li>
    <p>For an unknown point $x$, the posterior probability for each class is $P(y~|~x) \propto P(x~|~y)P(y)$.
The class which maximizes this posterior is the label assigned to the point.</p>
  </li>
</ol>

<p>The algorithm is straightforward and intuitive to understand; the more difficult piece is couching it within the Scikit-Learn framework in order to make use of the grid search and cross-validation architecture.</p>

<p>This is the code that implements the algorithm within the Scikit-Learn framework; we will step through it following the code block:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">ClassifierMixin</span>


<span class="k">class</span> <span class="nc">KDEClassifier</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">ClassifierMixin</span><span class="p">):</span>
    <span class="s">"""Bayesian generative classification based on KDE
    
    Parameters
    ----------
    bandwidth : float
        the kernel bandwidth within each class
    kernel : str
        the kernel name, passed to KernelDensity
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bandwidth</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s">'gaussian'</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bandwidth</span> <span class="o">=</span> <span class="n">bandwidth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span>
        
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
        <span class="n">training_sets</span> <span class="o">=</span> <span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">yi</span><span class="p">]</span> <span class="k">for</span> <span class="n">yi</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">models_</span> <span class="o">=</span> <span class="p">[</span><span class="n">KernelDensity</span><span class="p">(</span><span class="n">bandwidth</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bandwidth</span><span class="p">,</span>
                                      <span class="n">kernel</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xi</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">Xi</span> <span class="ow">in</span> <span class="n">training_sets</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logpriors_</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">Xi</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                           <span class="k">for</span> <span class="n">Xi</span> <span class="ow">in</span> <span class="n">training_sets</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span>
        
    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">logprobs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">model</span><span class="o">.</span><span class="n">score_samples</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
                             <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">models_</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logprobs</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">logpriors_</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span> <span class="o">/</span> <span class="n">result</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="mi">1</span><span class="p">)]</span>
</code></pre></div>    </div>
  </div>

</div>

<h3 id="the-anatomy-of-a-custom-estimator">The anatomy of a custom estimator</h3>

<p>Let’s step through this code and discuss the essential features:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">ClassifierMixin</span>

<span class="k">class</span> <span class="nc">KDEClassifier</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">ClassifierMixin</span><span class="p">):</span>
    <span class="s">"""Bayesian generative classification based on KDE
    
    Parameters
    ----------
    bandwidth : float
        the kernel bandwidth within each class
    kernel : str
        the kernel name, passed to KernelDensity
    """</span>
</code></pre></div></div>

<p>Each estimator in Scikit-Learn is a class, and it is most convenient for this class to inherit from the <code class="highlighter-rouge">BaseEstimator</code> class as well as the appropriate mixin, which provides standard functionality.
For example, among other things, here the <code class="highlighter-rouge">BaseEstimator</code> contains the logic necessary to clone/copy an estimator for use in a cross-validation procedure, and <code class="highlighter-rouge">ClassifierMixin</code> defines a default <code class="highlighter-rouge">score()</code> method used by such routines.
We also provide a doc string, which will be captured by IPython’s help functionality (see <a href="01.01-Help-And-Documentation.ipynb">Help and Documentation in IPython</a>).</p>

<p>Next comes the class initialization method:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bandwidth</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s">'gaussian'</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bandwidth</span> <span class="o">=</span> <span class="n">bandwidth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span>
</code></pre></div></div>

<p>This is the actual code that is executed when the object is instantiated with <code class="highlighter-rouge">KDEClassifier()</code>.
In Scikit-Learn, it is important that <em>initialization contains no operations</em> other than assigning the passed values by name to <code class="highlighter-rouge">self</code>.
This is due to the logic contained in <code class="highlighter-rouge">BaseEstimator</code> required for cloning and modifying estimators for cross-validation, grid search, and other functions.
Similarly, all arguments to <code class="highlighter-rouge">__init__</code> should be explicit: i.e. <code class="highlighter-rouge">*args</code> or <code class="highlighter-rouge">**kwargs</code> should be avoided, as they will not be correctly handled within cross-validation routines.</p>

<p>Next comes the <code class="highlighter-rouge">fit()</code> method, where we handle training data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
        <span class="n">training_sets</span> <span class="o">=</span> <span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">yi</span><span class="p">]</span> <span class="k">for</span> <span class="n">yi</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">models_</span> <span class="o">=</span> <span class="p">[</span><span class="n">KernelDensity</span><span class="p">(</span><span class="n">bandwidth</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bandwidth</span><span class="p">,</span>
                                      <span class="n">kernel</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xi</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">Xi</span> <span class="ow">in</span> <span class="n">training_sets</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logpriors_</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">Xi</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                           <span class="k">for</span> <span class="n">Xi</span> <span class="ow">in</span> <span class="n">training_sets</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span>
</code></pre></div></div>

<p>Here we find the unique classes in the training data, train a <code class="highlighter-rouge">KernelDensity</code> model for each class, and compute the class priors based on the number of input samples.
Finally, <code class="highlighter-rouge">fit()</code> should always return <code class="highlighter-rouge">self</code> so that we can chain commands. For example:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">label</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div></div>
<p>Notice that each persistent result of the fit is stored with a trailing underscore (e.g., <code class="highlighter-rouge">self.logpriors_</code>).
This is a convention used in Scikit-Learn so that you can quickly scan the members of an estimator (using IPython’s tab completion) and see exactly which members are fit to training data.</p>

<p>Finally, we have the logic for predicting labels on new data:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">logprobs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">model</span><span class="o">.</span><span class="n">score_samples</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
                              <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">models_</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logprobs</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">logpriors_</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span> <span class="o">/</span> <span class="n">result</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="mi">1</span><span class="p">)]</span>
</code></pre></div></div>
<p>Because this is a probabilistic classifier, we first implement <code class="highlighter-rouge">predict_proba()</code> which returns an array of class probabilities of shape <code class="highlighter-rouge">[n_samples, n_classes]</code>.
Entry <code class="highlighter-rouge">[i, j]</code> of this array is the posterior probability that sample <code class="highlighter-rouge">i</code> is a member of class <code class="highlighter-rouge">j</code>, computed by multiplying the likelihood by the class prior and normalizing.</p>

<p>Finally, the <code class="highlighter-rouge">predict()</code> method uses these probabilities and simply returns the class with the largest probability.</p>

<h3 id="using-our-custom-estimator">Using our custom estimator</h3>

<p>Let’s try this custom estimator on a problem we have seen before: the classification of hand-written digits.
Here we will load the digits, and compute the cross-validation score for a range of candidate bandwidths using the <code class="highlighter-rouge">GridSearchCV</code> meta-estimator (refer back to <a href="05.03-Hyperparameters-and-Model-Validation.ipynb">Hyperparameters and Model Validation</a>):</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="kn">from</span> <span class="nn">sklearn.grid_search</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="n">digits</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>

<span class="n">bandwidths</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">**</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">KDEClassifier</span><span class="p">(),</span> <span class="p">{</span><span class="s">'bandwidth'</span><span class="p">:</span> <span class="n">bandwidths</span><span class="p">})</span>
<span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="p">[</span><span class="n">val</span><span class="o">.</span><span class="n">mean_validation_score</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">grid</span><span class="o">.</span><span class="n">grid_scores_</span><span class="p">]</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Next we can plot the cross-validation score as a function of bandwidth:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">semilogx</span><span class="p">(</span><span class="n">bandwidths</span><span class="p">,</span> <span class="n">scores</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'bandwidth'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'KDE Model Performance'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'accuracy ='</span><span class="p">,</span> <span class="n">grid</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">
      <div class="output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'bandwidth': 7.0548023107186433}
accuracy = 0.966611018364
</code></pre></div>      </div>
    </div>
  </div>
  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_754_1.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>We see that this not-so-naive Bayesian classifier reaches a cross-validation accuracy of just over 96%; this is compared to around 80% for the naive Bayesian classification:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="n">cross_val_score</span><span class="p">(</span><span class="n">GaussianNB</span><span class="p">(),</span> <span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.81860038035501381
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>One benefit of such a generative classifier is interpretability of results: for each unknown sample, we not only get a probabilistic classification, but a <em>full model</em> of the distribution of points we are comparing it to!
If desired, this offers an intuitive window into the reasons for a particular classification that algorithms like SVMs and random forests tend to obscure.</p>

<p>If you would like to take this further, there are some improvements that could be made to our KDE classifier model:</p>

<ul>
  <li>we could allow the bandwidth in each class to vary independently</li>
  <li>we could optimize these bandwidths not based on their prediction score, but on the likelihood of the training data under the generative model within each class (i.e. use the scores from <code class="highlighter-rouge">KernelDensity</code> itself rather than the global prediction accuracy)</li>
</ul>

<p>Finally, if you want some practice building your own estimator, you might tackle building a similar Bayesian classifier using Gaussian Mixture Models instead of KDE.</p>

<!--NAVIGATION-->
<p>&lt; <a href="05.12-Gaussian-Mixtures.ipynb">In Depth: Gaussian Mixture Models</a> | <a href="Index.ipynb">Contents</a> | <a href="05.14-Image-Features.ipynb">Application: A Face Detection Pipeline</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.13-Kernel-Density-Estimation.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<!--BOOK_INFORMATION-->
<p><img align="left" style="padding-right:10px;" src="figures/PDSH-cover-small.png" /></p>

<p><em>This notebook contains an excerpt from the <a href="http://shop.oreilly.com/product/0636920034919.do">Python Data Science Handbook</a> by Jake VanderPlas; the content is available <a href="https://github.com/jakevdp/PythonDataScienceHandbook">on GitHub</a>.</em></p>

<p><em>The text is released under the <a href="https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode">CC-BY-NC-ND license</a>, and code is released under the <a href="https://opensource.org/licenses/MIT">MIT license</a>. If you find this content useful, please consider supporting the work by <a href="http://shop.oreilly.com/product/0636920034919.do">buying the book</a>!</em></p>

<!--NAVIGATION-->
<p>&lt; <a href="05.13-Kernel-Density-Estimation.ipynb">In-Depth: Kernel Density Estimation</a> | <a href="Index.ipynb">Contents</a> | <a href="05.15-Learning-More.ipynb">Further Machine Learning Resources</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.14-Image-Features.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<h1 id="application-a-face-detection-pipeline">Application: A Face Detection Pipeline</h1>

<p>This chapter has explored a number of the central concepts and algorithms of machine learning.
But moving from these concepts to real-world application can be a challenge.
Real-world datasets are noisy and heterogeneous, may have missing features, and data may be in a form that is difficult to map to a clean <code class="highlighter-rouge">[n_samples, n_features]</code> matrix.
Before applying any of the methods discussed here, you must first extract these features from your data: there is no formula for how to do this that applies across all domains, and thus this is where you as a data scientist must exercise your own intuition and expertise.</p>

<p>One interesting and compelling application of machine learning is to images, and we have already seen a few examples of this where pixel-level features are used for classification.
In the real world, data is rarely so uniform and simple pixels will not be suitable: this has led to a large literature on <em>feature extraction</em> methods for image data (see <a href="05.04-Feature-Engineering.ipynb">Feature Engineering</a>).</p>

<p>In this section, we will take a look at one such feature extraction technique, the <a href="https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients">Histogram of Oriented Gradients</a> (HOG), which transforms image pixels into a vector representation that is sensitive to broadly informative image features regardless of confounding factors like illumination.
We will use these features to develop a simple face detection pipeline, using machine learning algorithms and concepts we’ve seen throughout this chapter.</p>

<p>We begin with the standard imports:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span><span class="p">;</span> <span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">()</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
</code></pre></div>    </div>
  </div>

</div>

<h2 id="hog-features">HOG Features</h2>

<p>The Histogram of Gradients is a straightforward feature extraction procedure that was developed in the context of identifying pedestrians within images.
HOG involves the following steps:</p>

<ol>
  <li>Optionally pre-normalize images. This leads to features that resist dependence on variations in illumination.</li>
  <li>Convolve the image with two filters that are sensitive to horizontal and vertical brightness gradients. These capture edge, contour, and texture information.</li>
  <li>Subdivide the image into cells of a predetermined size, and compute a histogram of the gradient orientations within each cell.</li>
  <li>Normalize the histograms in each cell by comparing to the block of neighboring cells. This further suppresses the effect of illumination across the image.</li>
  <li>Construct a one-dimensional feature vector from the information in each cell.</li>
</ol>

<p>A fast HOG extractor is built into the Scikit-Image project, and we can try it out relatively quickly and visualize the oriented gradients within each cell:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">skimage</span> <span class="kn">import</span> <span class="n">data</span><span class="p">,</span> <span class="n">color</span><span class="p">,</span> <span class="n">feature</span>
<span class="kn">import</span> <span class="nn">skimage.data</span>

<span class="n">image</span> <span class="o">=</span> <span class="n">color</span><span class="o">.</span><span class="n">rgb2gray</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">chelsea</span><span class="p">())</span>
<span class="n">hog_vec</span><span class="p">,</span> <span class="n">hog_vis</span> <span class="o">=</span> <span class="n">feature</span><span class="o">.</span><span class="n">hog</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">visualise</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span>
                       <span class="n">subplot_kw</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[]))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'input image'</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">hog_vis</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'visualization of HOG features'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_765_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<h2 id="hog-in-action-a-simple-face-detector">HOG in Action: A Simple Face Detector</h2>

<p>Using these HOG features, we can build up a simple facial detection algorithm with any Scikit-Learn estimator; here we will use a linear support vector machine (refer back to <a href="05.07-Support-Vector-Machines.ipynb">In-Depth: Support Vector Machines</a> if you need a refresher on this).
The steps are as follows:</p>

<ol>
  <li>Obtain a set of image thumbnails of faces to constitute “positive” training samples.</li>
  <li>Obtain a set of image thumbnails of non-faces to constitute “negative” training samples.</li>
  <li>Extract HOG features from these training samples.</li>
  <li>Train a linear SVM classifier on these samples.</li>
  <li>For an “unknown” image, pass a sliding window across the image, using the model to evaluate whether that window contains a face or not.</li>
  <li>If detections overlap, combine them into a single window.</li>
</ol>

<p>Let’s go through these steps and try it out:</p>

<h3 id="1-obtain-a-set-of-positive-training-samples">1. Obtain a set of positive training samples</h3>

<p>Let’s start by finding some positive training samples that show a variety of faces.
We have one easy set of data to work with—the Labeled Faces in the Wild dataset, which can be downloaded by Scikit-Learn:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_lfw_people</span>
<span class="n">faces</span> <span class="o">=</span> <span class="n">fetch_lfw_people</span><span class="p">()</span>
<span class="n">positive_patches</span> <span class="o">=</span> <span class="n">faces</span><span class="o">.</span><span class="n">images</span>
<span class="n">positive_patches</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(13233, 62, 47)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>This gives us a sample of 13,000 face images to use for training.</p>

<h3 id="2-obtain-a-set-of-negative-training-samples">2. Obtain a set of negative training samples</h3>

<p>Next we need a set of similarly sized thumbnails which <em>do not</em> have a face in them.
One way to do this is to take any corpus of input images, and extract thumbnails from them at a variety of scales.
Here we can use some of the images shipped with Scikit-Image, along with Scikit-Learn’s <code class="highlighter-rouge">PatchExtractor</code>:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">skimage</span> <span class="kn">import</span> <span class="n">data</span><span class="p">,</span> <span class="n">transform</span>

<span class="n">imgs_to_use</span> <span class="o">=</span> <span class="p">[</span><span class="s">'camera'</span><span class="p">,</span> <span class="s">'text'</span><span class="p">,</span> <span class="s">'coins'</span><span class="p">,</span> <span class="s">'moon'</span><span class="p">,</span>
               <span class="s">'page'</span><span class="p">,</span> <span class="s">'clock'</span><span class="p">,</span> <span class="s">'immunohistochemistry'</span><span class="p">,</span>
               <span class="s">'chelsea'</span><span class="p">,</span> <span class="s">'coffee'</span><span class="p">,</span> <span class="s">'hubble_deep_field'</span><span class="p">]</span>
<span class="n">images</span> <span class="o">=</span> <span class="p">[</span><span class="n">color</span><span class="o">.</span><span class="n">rgb2gray</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">name</span><span class="p">)())</span>
          <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">imgs_to_use</span><span class="p">]</span>
</code></pre></div>    </div>
  </div>

</div>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.image</span> <span class="kn">import</span> <span class="n">PatchExtractor</span>

<span class="k">def</span> <span class="nf">extract_patches</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">patch_size</span><span class="o">=</span><span class="n">positive_patches</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">):</span>
    <span class="n">extracted_patch_size</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">((</span><span class="n">scale</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">patch_size</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">))</span>
    <span class="n">extractor</span> <span class="o">=</span> <span class="n">PatchExtractor</span><span class="p">(</span><span class="n">patch_size</span><span class="o">=</span><span class="n">extracted_patch_size</span><span class="p">,</span>
                               <span class="n">max_patches</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">patches</span> <span class="o">=</span> <span class="n">extractor</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">img</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">scale</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">patches</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">transform</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">patch</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">)</span>
                            <span class="k">for</span> <span class="n">patch</span> <span class="ow">in</span> <span class="n">patches</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">patches</span>

<span class="n">negative_patches</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">extract_patches</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
                              <span class="k">for</span> <span class="n">im</span> <span class="ow">in</span> <span class="n">images</span> <span class="k">for</span> <span class="n">scale</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]])</span>
<span class="n">negative_patches</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(30000, 62, 47)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>We now have 30,000 suitable image patches which do not contain faces.
Let’s take a look at a few of them to get an idea of what they look like:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axi</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">flat</span><span class="p">):</span>
    <span class="n">axi</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">negative_patches</span><span class="p">[</span><span class="mi">500</span> <span class="o">*</span> <span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
    <span class="n">axi</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_774_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Our hope is that these would sufficiently cover the space of “non-faces” that our algorithm is likely to see.</p>

<h3 id="3-combine-sets-and-extract-hog-features">3. Combine sets and extract HOG features</h3>

<p>Now that we have these positive samples and negative samples, we can combine them and compute HOG features.
This step takes a little while, because the HOG features involve a nontrivial computation for each image:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">chain</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">feature</span><span class="o">.</span><span class="n">hog</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">im</span> <span class="ow">in</span> <span class="n">chain</span><span class="p">(</span><span class="n">positive_patches</span><span class="p">,</span>
                                    <span class="n">negative_patches</span><span class="p">)])</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">y_train</span><span class="p">[:</span><span class="n">positive_patches</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
</code></pre></div>    </div>
  </div>

</div>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(43233, 1215)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>We are left with 43,000 training samples in 1,215 dimensions, and we now have our data in a form that we can feed into Scikit-Learn!</p>

<h3 id="4-training-a-support-vector-machine">4. Training a support vector machine</h3>

<p>Next we use the tools we have been exploring in this chapter to create a classifier of thumbnail patches.
For such a high-dimensional binary classification task, a Linear support vector machine is a good choice.
We will use Scikit-Learn’s <code class="highlighter-rouge">LinearSVC</code>, because in comparison to <code class="highlighter-rouge">SVC</code> it often has better scaling for large number of samples.</p>

<p>First, though, let’s use a simple Gaussian naive Bayes to get a quick baseline:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="n">cross_val_score</span><span class="p">(</span><span class="n">GaussianNB</span><span class="p">(),</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([ 0.9408785 ,  0.8752342 ,  0.93976823])
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>We see that on our training data, even a simple naive Bayes algorithm gets us upwards of 90% accuracy.
Let’s try the support vector machine, with a grid search over a few choices of the C parameter:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>
<span class="kn">from</span> <span class="nn">sklearn.grid_search</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">LinearSVC</span><span class="p">(),</span> <span class="p">{</span><span class="s">'C'</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">]})</span>
<span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">grid</span><span class="o">.</span><span class="n">best_score_</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.98667684407744083
</code></pre></div>      </div>

    </div>
  </div>
</div>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">grid</span><span class="o">.</span><span class="n">best_params_</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'C': 4.0}
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>Let’s take the best estimator and re-train it on the full dataset:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">best_estimator_</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>LinearSVC(C=4.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<h3 id="5-find-faces-in-a-new-image">5. Find faces in a new image</h3>

<p>Now that we have this model in place, let’s grab a new image and see how the model does.
We will use one portion of the astronaut image for simplicity (see discussion of this in <a href="#Caveats-and-Improvements">Caveats and Improvements</a>), and run a sliding window over it and evaluate each patch:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_image</span> <span class="o">=</span> <span class="n">skimage</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">astronaut</span><span class="p">()</span>
<span class="n">test_image</span> <span class="o">=</span> <span class="n">skimage</span><span class="o">.</span><span class="n">color</span><span class="o">.</span><span class="n">rgb2gray</span><span class="p">(</span><span class="n">test_image</span><span class="p">)</span>
<span class="n">test_image</span> <span class="o">=</span> <span class="n">skimage</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">rescale</span><span class="p">(</span><span class="n">test_image</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">test_image</span> <span class="o">=</span> <span class="n">test_image</span><span class="p">[:</span><span class="mi">160</span><span class="p">,</span> <span class="mi">40</span><span class="p">:</span><span class="mi">180</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">test_image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">);</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_788_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Next, let’s create a window that iterates over patches of this image, and compute HOG features for each patch:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sliding_window</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">patch_size</span><span class="o">=</span><span class="n">positive_patches</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                   <span class="n">istep</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">jstep</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="n">Ni</span><span class="p">,</span> <span class="n">Nj</span> <span class="o">=</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">scale</span> <span class="o">*</span> <span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">patch_size</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">Ni</span><span class="p">,</span> <span class="n">istep</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">Ni</span><span class="p">,</span> <span class="n">jstep</span><span class="p">):</span>
            <span class="n">patch</span> <span class="o">=</span> <span class="n">img</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">Ni</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span><span class="n">j</span> <span class="o">+</span> <span class="n">Nj</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">scale</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">patch</span> <span class="o">=</span> <span class="n">transform</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">patch</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">)</span>
            <span class="k">yield</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">),</span> <span class="n">patch</span>
            
<span class="n">indices</span><span class="p">,</span> <span class="n">patches</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">sliding_window</span><span class="p">(</span><span class="n">test_image</span><span class="p">))</span>
<span class="n">patches_hog</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">feature</span><span class="o">.</span><span class="n">hog</span><span class="p">(</span><span class="n">patch</span><span class="p">)</span> <span class="k">for</span> <span class="n">patch</span> <span class="ow">in</span> <span class="n">patches</span><span class="p">])</span>
<span class="n">patches_hog</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(1911, 1215)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>Finally, we can take these HOG-featured patches and use our model to evaluate whether each patch contains a face:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">labels</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">patches_hog</span><span class="p">)</span>
<span class="n">labels</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>33.0
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>We see that out of nearly 2,000 patches, we have found 30 detections.
Let’s use the information we have about these patches to show where they lie on our test image, drawing them as rectangles:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">test_image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>

<span class="n">Ni</span><span class="p">,</span> <span class="n">Nj</span> <span class="o">=</span> <span class="n">positive_patches</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">[</span><span class="n">labels</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]:</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">),</span> <span class="n">Nj</span><span class="p">,</span> <span class="n">Ni</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s">'red'</span><span class="p">,</span>
                               <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s">'none'</span><span class="p">))</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/11ReadBook/03handbook-en/merge_5_794_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>All of the detected patches overlap and found the face in the image!
Not bad for a few lines of Python.</p>

<h2 id="caveats-and-improvements">Caveats and Improvements</h2>

<p>If you dig a bit deeper into the preceding code and examples, you’ll see that we still have a bit of work before we can claim a production-ready face detector.
There are several issues with what we’ve done, and several improvements that could be made. In particular:</p>

<h3 id="our-training-set-especially-for-negative-features-is-not-very-complete">Our training set, especially for negative features, is not very complete</h3>

<p>The central issue is that there are many face-like textures that are not in the training set, and so our current model is very prone to false positives.
You can see this if you try out the above algorithm on the <em>full</em> astronaut image: the current model leads to many false detections in other regions of the image.</p>

<p>We might imagine addressing this by adding a wider variety of images to the negative training set, and this would probably yield some improvement.
Another way to address this is to use a more directed approach, such as <em>hard negative mining</em>.
In hard negative mining, we take a new set of images that our classifier has not seen, find all the patches representing false positives, and explicitly add them as negative instances in the training set before re-training the classifier.</p>

<h3 id="our-current-pipeline-searches-only-at-one-scale">Our current pipeline searches only at one scale</h3>

<p>As currently written, our algorithm will miss faces that are not approximately 62×47 pixels.
This can be straightforwardly addressed by using sliding windows of a variety of sizes, and re-sizing each patch using <code class="highlighter-rouge">skimage.transform.resize</code> before feeding it into the model.
In fact, the <code class="highlighter-rouge">sliding_window()</code> utility used here is already built with this in mind.</p>

<h3 id="we-should-combine-overlapped-detection-patches">We should combine overlapped detection patches</h3>

<p>For a production-ready pipeline, we would prefer not to have 30 detections of the same face, but to somehow reduce overlapping groups of detections down to a single detection.
This could be done via an unsupervised clustering approach (MeanShift Clustering is one good candidate for this), or via a procedural approach such as <em>non-maximum suppression</em>, an algorithm common in machine vision.</p>

<h3 id="the-pipeline-should-be-streamlined">The pipeline should be streamlined</h3>

<p>Once we address these issues, it would also be nice to create a more streamlined pipeline for ingesting training images and predicting sliding-window outputs.
This is where Python as a data science tool really shines: with a bit of work, we could take our prototype code and package it with a well-designed object-oriented API that give the user the ability to use this easily.
I will leave this as a proverbial “exercise for the reader”.</p>

<h3 id="more-recent-advances-deep-learning">More recent advances: Deep Learning</h3>

<p>Finally, I should add that HOG and other procedural feature extraction methods for images are no longer state-of-the-art techniques.
Instead, many modern object detection pipelines use variants of deep neural networks: one way to think of neural networks is that they are an estimator which determines optimal feature extraction strategies from the data, rather than relying on the intuition of the user.
An intro to these deep neural net methods is conceptually (and computationally!) beyond the scope of this section, although open tools like Google’s <a href="https://www.tensorflow.org/">TensorFlow</a> have recently made deep learning approaches much more accessible than they once were.
As of the writing of this book, deep learning in Python is still relatively young, and so I can’t yet point to any definitive resource.
That said, the list of references in the following section should provide a useful place to start!</p>

<!--NAVIGATION-->
<p>&lt; <a href="05.13-Kernel-Density-Estimation.ipynb">In-Depth: Kernel Density Estimation</a> | <a href="Index.ipynb">Contents</a> | <a href="05.15-Learning-More.ipynb">Further Machine Learning Resources</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.14-Image-Features.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<!--BOOK_INFORMATION-->
<p><img align="left" style="padding-right:10px;" src="figures/PDSH-cover-small.png" /></p>

<p><em>This notebook contains an excerpt from the <a href="http://shop.oreilly.com/product/0636920034919.do">Python Data Science Handbook</a> by Jake VanderPlas; the content is available <a href="https://github.com/jakevdp/PythonDataScienceHandbook">on GitHub</a>.</em></p>

<p><em>The text is released under the <a href="https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode">CC-BY-NC-ND license</a>, and code is released under the <a href="https://opensource.org/licenses/MIT">MIT license</a>. If you find this content useful, please consider supporting the work by <a href="http://shop.oreilly.com/product/0636920034919.do">buying the book</a>!</em></p>

<!--NAVIGATION-->
<p>&lt; <a href="05.14-Image-Features.ipynb">Application: A Face Detection Pipeline</a> | <a href="Index.ipynb">Contents</a> | <a href="06.00-Figure-Code.ipynb">Appendix: Figure Code</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.15-Learning-More.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<h1 id="further-machine-learning-resources">Further Machine Learning Resources</h1>

<p>This chapter has been a quick tour of machine learning in Python, primarily using the tools within the Scikit-Learn library.
As long as the chapter is, it is still too short to cover many interesting and important algorithms, approaches, and discussions.
Here I want to suggest some resources to learn more about machine learning for those who are interested.</p>

<h2 id="machine-learning-in-python">Machine Learning in Python</h2>

<p>To learn more about machine learning in Python, I’d suggest some of the following resources:</p>

<ul>
  <li>
    <p><a href="http://scikit-learn.org">The Scikit-Learn website</a>: The Scikit-Learn website has an impressive breadth of documentation and examples covering some of the models discussed here, and much, much more. If you want a brief survey of the most important and often-used machine learning algorithms, this website is a good place to start.</p>
  </li>
  <li>
    <p><em>SciPy, PyCon, and PyData tutorial videos</em>: Scikit-Learn and other machine learning topics are perennial favorites in the tutorial tracks of many Python-focused conference series, in particular the PyCon, SciPy, and PyData conferences. You can find the most recent ones via a simple web search.</p>
  </li>
  <li>
    <p><a href="http://shop.oreilly.com/product/0636920030515.do"><em>Introduction to Machine Learning with Python</em></a>: Written by Andreas C. Mueller and Sarah Guido, this book includes a fuller treatment of the topics in this chapter. If you’re interested in reviewing the fundamentals of Machine Learning and pushing the Scikit-Learn toolkit to its limits, this is a great resource, written by one of the most prolific developers on the Scikit-Learn team.</p>
  </li>
  <li>
    <p><a href="https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning"><em>Python Machine Learning</em></a>: Sebastian Raschka’s book focuses less on Scikit-learn itself, and more on the breadth of machine learning tools available in Python. In particular, there is some very useful discussion on how to scale Python-based machine learning approaches to large and complex datasets.</p>
  </li>
</ul>

<h2 id="general-machine-learning">General Machine Learning</h2>

<p>Of course, machine learning is much broader than just the Python world. There are many good resources to take your knowledge further, and here I will highlight a few that I have found useful:</p>

<ul>
  <li>
    <p><a href="https://www.coursera.org/learn/machine-learning"><em>Machine Learning</em></a>: Taught by Andrew Ng (Coursera), this is a very clearly-taught free online course which covers the basics of machine learning from an algorithmic perspective. It assumes undergraduate-level understanding of mathematics and programming, and steps through detailed considerations of some of the most important machine learning algorithms. Homework assignments, which are algorithmically graded, have you actually implement some of these models yourself.</p>
  </li>
  <li>
    <p><a href="http://www.springer.com/us/book/9780387310732"><em>Pattern Recognition and Machine Learning</em></a>: Written by Christopher Bishop, this classic technical text covers the concepts of machine learning discussed in this chapter in detail. If you plan to go further in this subject, you should have this book on your shelf.</p>
  </li>
  <li>
    <p><a href="https://mitpress.mit.edu/books/machine-learning-0"><em>Machine Learning: a Probabilistic Perspective</em></a>: Written by Kevin Murphy, this is an excellent graduate-level text that explores nearly all important machine learning algorithms from a ground-up, unified probabilistic perspective.</p>
  </li>
</ul>

<p>These resources are more technical than the material presented in this book, but to really understand the fundamentals of these methods requires a deep dive into the mathematics behind them.
If you’re up for the challenge and ready to bring your data science to the next level, don’t hesitate to dive-in!</p>

<!--NAVIGATION-->
<p>&lt; <a href="05.14-Image-Features.ipynb">Application: A Face Detection Pipeline</a> | <a href="Index.ipynb">Contents</a> | <a href="06.00-Figure-Code.ipynb">Appendix: Figure Code</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.15-Learning-More.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>


                <nav class="c-page__nav">
  
    
    <a id="js-page__nav__prev" class="c-page__nav__prev" href="/blog/blog2/11ReadBook/03handbook-en/merge_4">
      〈 <span class="u-margin-right-tiny"></span> merge_4
    </a>
  

  
    
    <a id="js-page__nav__next" class="c-page__nav__next" href="/blog/blog2/11ReadBook/04MachineLearningPractice/index">
      MachineLearningPractice <span class="u-margin-right-tiny"></span> 〉
    </a>
  
</nav>

            </div>
        </div>
    </main>
</div>

</body>
</html>
